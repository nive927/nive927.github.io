[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Project Portfolio",
    "section": "",
    "text": "Movie Recommender\n\n\n\nAI Agents\n\n\nLLMs\n\n\nWeb Automation\n\n\n\nAn Autonomous Gmail-Based Deal Discovery and Comparison Agent\n\n\n\nNivedhitha Dhanasekaran\n\n\nApr 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPromo Miner: Shopping AIAgent\n\n\n\nAI Agents\n\n\nAward Winner\n\n\nLarge Language Models\n\n\nMonitoring\n\n\nNatural Language Processing\n\n\nUI/UX\n\n\nWeb Development\n\n\n\nAn Autonomous Gmail-Based Deal Discovery and Comparison Agent\n\n\n\nNivedhitha Dhanasekaran\n\n\nApr 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArduino Gesture-based Game Controller Joystick Design\n\n\n\nComputer Vision\n\n\nDeep Learning\n\n\nHuman-computer Interaction\n\n\nInteraction Design\n\n\nMachine Learning Systems\n\n\nOn-Device AI\n\n\nSignal Processing\n\n\n\nMid-air, two-hand Arduino gesture controller with on-device neural inference and BLE-based wireless control for 3D interaction\n\n\n\nNivedhitha Dhanasekaran\n\n\nMar 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPromo Miner: AI Shopping Agent\n\n\n\nAI Agents\n\n\nLLMs\n\n\nWeb Automation\n\n\n\nAn Autonomous Gmail-Based Deal Discovery and Comparison Agent\n\n\n\nNivedhitha Dhanasekaran\n\n\nDec 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHarnessing Everyday Users‚Äô Power to Detect Harmful Behaviors in GenAI\n\n\n\nHCI\n\n\nResponsible AI\n\n\nUI/UX\n\n\nResearch\n\n\n\nUser-centered Research and Evaluation for Responsible GenAI\n\n\n\nNivedhitha Dhanasekaran\n\n\nMay 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMusic Magician\n\n\n\nInteractive Intelligence\n\n\nData Visualization\n\n\nWeb Development\n\n\nUI/UX\n\n\nHCI\n\n\n\nSpotify Music Artist Influence Analytics Dashboard\n\n\n\nNivedhitha Dhanasekaran\n\n\nDec 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutomated Detection of Giant Cell Arteritis from Temporal Artery Biopsy Specimens Using Deep Learning\n\n\n\nComputational Biomedicine\n\n\nData Science\n\n\nDeep Learning\n\n\nFunding\n\n\nGPU Training\n\n\nMachine Learning Systems\n\n\nModel Explainability\n\n\nPublication\n\n\nResearch\n\n\n\nResNet-based classification of histopathological slides for vascular inflammation diagnosis\n\n\n\nNivedhitha Dhanasekaran\n\n\nJul 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBraille Voice\n\n\n\nComputer Vision\n\n\nNLP\n\n\nWeb Development\n\n\nMobile Development\n\n\nDeep Learning\n\n\nHCI\n\n\nResearch\n\n\n\nA Language Agnostic Assistive Technology for Braille-to-Text Translation\n\n\n\nNivedhitha Dhanasekaran\n\n\nJul 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nORCA: Underwater Robot\n\n\n\nAward Winner\n\n\nComputer Vision\n\n\nDeep Learning\n\n\nFunding\n\n\nHuman-Computer Interaction\n\n\nMachine Learning Systems\n\n\nOn-Device AI\n\n\nPublication\n\n\nResearch\n\n\nRobotics\n\n\nSignal Processing\n\n\n\nAn Inspection Class Remotely Operated Underwater Vehicle for Autonomous and Manual Navigation\n\n\n\nNivedhitha Dhanasekaran\n\n\nJun 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo Stage Flight Delay Prediction\n\n\n\nMachine Learning\n\n\nData Visualization\n\n\nStatistical Analysis\n\n\n\nPredicting the On-time Performance of Flights in the USA\n\n\n\nNivedhitha Dhanasekaran\n\n\nJun 30, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/2025-04-13-promo-miner/index.html",
    "href": "projects/2025-04-13-promo-miner/index.html",
    "title": "Promo Miner: Shopping AIAgent",
    "section": "",
    "text": "Related Links & Artifacts\n\nHackathon Website\nDownload Pitch Deck (PDF)\nüí∞ Sponsored by Google & DeepMind ($500 Prize)\n\n\n\nProject Overview\nPromo Miner is an autonomous AI-powered shopping assistant that uncovers hidden savings in users‚Äô Gmail inboxes. It identifies promotional offers from the Gmail Promotions tab, uses LLMs to rank deal quality, and performs real-time web comparisons to ensure users always get the best available deal.\n\n‚ú® Built a fully integrated shopping agent that autonomously performs Gmail parsing, promo reasoning, cross-site comparison, and Chrome-based alerting in real time.\n‚ú® Combines Gemini LLM reasoning with Exa-based search and push notifications to deliver ranked savings with minimal user effort.\n\n\n\nDescription\nThe Promo Miner system securely accesses only the Gmail Promotions tab using OAuth and filters out irrelevant emails like welcome messages. Valid promotional content is parsed and structured, extracting metadata such as discounts, brand names, expiration dates, and coupon codes. These are stored in Firestore for persistent deal tracking.\nA Gemini-based LLM agent then reasons over this data to classify and rank deals based on urgency, value, and usefulness. Deals that are likely to expire soon or offer significant savings are highlighted. The system also employs an Exa-powered agent to compare prices across platforms like Amazon. If a better offer is found, the user receives a real-time push notification through a Chrome Extension interface.\nDownload PDF file.\n\n\n1. Email Access & Filtering\n\n\nUses Gmail API with OAuth 2.0 to securely access the Promotions tab\n\nFilters out onboarding/welcome emails and identifies valid promotional content\n\nParses subject line, sender, body text, promo code, and expiration cues\n\n\n\n\n2. Data Storage & Processing\n\n\nExtracted metadata is stored in Google Firestore for persistent tracking\n\nSupports longitudinal deal analysis and browser-based rendering\n\nIntegrates with Chrome Extension for lightweight front-end access\n\n\n\n\n3. LLM-Based Promo Reasoning\n\n\nGemini LLM classifies each deal on urgency, product type, savings, and utility\n\nGenerates user-specific promo rankings using context-aware heuristics\n\nAnnotates whether a deal includes coupons, bundle offers, or price drops\n\n\n\n\n4. Web Comparison Agent\n\n\nExa search agent compares extracted deals to live listings on platforms like Amazon\n\nFlags cheaper alternatives or higher-value bundles\n\nTriggers push notifications to users for better savings\n\n\n\n\n5. Deployment & Interface\n\n\nChrome Extension provides a ranked deal dashboard\n\nUsers receive timely alerts on new or expiring promos\n\nEntire pipeline runs in real time, with low-latency triggers from backend to browser\n\n\n\n\nTools & Frameworks\n\n\n\n\n\n\n\nComponent\nStack / Tools Used\n\n\n\n\nEmail Access\nGmail API, OAuth 2.0\n\n\nParsing & Storage\nFlask, Google Firestore, Regex, JSON Extractors\n\n\nReasoning Agent\nGemini, Python, Structured Query Prompting, LlamaIndex\n\n\nWeb Comparison Agent\nExa, Semantic Search, Jina Reader API\n\n\nFrontend Interface\nChrome Extension, JavaScript, HTML, CSS, Browser Notifications API\n\n\nBack-end Infrastructure\nFlask REST API, Firestore Rules"
  },
  {
    "objectID": "projects/2024-12-07-note-recall/index.html",
    "href": "projects/2024-12-07-note-recall/index.html",
    "title": "Promo Miner: AI Shopping Agent",
    "section": "",
    "text": "Most Effective Project - Winner at AI Agents Weekend 2025 by Google & DeepMind\nDownload PDF file.\n\n\nProject Description\nPromo Miner is an autonomous AI-powered shopping assistant designed to help users unlock hidden savings by surfacing the best promotional deals directly from their inbox. By integrating Gmail‚Äôs Promotions tab, real-time web search, and intelligent agents, Promo Miner ensures users never miss a better deal.\nThe system begins by securely accessing only the user‚Äôs Gmail Promotions tab via OAuth, filtering out irrelevant messages such as welcome emails. Valid promotional emails are parsed and stored in Firebase, enabling persistent deal tracking.\nPromo content is then analyzed by a Gemini Reasoning Agent that classifies each offer based on discount, urgency, product category, coupon codes, and other contextual cues. The agent scores each promo and generates a personalized ranking, triggering notifications for high-value or expiring deals.\nPromo Miner extends its reasoning capability by using an Exa Web Agent to perform real-time comparisons of the promoted product with listings on platforms like Amazon. When a better price or more valuable alternative is detected, the system alerts the user via a push notification.\nDeployed as a Chrome Extension, Promo Miner provides a real-time, personalized view of top-ranked deals and recommended alternatives. Built using Gemini LLMs, Flask, Firebase, Jina Reader, and Exa Search, the system demonstrates full-stack AI agent behavior‚Äîfrom perception and reasoning to real-time action.\n\n\nFeatures\n\nPrivacy-Preserving OAuth Login ‚Äì Only accesses Gmail Promotions tab, maintaining user privacy.\nDeal Filtering & Storage ‚Äì Automatically detects valid promos and saves them securely in Firebase.\nGemini-Based Ranking ‚Äì Contextually scores deals by urgency, discount value, and coupon presence.\nReal-Time Comparison ‚Äì Searches the web for better deals using Exa and notifies users when found.\nBrowser Extension Interface ‚Äì Presents top deals and alternatives in a simple, ranked view."
  },
  {
    "objectID": "projects/2023-12-31-music-magician-spotify-dashboard/index.html",
    "href": "projects/2023-12-31-music-magician-spotify-dashboard/index.html",
    "title": "Music Magician",
    "section": "",
    "text": "Music Magicians: Spotify Music Artist Influence Analytics Dashboard\n\nI developed an interactive analytics dashboard using Streamlit, Vega-Altair, Plotly, NetworkX, and Python. The dashboard focuses on exploring the evolution of music and quantifying artists‚Äô influence. It features informative visualizations and interactive elements to analyze music trends, artist characteristics, and the influence of past music on new compositions. This project successfully addressed the challenge by creating a user-friendly interface that facilitates in-depth exploration of the dataset, enabling storytelling and uncovering patterns and correlations in music."
  },
  {
    "objectID": "projects/2022-07-01-braillevoice/index.html",
    "href": "projects/2022-07-01-braillevoice/index.html",
    "title": "Braille Voice",
    "section": "",
    "text": "Project Poster\n\n\n\n\n\nDownload PDF file.\n\n\nProject Description\nA novel multi-stage pipeline (Mobile & Web app) for Braille-to-Text translation of whole single-sided printed braille documents with support for text-to-speech audio playback & text summarization in two languages: English & Tamil."
  },
  {
    "objectID": "projects/2021-05-01-orca/index.html",
    "href": "projects/2021-05-01-orca/index.html",
    "title": "ORCA: Underwater Robot",
    "section": "",
    "text": "Related Links & Artifacts\n\nWatch Demo\nIEEE Publication\nLab Website\nFunded: ‚Çπ9.5 Lakh INR\nüìÖ Project Duration: Jun 2019 - Jun 2022\n\n\n\nProject Overview\nA 500x350x210 mm inspection-class underwater robot equipped for hybrid manual-autonomous navigation. ORCA features a 6-thruster mobility suite, real-time telemetry, and multi-sensor feedback including pressure, temperature, leak, and IMU modules. This system integrates a joystick-driven manual mode and an autonomous navigation module built on MOOS-IvP (C++ mission planning suite from MIT).\n\n‚ú® Led a 7-member team to build and test ORCA‚Äôs software stack with kill-switch safety, homing, and GAN-enhanced visibility.\n‚ú® Achieved &gt;90% mission success across test runs with real-time diagnostics and fail-safe handling.\n‚ú® Integrated autonomous control, camera feedback, and tethered communication using custom C++ modules.\n\n\n\n\nTimeline of Contributions\n\n\n1. ORCA Robot: Joystick Operation\n\n\nDeveloped an autonomous + manual navigation stack on MOOS-IvP.\n\nBuilt joystick control, kill switch, IMU fusion, and thrust mapping systems.\n\nBench + pool tested with full sensor telemetry and thrust validation.\n\nPublished at IEEE and won the Asia-Pacific Region IEEE R10 UGPVC.\n\n\n\n\n2. KYOGRE: Autonomous Navigation Suite with Perception Sub-systems\n\n\nInternally funded with ‚Çπ30,000 (SSN IFSP).\n\nDeveloped deep RL-based waypoint control for UWVs.\n\nIntegrated into ORCA system as a future planning module.\n\n\n\n\n3. EyeSea: Real-Time Marine Threat Alerting Sub-system\n\n\nBuilt a YOLOv3 + MobileNetV2 pipeline to detect marine species threats.\n\nArduino-based IMU fusion + tethered alert system for swimmer safety.\n\nDeveloped for National Institute of Ocean Technology, winning Smart India Hackathon 2022 (Prelims).\n\n\n\n\n4. SatVision: Satellite-Based Built-Up Detection\n\n\nDeveloped a two-step segmentation and merging approach using satellite imagery.\n\nIdentified non-residential built-up clusters in dense Indian cities (e.g., Mumbai, Delhi).\n\nüèÖ Won Smart India Hackathon 2022 ‚Äì Software Edition, awarded ‚Çπ1,00,000 by Ministry of Earth Sciences.\n\n\n\n\n5. C-GAN for Enhanced Localization\n\n\nDesigned a Conditional GAN to refine robot localization sequences.\n\nCorrected drift and improved tracking precision for mission-critical tasks.\n\nContributed to robustness of multi-agent localization under low-visibility conditions.\n\n\n\n\nAwards & Recognition\n\nüèÜ IEEE R10 Undergraduate Project Video Contest Winner (Aug 2021)\n‚Äì Winner from 7 countries and 50+ councils\n‚Äì Project video featured on IEEE YouTube channel\n‚Äì $300 cash prize\nüèÖ Smart India Hackathon 2022 ‚Äì Software Edition\n‚Äì Winner for PSID: GR823 (SatVision Project)\n‚Äì ‚Çπ1,00,000 grant awarded by the Ministry of Earth Sciences, Government of India\nüí∞ Internally Funded Student Project (SSN College of Engineering)\n‚Äì ‚Çπ30,000 grant for KYOGRE (autonomous navigation sub-system for ORCA)\n\n\n\nTools & Frameworks\n\n\n\n\n\n\n\nSystem Layer\nTools / Frameworks\n\n\n\n\nNavigation & Control\nC++, MOOS-IvP, Arduino, Joystick\n\n\nComputer Vision\nYOLOv3, MobileNetV2, OpenCV, GAN, Real-time Video Analytics\n\n\nEmbedded Systems\nIMU Fusion, Leak Sensor, Pressure Sensor, Arduino Serial Comms\n\n\nRobotics\n6-Thruster Dynamics, Manual-Autonomous Switch, Kill Switch Logic\n\n\nDeployment & Monitoring\nTelemetry Monitoring, Underwater Testing, Sensor Drift Handling"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nivedhitha Dhanasekaran",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     Twitter\n  \n  \n    \n     GitHub\n  \n  \n    \n     ndhanase@alumni.cmu.edu\n  \n  \n    \n     Google Scholar\n  \n\n  \n  \n\nHi, I‚Äôm Nive! \nI‚Äôm a Software Engineer with a Master‚Äôs in Computational Data Science from Carnegie Mellon University. I specialize in building scalable ML systems at the intersection of Real-time Data Infrastructure, Intelligent Agents, and High-performance Computing Backend Platforms.\nMy experience spans end-to-end ML workflows, edge inference optimization, and LLM-based agents with structured reasoning. I‚Äôve contributed to distributed systems, observability pipelines, and database-driven ETL platforms using technologies like Kafka, Kubernetes, PyTorch, TensorFlow Lite, PostgreSQL, AWS, and GCP.\nAt Citi and Fidelity, I modernized infrastructure, boosted backend throughput, and automated secure, large-scale ETL pipelines powering critical analytics. I‚Äôm currently seeking roles in Software Engineering, ML Platforms, or Data Infrastructure, where I can build reliable and efficient systems at scale. Latest Resume\n\n\n\n\n\nüöÄ Actively Seeking Roles\n\nTarget Roles: Software Engineer,Machine Learning Engineer,Data Engineer,Data Scientist,Applied Scientist\nCore Focus Areas: Real-Time ETL & Observability, Distributed ML Systems, High-Performance Computing, Backend Infrastructure, LLM Agents & Tooling, On-Device AI (Wearables), Cloud-Native Pipelines (AWS, GCP, Kubernetes)\n\n\n\n‚ú® Highlights\n\n15 May 2025: My summer research was ACCEPTED at the Findings of the Association for Computational Linguistics 2025.\n11 May 2025: Graduated from the School of Computer Science, Carnegie Mellon University, completing my Master‚Äôs in Computational Data Science with dual concentrations in Analytics and Human-AI Interaction.\n\n13 April 2025: Winner of the AI Agents Weekend 2025 hackathon sponsored by Google & DeepMind for the Most Effective Project.\n\n13 March 2025: Filed Indian Patent for BrailleVoice (UG Capstone Project), an accessible voice-assisted Braille reader for the visually impaired."
  },
  {
    "objectID": "awards.html",
    "href": "awards.html",
    "title": "Honors & Awards",
    "section": "",
    "text": "AI Agents Hackathon: Most Effective Project Winner\nApr 2025 Google & DeepMind\nBuilt an LLM-based multi-agent system for contextual information synthesis and reasoning at Carnegie Mellon University."
  },
  {
    "objectID": "awards.html#section",
    "href": "awards.html#section",
    "title": "Honors & Awards",
    "section": "",
    "text": "AI Agents Hackathon: Most Effective Project Winner\nApr 2025 Google & DeepMind\nBuilt an LLM-based multi-agent system for contextual information synthesis and reasoning at Carnegie Mellon University."
  },
  {
    "objectID": "awards.html#section-1",
    "href": "awards.html#section-1",
    "title": "Honors & Awards",
    "section": "2022",
    "text": "2022\n\nSmart India Hackathon: Winner\nAug 2022 Ministry of Earth Sciences ‚Çπ1,00,000\nDeveloped a marine species threat detection system using real-time underwater vision.\nInternally Funded Student Project (IFSP)\nMay 2022 SSN College of Engineering ‚Çπ30,000\nLed the navigation software team to build ORCA and KYOGRE underwater robots under Dr.¬†S. Sakthivel Murugan\n UWARL Lab"
  },
  {
    "objectID": "awards.html#section-2",
    "href": "awards.html#section-2",
    "title": "Honors & Awards",
    "section": "2021",
    "text": "2021\n\nAIRSA Hackathon: Winner\nNov 2021\nProposed a deep learning model for satellite image segmentation.\n GitHub Repo\nAsian Machine Learning School + ACML: Selected Participant\nOct 2021\nInvited to join a high-impact ML education and research community across Asia-Pacific.\nIEEE R10 Student Project Video Contest: Asia-Pacific Champion\nAug 2021 $300\nWinner across 7 countries and 50+ student councils.\n Watch Video\nGrace Hopper Celebration (vGHC): Scholarship Awardee\nJul 2021\nSelected among 1,200 scholars globally for academic and community leadership.\nTechstars Startup Weekend Chennai: Runner-Up\nFeb 2021\nRanked 4th among 90+ idea pitches during a national startup sprint."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Nivedhitha Dhanasekaran",
    "section": "",
    "text": "üß† How I Work\nI‚Äôm an engineer who thrives at the intersection of Machine Learning Systems, High-performance Data & Computing Infrastructure, and Product-Focused Thinking. As an INTJ-A, I approach problems with a systems mindset-breaking down complexity into clear abstractions and scalable workflows.\nI operate with urgency and intention. I‚Äôm most engaged when iterating quickly, building reliable solutions that scale quietly behind the scenes, and turning ambiguous problems into actionable plans. Whether it‚Äôs an LLM agent, a real-time ETL pipeline, or an edge-deployed model, I enjoy driving ideas from prototype to production.\n\n\nüõ† Operating Principles\n\nOwn the Full Stack: From backend services to ML deployment, I take responsibility for system performance, reliability, and user experience.\nBuild for Scale and Clarity: I reduce operational overhead by designing interfaces and pipelines that are modular, observable, and easy to extend.\nBias for Iteration: I move fast, deliver early, and treat experimentation as a core part of engineering, not a separate step.\nKeep the End-User in Mind: Whether I‚Äôm building an ML co-pilot or internal tooling, I optimize for clarity, trust, and usefulness.\nLearn Fast, Dive Deep: I ramp quickly into unfamiliar domains, ask good questions, and continuously sharpen my technical judgment.\nDocument and Share: I make my thinking visible early, unblock teammates through clear async communication, and open-source when I can.\n\n\n\nüí° Personal Interests\nOutside of work, I enjoy rapid prototyping under constraints, whether it‚Äôs during hackathons (see Awards) or solo weekend builds.\nI‚Äôm a 108 WPM touch typist and a keyboard enthusiast who loves working close to the metal, especially when designing fast, usable command-line interfaces. In undergrad, I led a robotics team that developed two underwater navigation suites, KYOGRE and ORCA, for inspection-class autonomous marine vehicles.\nWhether it‚Äôs through competitive projects, emerging tech, or deep system dives, I‚Äôm driven by curiosity, craft, and the satisfaction of building things that work well at scale."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "leadership.html",
    "href": "leadership.html",
    "title": "Volunteering, Leadership & Teaching",
    "section": "",
    "text": "Jan‚ÄìDec 2024 CMU HCII\nGraduate Teaching Assistant, 05-839 Interactive Data Science\nLed a team of TAs for a 100+ student graduate course on data science, visualization, and app development.\n‚û§ Supervised assignments, tutorials, and capstones across Spring and Fall semesters.\n‚û§ Mentored projects on interactive tools using Python, SQL, Streamlit, Vega-Altair, and JS.\n‚û§ Integrated interpretability, A/B testing, and fairness tools through HCI research with DIG.\n‚û§ Managed infrastructure on GitHub Classroom and Canvas to streamline CI/CD workflows.\nSupervisor: Dr.¬†John Stamper\nApr 2024‚ÄìApr 2025 CMU LTI\nFLAME Seminar Committee Member\nCoordinated seminar logistics for the Foundation and Language Model (FLAME) series.\n‚û§ Managed backend operations including procurement, vendor communication, and faculty correspondence for faculty-hosted NLP seminars.\n‚û§ Ensured timely refreshments and room setup to support 50+ attendees and guest speakers."
  },
  {
    "objectID": "leadership.html#section",
    "href": "leadership.html#section",
    "title": "Volunteering, Leadership & Teaching",
    "section": "",
    "text": "Jan‚ÄìDec 2024 CMU HCII\nGraduate Teaching Assistant, 05-839 Interactive Data Science\nLed a team of TAs for a 100+ student graduate course on data science, visualization, and app development.\n‚û§ Supervised assignments, tutorials, and capstones across Spring and Fall semesters.\n‚û§ Mentored projects on interactive tools using Python, SQL, Streamlit, Vega-Altair, and JS.\n‚û§ Integrated interpretability, A/B testing, and fairness tools through HCI research with DIG.\n‚û§ Managed infrastructure on GitHub Classroom and Canvas to streamline CI/CD workflows.\nSupervisor: Dr.¬†John Stamper\nApr 2024‚ÄìApr 2025 CMU LTI\nFLAME Seminar Committee Member\nCoordinated seminar logistics for the Foundation and Language Model (FLAME) series.\n‚û§ Managed backend operations including procurement, vendor communication, and faculty correspondence for faculty-hosted NLP seminars.\n‚û§ Ensured timely refreshments and room setup to support 50+ attendees and guest speakers."
  },
  {
    "objectID": "leadership.html#section-1",
    "href": "leadership.html#section-1",
    "title": "Volunteering, Leadership & Teaching",
    "section": "2023",
    "text": "2023\n\nSep 2023 CMU SCS\nVolunteer, Pretty Good Race 2023\nSupported logistics for CMU‚Äôs community fun run hosted by the School of Computer Science.\n‚û§ Assisted in event setup, coordination, and finish-line result tracking, including timing runners and recording placements for over 120 participants."
  },
  {
    "objectID": "leadership.html#section-2",
    "href": "leadership.html#section-2",
    "title": "Volunteering, Leadership & Teaching",
    "section": "2022",
    "text": "2022\n\nMay‚ÄìSep 2022 SSNCE\nTeaching Assistant, C Programming Course\nDelivered a structured certificate course with weekly classes, assignments, and projects.\n‚û§ Created code walkthroughs and a project-based evaluation system for merit certification.\n GitHub Repo\nApr‚ÄìJun 2022 SSNCE\nChief Editor, Smriti Newsletter\nOversaw writing, editing, and design for institutional print & digital distribution.\n‚û§ Managed deadlines, proofing, and creative production workflows.\n Publication"
  },
  {
    "objectID": "leadership.html#section-3",
    "href": "leadership.html#section-3",
    "title": "Volunteering, Leadership & Teaching",
    "section": "2021",
    "text": "2021\n\nAug 2021‚ÄìJun 2022 SSNCE\nSecretary, Assoc. of Computer Engineers (ACE)\nLed a team that organized 8+ events and launched a flagship hackathon at Invente 6.0.\n‚û§ Raised ‚Çπ2.3 lakhs in sponsorship, handled marketing, editorial, and design.\n Certificate\nJul 2020‚ÄìApr 2022 SSN ACM Chapter\nSecretary, ACM Student Chapter\nCoordinated chapter operations across 9+ events and led content, PR, and media teams.\n Events\nMay‚ÄìAug 2021 WomenTech Network\nGlobal Ambassador\nRepresented the global women-in-tech movement and promoted diversity and inclusion.\n‚û§ Participated in the annual leadership summit and community-driven campaigns."
  },
  {
    "objectID": "leadership.html#section-4",
    "href": "leadership.html#section-4",
    "title": "Volunteering, Leadership & Teaching",
    "section": "2020",
    "text": "2020\n\nDec 2018‚ÄìJul 2020 National Service Scheme\nStudent Volunteer\nCompleted 85+ hours of service in sustainability, documentation, and outreach events.\n‚û§ Led initiatives on campus cleanups, gardening, and community workshops."
  },
  {
    "objectID": "projects/2021-06-30-flight-delay-prediction/index.html",
    "href": "projects/2021-06-30-flight-delay-prediction/index.html",
    "title": "Two Stage Flight Delay Prediction",
    "section": "",
    "text": "Flight Delay Prediction\n\nI designed a two-stage predictive machine learning engine to forecast the on-time performance of flights at 15 different USA airports using 2016-2017 data. This project involved data cleaning, pre-processing, and merging flight and weather data. To address flight delays, the engine first classifies whether a flight will be delayed and then predicts the arrival delay in minutes for delayed flights. Due to class imbalance favoring on-time flights, SMOTE sampling was used before classification. The Random Forest classifier achieved the best performance with an F1 score of 0.78 and a Recall of 0.74 for delayed flights. For regression, the Random Forest regressor yielded an MAE of 7.178 minutes, an RMSE of 11.283 minutes, and an R-squared score of 0.977.\nDownload PDF file."
  },
  {
    "objectID": "projects/2023-02-17-gca/index.html",
    "href": "projects/2023-02-17-gca/index.html",
    "title": "Automated Detection of Giant Cell Arteritis from Temporal Artery Biopsy Specimens Using Deep Learning",
    "section": "",
    "text": "Related Links & Artifacts\n\nGitHub Repository\nDownload Poster (PDF)\nRead on IOVS (ARVO 2022)\nDr.¬†Naveena Yanamala\nüìÖ Project Duration: Jan 2021 ‚Äì Jul 2022\n\n\n\nProject Overview\nThis project was developed at the intersection of medical imaging and applied deep learning, in collaboration with pathologists from Carnegie Mellon University, Rutgers and Tulane. The work focused on building an end-to-end diagnostic system that brings explainable, high-performance ML models into digital pathology pipelines for vascular autoimmune disorders like Giant Cell Arteritis (GCA).\n\n‚ú® Built a fully automated ML pipeline achieving 92.32% ROI-level accuracy and 0.93 AUC for inflammation detection in digitized biopsy slides.\n‚ú® Trained and deployed a ResNet-34 classifier optimized for low-latency inference (168 ms/16 ROIs) using clinical-grade image preprocessing.\n‚ú® Published and presented results at ARVO 2022, with GradCAM visual validation accepted by board-certified ophthalmic pathologists.\n\n\nFUNDED BY:  NIH U54 GM104942 Oliver and Carroll Dabezies Tulane Endowed Chair\n\n\n\nDescription\nThis project automated the detection of Giant Cell Arteritis (GCA) from digitized temporal artery biopsy (TAB) slides, spanning every stage from raw data handling to clinical deployment readiness and publication. I collaborated directly with clinicians, developed the ML pipeline, validated it with expert feedback, and co-authored the peer-reviewed publication.\nDownload PDF file.\n\n\n1. Data Collection & Expert Labeling\n\n\nCurated a dataset of 472 high-resolution TAB slides spanning 20 years (2000‚Äì2019).\n\nParticipated in quality control and slide review, excluding 192 suboptimal samples.\n\nCollaborated with ophthalmic pathologists to annotate slides with binary GCA labels, forming the ground truth.\n\n\n\n\n2. Image Preprocessing\n\n\nDeveloped a CV pipeline to automatically detect artery regions from stain intensity.\n\nExtracted and padded 3,558 ROIs to standardized 512√ó512 px RGB tiles.\n\nApplied color jittering and rotation (0¬∞, 90¬∞, 180¬∞, 270¬∞) for augmentation and rotational invariance.\n\n\n\n\n3. Model Development\n\n\nTrained a ResNet-34 model fine-tuned on ImageNet for binary GCA classification.\n\nDesigned a dual-level prediction strategy: ROI-level and slide-level classification.\n\nBalanced model complexity and latency to enable fast inference (~168 ms for 16 ROIs) on an Apple M1 GPU.\n\n\n\n\n4. Validation & Explainability\n\n\nIntegrated GradCAM to generate region-wise visual explanations for clinical review.\n\nValidated that heatmaps aligned with features like lymphocytic infiltrates and giant cells.\n\nWorked with pathologists to verify that model attention matched diagnostic hotspots in expert-labeled slides.\n\n\n\n\n5. Model Optimization\n\n\nBenchmarked performance across ResNet-18, 34, and 50, selecting ResNet-34 for its 92.32% accuracy and 0.93 AUC on the held-out 2019 test set.\n\nTuned augmentation, data balancing, and inference pipeline for clinical deployment feasibility.\n\nDemonstrated robust performance across time-split validation, reinforcing generalizability.\n\n\n\n\n6. Publication & Reporting\n\n\nCo-authored and published results in Investigative Ophthalmology & Visual Science (ARVO 2022)\n\nDesigned all pipeline diagrams, GradCAM visualizations, and tables for the final report and poster.\n\nCreated the project poster for dissemination at conferences and institutional reviews.\n\n\n\n\nTools & Frameworks\n\n\n\n\n\n\n\nResearch Focus\nStack / Tools Used\n\n\n\n\nDeep Learning Pipelines\nPyTorch, Torchvision, ImageNet, Transfer Learning, scikit-learn\n\n\nMedical Image Preprocessing\nOpenSlide, PIL, OpenCV, skimage, IBM H&E Normalization, ROI Extraction, optparse\n\n\nExplainability & Debugging\nGradCAM, pytorch-grad-cam, Matplotlib, Seaborn\n\n\nHigh-Performance Inference\nBatch Processing, Multi-threading, GPU Training, WSI Multiprocessing\n\n\nDeployment Readiness\nPipeline Automation (CMD), PDF Reporting, Clinical Evaluation Integration"
  },
  {
    "objectID": "projects/2024-05-03-ucre-project-portfolio/index.html",
    "href": "projects/2024-05-03-ucre-project-portfolio/index.html",
    "title": "Harnessing Everyday Users‚Äô Power to Detect Harmful Behaviors in GenAI",
    "section": "",
    "text": "In the Spring of 2024, I worked on a semester-long project for the 05-610 User-centered Research and Evaluation course as part of my Human-centered Data Science (Human Computer Interaction) concentration for the Master of Computational Data Science program at Carnegie Mellon University.\n\n\n\n\n  \n\n\n\n\n\n\nThe ‚ÄúEngage to Change: Rewarding User Reports for Bias Mitigation in Generative AI‚Äù project focuses on mitigating generative AI (GenAI) bias by rewarding users for reporting biases.\n\nObjective: The project aims to transform every AI interaction into an opportunity for eliminating bias, addressing the challenge that users often do not report biases due to cumbersome reporting mechanisms, privacy concerns, and a lack of motivation.\nMethodology: I employed various methods such as contextual inquiry, affinity clustering, prototyping, user flow modeling, and data analysis. These helped understand user behavior and design effective solutions.\nSolution: The core innovation is a digital incentive program where users earn tokens for reporting biases. These tokens can be exchanged for service upgrades or other rewards, integrating a sense of progress and achievement into the reporting process.\nDesign Features: The project introduces non-disruptive, context-aware reminders, robust privacy protections, and an intuitive, effortless UI/UX. These features are not only intended to encourage user participation but also to reassure them about the simplicity, security, and privacy of the reporting process.\nUser Feedback: Initial feedback indicates that users are motivated by rewards such as monetary incentives or platform credits. Strategic reminders and the potential to earn rewards are significant motivators for consistent engagement in bias reporting.\nImpact: The project underscores the transformative potential of every user‚Äôs contribution, with the ultimate goal of creating smarter, fairer, and bias-free AI solutions. This emphasis on fairness and bias mitigation inspires confidence in the proposed approach.\n\n\n\n\nIn this project, I had the opportunity to immerse myself in multiple roles, including that of a User Researcher, UI/UX designer, Data Scientist, and Project Manager. I embraced a multifaceted role that spanned several disciplines, allowing me to delve deeply into human-centered research. As a User Researcher, I planned and implemented a mixed-methods approach to gather insights from user study participants. This involved conducting observational fieldwork, utilizing interview techniques to uncover users‚Äô needs and motivations, and collecting quantitative data from both systems and their users. As a Data Scientist, my role also extended to analyzing this diverse data quantitatively to find patterns in behaviors, motivations, and unmet needs. Synthesizing these insights, I envisioned new systems to meet these identified needs. As a UI/UX Designer, I developed conceptual designs and prototypes, further enhancing my contributions by evaluating these through various research methods to ensure they met user requirements effectively. As a Project Manager, I oversaw the project‚Äôs progress, ensuring that research findings were communicated effectively to all stakeholders, from study participants to research team members, fostering a collaborative and informed project environment.\n\n\n\n\n\nBefore initiating the project, I conducted thorough preliminary research to build a solid foundation of knowledge and keep existing solutions distinct. My exploration into ‚ÄúHarnessing Everyday Users‚Äô Power to Detect Harmful Behaviors in Generative AI‚Äù involved experiential and informational searches.\nI gained firsthand experience by observing how my friends interacted with generative AI platforms and by experimenting with new technologies myself, such as Microsoft Co-pilot and various GPT models on ChatGPT 4.0, along with Midjourney. This allowed me to understand the user experience directly and explore emerging questions in everyday algorithm auditing.\nAdditionally, I consulted various sources, including news articles, academic journals, social media, online blogs, and national reports, to comprehensively understand the subject. From this research, I distilled critical insights into a brief report and organized this information visually on a Figma Jamboard.\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDownload PDF file.\n\n\n\nFollowing the initial research phase, I delved into domain-specific data provided by the teaching staff. This data consisted of aggregated results from 2,197 survey responses collected by the WeAudit team at CMU. This survey assessed how individuals‚Äô identities, experiences, and knowledge affect their ability to detect harmful algorithmic biases in image search contexts.\nTo analyze this data, I first converted it into a CSV file to facilitate exploratory and quantitative analysis using Python. I undertook data-wrangling steps to isolate the necessary columns for deeper analysis. Given the text-based nature of the data, standard analytics and visualizations took time to apply. I had to preprocess the data and engineer features to prepare it for meaningful analysis.\nI then created heatmaps to explore correlations between age groups, users‚Äô familiarity with digital media, and their likelihood of encountering biases. Additionally, I employed word clouds and topic modeling to derive more detailed insights from the data.\nDownload PDF file.\n\n\n\n\n\n\n\n\n\n\n\nUsers aged 25-34 and those 65 and above show noticeable patterns in encountering biases‚Äîmonthly and a few times a year, respectively. These findings suggest that user-auditing frequency might need adjustment to prevent user overload when interacting with news articles and testing systems.\nMost age groups, including those 55 and older, are familiar with using algorithmic systems. This observation requires further investigation, as it may be influenced by desirability bias or sampling errors inherent in self-reported data.\nThere appears to be a weak correlation between the frequency of using algorithmic systems and encountering algorithmic biases, suggesting that familiarity does not necessarily predict bias encounter frequency. This relationship warrants further validation.\nNo clear correlation was found between users‚Äô location and their sensitivity to bias. Sensitivity levels were consistent across different regions, although the West and Midwest were underrepresented in the survey data, indicating a need for broader geographic representation in future studies.\n\n\n\n\n\nTo assess how current solutions integrate bias reporting mechanisms that empower everyday users to help mitigate biases, I conducted a heuristic evaluation of the WeAudit TAIGA Tool, using Nielsen‚Äôs Ten Usability Heuristics.\nThis evaluation examined the system‚Äôs design and usability to determine how effectively it enables users to identify harmful algorithmic behaviors in image-generation AI systems. The key functionalities I assessed included:\n\nThe ability for users to input a search prompt and receive relevant images.\nUsers can review the returned images, highlight specific ones, and add comments.\nThe feature allows users to initiate discussion threads, which can be posted on the WeAudit forums.\n\n\n\nDownload PDF file.\n\n\n\nI conducted three usability tests on the WeAudit TAIGA tool to achieve the following objectives: - Identify specific challenges within the UI/UX when participants interact with TAIGA and other Generative AI platforms.\nDownload PDF file.\n\nGauge users‚Äô logical and emotional responses while interacting with websites like TAIGA.\nExplore user perceptions and experiences concerning bias.\nObserve user behavior concerning reporting biases via TAIGA and other GenAI platforms.\nCollect basic demographic information about the participants.\nAssess demographic groups‚Äô preferences regarding specific topics.\n\nTo facilitate these interviews, I developed a script to guide the discussions and ensured that consent forms were briefed to and collected from all participants before beginning the study. Additionally, I created an optional survey for participants to provide demographic information. This was designed to explore potential correlations between these demographic variables and participants‚Äô propensity to report biases influenced by previous experiences.\nDownload PDF file.\nThese interviews provided insight into the primary challenges participants encounter with the current bias reporting mechanisms, their feelings towards algorithmic biases, and the suitability of a collaborative approach for addressing these issues.\nDownload PDF file.\n\n\n\nWalking the Wall is a way of re-immersing your team in the data and the analysis you have performed on it. I collected all my data from the studies conducted so far and walked the wall to reframe and redefine the problem by focusing on the following questions:\n\n\n\n\n  \n\nWhat‚Äôs going on here?\nWhat does the user need?\nWhat can we do about it?\nQuestions to conduct further research and conduct data!\n\n\n\n\nAfter identifying users‚Äô pain points and needs regarding current methods for engaging everyday users in bias mitigation, I refined our broad objective into more targeted research questions in the form of How Might We (HMW) questions.\nTo redefine the primary research problem, I challenged the existing assumptions to pinpoint opportunities by applying ‚ÄúReverse Assumptions,‚Äù selected the most critical one, and recast the problem statement. This led to evolving our overarching aim from ‚ÄúHarnessing Everyday Users‚Äô Power to Detect Harmful Behaviors in Generative AI‚Äù to ‚ÄúHow Might We Transform AI Interactions Into Opportunities for Bias Elimination by Incentivizing Users and Reducing Cognitive Overload to Simplify the Reporting Process?‚Äù\nDownload PDF file.\n\n\n\nI conducted additional contextual inquiry interviews to develop new qualitative insights aligned with our more focused research goals. These interviews centered around ChatGPT, utilizing it as the primary generative AI platform for our study, aiming to address specific research questions that correlate with our outlined goals:\n\nGoal #1: To improve user awareness, enhance the quality of reflection on system responses, and identify algorithmic biases within generative AI systems like Microsoft Copilot and ChatGPT.\nGoal #2: Develop strategies to motivate users to report biases they naturally encounter in AI-generated content, exploring intrinsic motivation and possibly gamifying the reporting process.\n\nDownload PDF file.\n\n\n\nWhat types of guidance and feedback are most effective in aiding users in detecting and reporting biases?\nHow can we effectively educate users on the nature and existence of algorithmic biases within generative AI systems?\nWhat design features in the user interface can encourage users to examine the responses they receive from generative AI systems critically?\nHow can feedback mechanisms be seamlessly integrated into generative AI platforms to facilitate straightforward reporting of detected biases by users?\nHow can community-driven platforms improve everyday users‚Äô detection and reporting of algorithmic biases?\n\nDownload PDF file.\n\n\n\n\nAfter completing the interviews, I converted the session notes into interpretation notes. Using these notes, the team engaged in Affinity Clustering to organize the yellow interpretation notes into significant themes, ideas, and overall user concerns. We categorized these using blue, pink, and green labels to group them into a hierarchical structure that narratively outlines the overall user experience. To deepen our understanding of the data collected from the interviews, we constructed two models: an empathy map and a user journey flow map. These models helped us view the information from various perspectives, enhancing our comprehension of user experiences and interactions.\nDownload PDF file.\n\n\n\n\nHow can we make it more natural for users to report AI-generated bias?\n\nTo address the challenge of user unawareness and engagement with AI-generated biases, our initiative seeks to simplify the process for users to identify and report these biases. Recognizing that users often overlook or don‚Äôt critically evaluate AI biases, which affect the fairness and accuracy of AI results, we aim to enhance user awareness and interaction. We aimed to design intuitive user interfaces that encourage reflection, incorporate easy-to-use feedback mechanisms, and utilize common user behaviors to motivate more consistent auditing and reporting of biases.\n\nHow can we use current user interactions to unexpected responses to motivate user auditing and reporting?\n\nFurthermore, the interest in using current user reactions to unexpected responses as a catalyst for auditing and reporting behavior highlights an innovative approach to enhancing user participation in the quality control of AI outputs. Recognizing that users typically opt for re-prompting when faced with unsatisfactory AI responses, the goal is to integrate design approaches that make feedback provision a seamless and intuitive part of the user experience.\n\n\n\n\n\n\nWhat forms of guidance and feedback are most effective for supporting users in detecting and reporting biases?\nHow can we effectively educate users about the nature and presence of algorithmic biases within generative AI systems?\nWhat design elements in the user interface can prompt users to critically reflect on the responses they receive from generative AI systems?\nHow can feedback mechanisms be integrated into generative AI platforms to facilitate easy reporting of detected biases by users?\nHow can community-driven platforms enhance everyday users‚Äô detection and reporting of algorithmic biases?\n\n\n\n\n\nUsers do not prioritize identifying biases in GenAI outputs, as their primary focus is leveraging AI to support everyday tasks.\nThe current reporting mechanism is unnatural and doesn‚Äôt fit into the natural workflow of users as they typically resort to re-prompting as an immediate solution to unexpected or unsatisfactory GenAI responses, sometimes even before the generation process is complete by interrupting the flow instead of looking for features to report this behavior.\nUser apprehensions about anonymity and privacy loom significant when reporting biases, underscoring a critical barrier to transparency and accountability in addressing GenAI biases.\nUser sensitivity to biases in real life has little influence on reporting behavior since algorithmic biases don‚Äôt stand out similarly by eliciting a negative emotional response unless it is pronounced. Users need to be prompted or reminded to look for them in the responses - the more natural interpretation of results to look for how much the response matches their expectations.\nThe reminder strategy and effort required to provide feedback through UI/UX elements on different GenAI tools determine the likelihood of getting user feedback.\n\n\n\n\n\n\nAfter prioritizing the user needs from the contextual inquiry interviews through synthesis methods like affinity clustering and walking the wall again by adding new evidence, I rapidly prototyped solutions using the crazy 8s method. The goal of this activity is to brainstorm a set of ideas that meet user needs inspired by your analysis and reflection on your Walk the Wall activity.\nDownload PDF file.\nAfter analyzing the user breakdowns/needs, approaches, ideas, and unanswered questions raised in Crazy 8‚Äôs activity to identify the greatest areas of uncertainty and risk, we generated a list of user needs as a team from this analysis. Then, each team member selected one user need and created a set of three storyboards, each riskier than the previous.\nUsing these storyboards representing possible solution directions, I conducted speed-dating interviews with participants to rapidly explore design futures and prioritize the needs that appear strongly in user research and speed-dating sessions to reveal new design opportunities:\n\nToken Collection Strategy: Earn tokens for bias reports valued by usefulness (1-5 tokens); redeem 200 tokens for 15 free GPT 4.0 prompts or 500 for a week-long GPT 4.0 upgrade.\nCallout Reminder Strategy: Utilize a pop-up tool for users to critically evaluate AI responses, enabling tagging of biases in images or highlighting in text.\nMonetary Reward for Bias Reporting: Offer financial incentives for routine bias reporting or during major incidents, with a simple button or hashtag for direct reporting to social media.\nRecaptcha-Style Engagement Checks: Implement intermittent, non-intrusive prompts to verify user engagement and foster ongoing attention to detail\n\n\n\n\nFinally, I prioritized the token strategy idea, which received the most favorable feedback from the audience, and developed a low-fidelity prototype. I then conducted an initial interview with a participant to assess the prototype‚Äôs usefulness and its alignment with user needs. The participant was pleased with the new approach. Although there is still potential for enhancement, the prototype successfully achieved its goal of encouraging users to report biases in exchange for platform credits. Subsequently, I translated this prototype into a high-fidelity version using Figma.\n  \n\n\n\n\nWe presented our project findings during the course‚Äôs final poster session and received substantial feedback for future enhancements. Overall, the feedback was positive, and we were excited by the attendees‚Äô engagement level.\nDownload PDF file.\n\n\n\nDuring this project, we didn‚Äôt have time to conduct a comprehensive smoke test to verify user engagement increases after implementing the token strategy. I would love to spend more time in understanding how much time and effort users are willing to pay in exchange for platform credits. I also want to quantitatively determine the thresholds at which this scheme is feasible for companies to implement without incurring losses and simultaneously improving user engagement with bias reporting.\n\n\n\nWhen I started this course, my three primary learning objectives were:\n\nUnderstanding how user research is formally conducted and implementing it through the group project complement my second concentration in Human-centered Data Science.\nBecoming comfortable working with and speaking the language of designers and UX researchers to be a better software engineer and data scientist.\nImprove my design thinking skills, visualize data, and rapidly prototype low and high-fidelity designs.\n\nThroughout the group project, I learned the iterative nature of problem-solving, which involves continuous information gathering, problem redefinition, and solution exploration. Our diverse methods provided me with a comprehensive set of UX skills, enriching my approach to future projects. Notably, during the final poster session, I utilized Figma to create low and high-fidelity prototypes for our token strategy, an experience that expanded my technical toolkit. The course readings proved essential, deepening my understanding of each research method‚Äôs nuances. The practical application of synthesis methods like Affinity Clustering, Walk-the-Wall, and Speed Dating significantly enhanced my ability to reassess problems and discover varied solutions within the design realm. Additionally, analyzing complex and unstructured text data sharpened my data wrangling and visualization capabilities.\nThis project revealed that synthesis is a nuanced and intricate process. While research methods may appear straightforward in theory, their practical application is challenging, often requiring a shift in perspective to unearth valuable insights. This experience was gratifying, allowing me to view product development through the lens of a product manager and a UI/UX designer beyond my engineering and data science background. I am grateful to have met all my initial learning goals and feel more confident in my professional abilities. A heartfelt thank you to the teaching team for a truly enriching experience!"
  },
  {
    "objectID": "projects/2024-05-03-ucre-project-portfolio/index.html#engage-to-change-rewarding-user-reports-for-bias-mitigation-in-generative-ai",
    "href": "projects/2024-05-03-ucre-project-portfolio/index.html#engage-to-change-rewarding-user-reports-for-bias-mitigation-in-generative-ai",
    "title": "Harnessing Everyday Users‚Äô Power to Detect Harmful Behaviors in GenAI",
    "section": "",
    "text": "The ‚ÄúEngage to Change: Rewarding User Reports for Bias Mitigation in Generative AI‚Äù project focuses on mitigating generative AI (GenAI) bias by rewarding users for reporting biases.\n\nObjective: The project aims to transform every AI interaction into an opportunity for eliminating bias, addressing the challenge that users often do not report biases due to cumbersome reporting mechanisms, privacy concerns, and a lack of motivation.\nMethodology: I employed various methods such as contextual inquiry, affinity clustering, prototyping, user flow modeling, and data analysis. These helped understand user behavior and design effective solutions.\nSolution: The core innovation is a digital incentive program where users earn tokens for reporting biases. These tokens can be exchanged for service upgrades or other rewards, integrating a sense of progress and achievement into the reporting process.\nDesign Features: The project introduces non-disruptive, context-aware reminders, robust privacy protections, and an intuitive, effortless UI/UX. These features are not only intended to encourage user participation but also to reassure them about the simplicity, security, and privacy of the reporting process.\nUser Feedback: Initial feedback indicates that users are motivated by rewards such as monetary incentives or platform credits. Strategic reminders and the potential to earn rewards are significant motivators for consistent engagement in bias reporting.\nImpact: The project underscores the transformative potential of every user‚Äôs contribution, with the ultimate goal of creating smarter, fairer, and bias-free AI solutions. This emphasis on fairness and bias mitigation inspires confidence in the proposed approach."
  },
  {
    "objectID": "projects/2024-05-03-ucre-project-portfolio/index.html#roles-in-the-project",
    "href": "projects/2024-05-03-ucre-project-portfolio/index.html#roles-in-the-project",
    "title": "Harnessing Everyday Users‚Äô Power to Detect Harmful Behaviors in GenAI",
    "section": "",
    "text": "In this project, I had the opportunity to immerse myself in multiple roles, including that of a User Researcher, UI/UX designer, Data Scientist, and Project Manager. I embraced a multifaceted role that spanned several disciplines, allowing me to delve deeply into human-centered research. As a User Researcher, I planned and implemented a mixed-methods approach to gather insights from user study participants. This involved conducting observational fieldwork, utilizing interview techniques to uncover users‚Äô needs and motivations, and collecting quantitative data from both systems and their users. As a Data Scientist, my role also extended to analyzing this diverse data quantitatively to find patterns in behaviors, motivations, and unmet needs. Synthesizing these insights, I envisioned new systems to meet these identified needs. As a UI/UX Designer, I developed conceptual designs and prototypes, further enhancing my contributions by evaluating these through various research methods to ensure they met user requirements effectively. As a Project Manager, I oversaw the project‚Äôs progress, ensuring that research findings were communicated effectively to all stakeholders, from study participants to research team members, fostering a collaborative and informed project environment."
  },
  {
    "objectID": "projects/2024-05-03-ucre-project-portfolio/index.html#methodology",
    "href": "projects/2024-05-03-ucre-project-portfolio/index.html#methodology",
    "title": "Harnessing Everyday Users‚Äô Power to Detect Harmful Behaviors in GenAI",
    "section": "",
    "text": "Before initiating the project, I conducted thorough preliminary research to build a solid foundation of knowledge and keep existing solutions distinct. My exploration into ‚ÄúHarnessing Everyday Users‚Äô Power to Detect Harmful Behaviors in Generative AI‚Äù involved experiential and informational searches.\nI gained firsthand experience by observing how my friends interacted with generative AI platforms and by experimenting with new technologies myself, such as Microsoft Co-pilot and various GPT models on ChatGPT 4.0, along with Midjourney. This allowed me to understand the user experience directly and explore emerging questions in everyday algorithm auditing.\nAdditionally, I consulted various sources, including news articles, academic journals, social media, online blogs, and national reports, to comprehensively understand the subject. From this research, I distilled critical insights into a brief report and organized this information visually on a Figma Jamboard.\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDownload PDF file.\n\n\n\nFollowing the initial research phase, I delved into domain-specific data provided by the teaching staff. This data consisted of aggregated results from 2,197 survey responses collected by the WeAudit team at CMU. This survey assessed how individuals‚Äô identities, experiences, and knowledge affect their ability to detect harmful algorithmic biases in image search contexts.\nTo analyze this data, I first converted it into a CSV file to facilitate exploratory and quantitative analysis using Python. I undertook data-wrangling steps to isolate the necessary columns for deeper analysis. Given the text-based nature of the data, standard analytics and visualizations took time to apply. I had to preprocess the data and engineer features to prepare it for meaningful analysis.\nI then created heatmaps to explore correlations between age groups, users‚Äô familiarity with digital media, and their likelihood of encountering biases. Additionally, I employed word clouds and topic modeling to derive more detailed insights from the data.\nDownload PDF file.\n\n\n\n\n\n\n\n\n\n\n\nUsers aged 25-34 and those 65 and above show noticeable patterns in encountering biases‚Äîmonthly and a few times a year, respectively. These findings suggest that user-auditing frequency might need adjustment to prevent user overload when interacting with news articles and testing systems.\nMost age groups, including those 55 and older, are familiar with using algorithmic systems. This observation requires further investigation, as it may be influenced by desirability bias or sampling errors inherent in self-reported data.\nThere appears to be a weak correlation between the frequency of using algorithmic systems and encountering algorithmic biases, suggesting that familiarity does not necessarily predict bias encounter frequency. This relationship warrants further validation.\nNo clear correlation was found between users‚Äô location and their sensitivity to bias. Sensitivity levels were consistent across different regions, although the West and Midwest were underrepresented in the survey data, indicating a need for broader geographic representation in future studies.\n\n\n\n\n\nTo assess how current solutions integrate bias reporting mechanisms that empower everyday users to help mitigate biases, I conducted a heuristic evaluation of the WeAudit TAIGA Tool, using Nielsen‚Äôs Ten Usability Heuristics.\nThis evaluation examined the system‚Äôs design and usability to determine how effectively it enables users to identify harmful algorithmic behaviors in image-generation AI systems. The key functionalities I assessed included:\n\nThe ability for users to input a search prompt and receive relevant images.\nUsers can review the returned images, highlight specific ones, and add comments.\nThe feature allows users to initiate discussion threads, which can be posted on the WeAudit forums.\n\n\n\nDownload PDF file.\n\n\n\nI conducted three usability tests on the WeAudit TAIGA tool to achieve the following objectives: - Identify specific challenges within the UI/UX when participants interact with TAIGA and other Generative AI platforms.\nDownload PDF file.\n\nGauge users‚Äô logical and emotional responses while interacting with websites like TAIGA.\nExplore user perceptions and experiences concerning bias.\nObserve user behavior concerning reporting biases via TAIGA and other GenAI platforms.\nCollect basic demographic information about the participants.\nAssess demographic groups‚Äô preferences regarding specific topics.\n\nTo facilitate these interviews, I developed a script to guide the discussions and ensured that consent forms were briefed to and collected from all participants before beginning the study. Additionally, I created an optional survey for participants to provide demographic information. This was designed to explore potential correlations between these demographic variables and participants‚Äô propensity to report biases influenced by previous experiences.\nDownload PDF file.\nThese interviews provided insight into the primary challenges participants encounter with the current bias reporting mechanisms, their feelings towards algorithmic biases, and the suitability of a collaborative approach for addressing these issues.\nDownload PDF file.\n\n\n\nWalking the Wall is a way of re-immersing your team in the data and the analysis you have performed on it. I collected all my data from the studies conducted so far and walked the wall to reframe and redefine the problem by focusing on the following questions:\n\n\n\n\n  \n\nWhat‚Äôs going on here?\nWhat does the user need?\nWhat can we do about it?\nQuestions to conduct further research and conduct data!\n\n\n\n\nAfter identifying users‚Äô pain points and needs regarding current methods for engaging everyday users in bias mitigation, I refined our broad objective into more targeted research questions in the form of How Might We (HMW) questions.\nTo redefine the primary research problem, I challenged the existing assumptions to pinpoint opportunities by applying ‚ÄúReverse Assumptions,‚Äù selected the most critical one, and recast the problem statement. This led to evolving our overarching aim from ‚ÄúHarnessing Everyday Users‚Äô Power to Detect Harmful Behaviors in Generative AI‚Äù to ‚ÄúHow Might We Transform AI Interactions Into Opportunities for Bias Elimination by Incentivizing Users and Reducing Cognitive Overload to Simplify the Reporting Process?‚Äù\nDownload PDF file.\n\n\n\nI conducted additional contextual inquiry interviews to develop new qualitative insights aligned with our more focused research goals. These interviews centered around ChatGPT, utilizing it as the primary generative AI platform for our study, aiming to address specific research questions that correlate with our outlined goals:\n\nGoal #1: To improve user awareness, enhance the quality of reflection on system responses, and identify algorithmic biases within generative AI systems like Microsoft Copilot and ChatGPT.\nGoal #2: Develop strategies to motivate users to report biases they naturally encounter in AI-generated content, exploring intrinsic motivation and possibly gamifying the reporting process.\n\nDownload PDF file.\n\n\n\nWhat types of guidance and feedback are most effective in aiding users in detecting and reporting biases?\nHow can we effectively educate users on the nature and existence of algorithmic biases within generative AI systems?\nWhat design features in the user interface can encourage users to examine the responses they receive from generative AI systems critically?\nHow can feedback mechanisms be seamlessly integrated into generative AI platforms to facilitate straightforward reporting of detected biases by users?\nHow can community-driven platforms improve everyday users‚Äô detection and reporting of algorithmic biases?\n\nDownload PDF file.\n\n\n\n\nAfter completing the interviews, I converted the session notes into interpretation notes. Using these notes, the team engaged in Affinity Clustering to organize the yellow interpretation notes into significant themes, ideas, and overall user concerns. We categorized these using blue, pink, and green labels to group them into a hierarchical structure that narratively outlines the overall user experience. To deepen our understanding of the data collected from the interviews, we constructed two models: an empathy map and a user journey flow map. These models helped us view the information from various perspectives, enhancing our comprehension of user experiences and interactions.\nDownload PDF file.\n\n\n\n\nHow can we make it more natural for users to report AI-generated bias?\n\nTo address the challenge of user unawareness and engagement with AI-generated biases, our initiative seeks to simplify the process for users to identify and report these biases. Recognizing that users often overlook or don‚Äôt critically evaluate AI biases, which affect the fairness and accuracy of AI results, we aim to enhance user awareness and interaction. We aimed to design intuitive user interfaces that encourage reflection, incorporate easy-to-use feedback mechanisms, and utilize common user behaviors to motivate more consistent auditing and reporting of biases.\n\nHow can we use current user interactions to unexpected responses to motivate user auditing and reporting?\n\nFurthermore, the interest in using current user reactions to unexpected responses as a catalyst for auditing and reporting behavior highlights an innovative approach to enhancing user participation in the quality control of AI outputs. Recognizing that users typically opt for re-prompting when faced with unsatisfactory AI responses, the goal is to integrate design approaches that make feedback provision a seamless and intuitive part of the user experience.\n\n\n\n\n\n\nWhat forms of guidance and feedback are most effective for supporting users in detecting and reporting biases?\nHow can we effectively educate users about the nature and presence of algorithmic biases within generative AI systems?\nWhat design elements in the user interface can prompt users to critically reflect on the responses they receive from generative AI systems?\nHow can feedback mechanisms be integrated into generative AI platforms to facilitate easy reporting of detected biases by users?\nHow can community-driven platforms enhance everyday users‚Äô detection and reporting of algorithmic biases?\n\n\n\n\n\nUsers do not prioritize identifying biases in GenAI outputs, as their primary focus is leveraging AI to support everyday tasks.\nThe current reporting mechanism is unnatural and doesn‚Äôt fit into the natural workflow of users as they typically resort to re-prompting as an immediate solution to unexpected or unsatisfactory GenAI responses, sometimes even before the generation process is complete by interrupting the flow instead of looking for features to report this behavior.\nUser apprehensions about anonymity and privacy loom significant when reporting biases, underscoring a critical barrier to transparency and accountability in addressing GenAI biases.\nUser sensitivity to biases in real life has little influence on reporting behavior since algorithmic biases don‚Äôt stand out similarly by eliciting a negative emotional response unless it is pronounced. Users need to be prompted or reminded to look for them in the responses - the more natural interpretation of results to look for how much the response matches their expectations.\nThe reminder strategy and effort required to provide feedback through UI/UX elements on different GenAI tools determine the likelihood of getting user feedback.\n\n\n\n\n\n\nAfter prioritizing the user needs from the contextual inquiry interviews through synthesis methods like affinity clustering and walking the wall again by adding new evidence, I rapidly prototyped solutions using the crazy 8s method. The goal of this activity is to brainstorm a set of ideas that meet user needs inspired by your analysis and reflection on your Walk the Wall activity.\nDownload PDF file.\nAfter analyzing the user breakdowns/needs, approaches, ideas, and unanswered questions raised in Crazy 8‚Äôs activity to identify the greatest areas of uncertainty and risk, we generated a list of user needs as a team from this analysis. Then, each team member selected one user need and created a set of three storyboards, each riskier than the previous.\nUsing these storyboards representing possible solution directions, I conducted speed-dating interviews with participants to rapidly explore design futures and prioritize the needs that appear strongly in user research and speed-dating sessions to reveal new design opportunities:\n\nToken Collection Strategy: Earn tokens for bias reports valued by usefulness (1-5 tokens); redeem 200 tokens for 15 free GPT 4.0 prompts or 500 for a week-long GPT 4.0 upgrade.\nCallout Reminder Strategy: Utilize a pop-up tool for users to critically evaluate AI responses, enabling tagging of biases in images or highlighting in text.\nMonetary Reward for Bias Reporting: Offer financial incentives for routine bias reporting or during major incidents, with a simple button or hashtag for direct reporting to social media.\nRecaptcha-Style Engagement Checks: Implement intermittent, non-intrusive prompts to verify user engagement and foster ongoing attention to detail\n\n\n\n\nFinally, I prioritized the token strategy idea, which received the most favorable feedback from the audience, and developed a low-fidelity prototype. I then conducted an initial interview with a participant to assess the prototype‚Äôs usefulness and its alignment with user needs. The participant was pleased with the new approach. Although there is still potential for enhancement, the prototype successfully achieved its goal of encouraging users to report biases in exchange for platform credits. Subsequently, I translated this prototype into a high-fidelity version using Figma."
  },
  {
    "objectID": "projects/2024-05-03-ucre-project-portfolio/index.html#poster-session",
    "href": "projects/2024-05-03-ucre-project-portfolio/index.html#poster-session",
    "title": "Harnessing Everyday Users‚Äô Power to Detect Harmful Behaviors in GenAI",
    "section": "",
    "text": "We presented our project findings during the course‚Äôs final poster session and received substantial feedback for future enhancements. Overall, the feedback was positive, and we were excited by the attendees‚Äô engagement level.\nDownload PDF file."
  },
  {
    "objectID": "projects/2024-05-03-ucre-project-portfolio/index.html#future-improvements",
    "href": "projects/2024-05-03-ucre-project-portfolio/index.html#future-improvements",
    "title": "Harnessing Everyday Users‚Äô Power to Detect Harmful Behaviors in GenAI",
    "section": "",
    "text": "During this project, we didn‚Äôt have time to conduct a comprehensive smoke test to verify user engagement increases after implementing the token strategy. I would love to spend more time in understanding how much time and effort users are willing to pay in exchange for platform credits. I also want to quantitatively determine the thresholds at which this scheme is feasible for companies to implement without incurring losses and simultaneously improving user engagement with bias reporting."
  },
  {
    "objectID": "projects/2024-05-03-ucre-project-portfolio/index.html#reflection",
    "href": "projects/2024-05-03-ucre-project-portfolio/index.html#reflection",
    "title": "Harnessing Everyday Users‚Äô Power to Detect Harmful Behaviors in GenAI",
    "section": "",
    "text": "When I started this course, my three primary learning objectives were:\n\nUnderstanding how user research is formally conducted and implementing it through the group project complement my second concentration in Human-centered Data Science.\nBecoming comfortable working with and speaking the language of designers and UX researchers to be a better software engineer and data scientist.\nImprove my design thinking skills, visualize data, and rapidly prototype low and high-fidelity designs.\n\nThroughout the group project, I learned the iterative nature of problem-solving, which involves continuous information gathering, problem redefinition, and solution exploration. Our diverse methods provided me with a comprehensive set of UX skills, enriching my approach to future projects. Notably, during the final poster session, I utilized Figma to create low and high-fidelity prototypes for our token strategy, an experience that expanded my technical toolkit. The course readings proved essential, deepening my understanding of each research method‚Äôs nuances. The practical application of synthesis methods like Affinity Clustering, Walk-the-Wall, and Speed Dating significantly enhanced my ability to reassess problems and discover varied solutions within the design realm. Additionally, analyzing complex and unstructured text data sharpened my data wrangling and visualization capabilities.\nThis project revealed that synthesis is a nuanced and intricate process. While research methods may appear straightforward in theory, their practical application is challenging, often requiring a shift in perspective to unearth valuable insights. This experience was gratifying, allowing me to view product development through the lens of a product manager and a UI/UX designer beyond my engineering and data science background. I am grateful to have met all my initial learning goals and feel more confident in my professional abilities. A heartfelt thank you to the teaching team for a truly enriching experience!"
  },
  {
    "objectID": "projects/2025-03-01-Gesture-Game-Controller/index.html",
    "href": "projects/2025-03-01-Gesture-Game-Controller/index.html",
    "title": "Arduino Gesture-based Game Controller Joystick Design",
    "section": "",
    "text": "Related Links & Artifacts\n\nFinal Report (PDF)\nAdvisor: Alexandra Ion\nüìÖ Project Duration: Jan 2025 ‚Äì Mar 2025\n\n\n\nProject Overview\nThis project presents a mid-air gesture recognition system using a dual-IMU Arduino controller, developed for hands-free 3D box manipulation tasks. Built on a two-handed ergonomic design and a neural model trained on 10 DOF IMU signals, the system supports 9 gestures for intuitive real-time control. It uses BLE for streaming predictions wirelessly and runs fully on-device via TinyML.\n\n‚ú® Built and deployed a two-handed Arduino Nano 33 BLE gesture controller with custom neural classifier\n‚ú® Achieved 99% on-device classification accuracy using stability filtering and confidence thresholds\n‚ú® Reduced average task time from ~170s to ~33s across 5 users by optimizing gesture mappings and prototype comfort\n‚ú® Integrated real-time BLE gesture streaming with Python client and custom UI for live square control\n\n\n\nDescription\nThe final system used two Arduinos mounted on a cardboard cutout for ergonomic handling. Extensive prototyping was done with glove and wand variants before finalizing this version based on user feedback, comfort, and accuracy. Gestures like up/down/left/right, clockwise/counterclockwise, and resizing (+/-) were supported. A trained dense neural network was deployed using TensorFlow Lite Micro for real-time on-device inference.\nDownload PDF file.\n\n\n1. Interaction Prototyping & User Testing\n\n\nCompared 3 form factors: wand, glove, and dual-hand controller\n\nFinal prototype mounted on cardboard for comfort and stability\n\nGesture sets included static poses, flicks, and rotation gestures\n\nTested across 5 users for timing, fatigue, and prediction stability\n\nFinal task averaged ~33s vs ~170s for initial glove/wand variants\n\n\n\n\n2. Model Architecture & Training\n\n\nTrained a feed-forward dense neural network (ReLU activations) on 50√ó12 time-series windows\n\nCollected balanced gesture data with capped 1-minute recordings per class\n\nEvaluated confusion matrices, training/validation curves, and misclassification overlap\n\nAccuracy peaked at 99% on-device due to clean data, axis separation, and class balancing\n\nModel exported to .tflite for microcontroller deployment\n\n\n\n\n3. On-Device Deployment & BLE Integration\n\n\nDeployed the .tflite model on Arduino Nano 33 BLE Rev2 using TFLite Micro\n\nUsed xxd to convert model into a C byte array\n\nReal-time inference pipeline: IMU ‚Üí buffer ‚Üí model ‚Üí BLE transmission\n\nBluetooth client built in Python using bleak library\n\nEnabled mobile, power bank‚Äìdriven untethered usage\n\n\n\n\n4. Post-Processing & Stability Enhancements\n\n\nConfidence thresholding (‚â• 0.95) filtered noisy predictions\n\nAdded a 20-frame stability window to trigger sensitive gestures like +, -, CW, CCW\n\nRemapped axes for rotation to reduce overlap with movement controls\n\nDynamic gestures (flicks) improved intuitiveness over static poses\n\n\n\n\n5. Results Summary\n\n\nTask time reduced from ~170s (glove) and ~146s (wand) to ~33s in final version\n\nAccuracy: 99% on-device with filtered predictions\n\nError rate: Near-zero for all users after filtering\n\nUser feedback: Positive on cardboard comfort, flick gesture intuitiveness, and UI responsiveness\n\n\n\n\nTools & Frameworks\n\n\n\n\n\n\n\nArea\nTools / Stack Used\n\n\n\n\nMicrocontroller + Sensors\nArduino Nano 33 BLE Rev2, BMI270 IMU, Two-Board Sync\n\n\nML Model Training\nKeras, TensorFlow, NumPy, scikit-learn, Matplotlib, Pandas\n\n\nOn-Device Inference\nTensorFlow Lite Micro, xxd, ArduinoBLE, C++, tflite::MicroInterpreter\n\n\nReal-Time BLE Streaming\nbleak (Python), Serial, UUID\n\n\nUI + Post-Processing\nPyQt5, Matplotlib, Python UI Thread, Stability Filter, Confidence Filter\n\n\nPrototyping & Analysis\nTraining Curves, User Logs, Confusion Matrix, Cardboard Mounting, GIFs"
  },
  {
    "objectID": "projects/2025-04-25-movie-recommender-engine/index.html",
    "href": "projects/2025-04-25-movie-recommender-engine/index.html",
    "title": "Movie Recommender",
    "section": "",
    "text": "Most Effective Project - Winner at AI Agents Weekend 2025 by Google & DeepMind\nDownload PDF file.\n\n\nProject Description\nPromo Miner is an autonomous AI-powered shopping assistant designed to help users unlock hidden savings by surfacing the best promotional deals directly from their inbox. By integrating Gmail‚Äôs Promotions tab, real-time web search, and intelligent agents, Promo Miner ensures users never miss a better deal.\nThe system begins by securely accessing only the user‚Äôs Gmail Promotions tab via OAuth, filtering out irrelevant messages such as welcome emails. Valid promotional emails are parsed and stored in Firebase, enabling persistent deal tracking.\nPromo content is then analyzed by a Gemini Reasoning Agent that classifies each offer based on discount, urgency, product category, coupon codes, and other contextual cues. The agent scores each promo and generates a personalized ranking, triggering notifications for high-value or expiring deals.\nPromo Miner extends its reasoning capability by using an Exa Web Agent to perform real-time comparisons of the promoted product with listings on platforms like Amazon. When a better price or more valuable alternative is detected, the system alerts the user via a push notification.\nDeployed as a Chrome Extension, Promo Miner provides a real-time, personalized view of top-ranked deals and recommended alternatives. Built using Gemini LLMs, Flask, Firebase, Jina Reader, and Exa Search, the system demonstrates full-stack AI agent behavior‚Äîfrom perception and reasoning to real-time action.\n\n\nFeatures\n\nPrivacy-Preserving OAuth Login ‚Äì Only accesses Gmail Promotions tab, maintaining user privacy.\nDeal Filtering & Storage ‚Äì Automatically detects valid promos and saves them securely in Firebase.\nGemini-Based Ranking ‚Äì Contextually scores deals by urgency, discount value, and coupon presence.\nReal-Time Comparison ‚Äì Searches the web for better deals using Exa and notifies users when found.\nBrowser Extension Interface ‚Äì Presents top deals and alternatives in a simple, ranked view."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "üìù Blogs\n\nUsing Captum for Interpreting PyTorch Models in Production\nA technical guide on explainability and observability for ML pipelines in production.\n Medium\nThinkBot: Nudging Critical Thinking in Everyday AI Conversations\nExploring Socratic prompting and persuasive design in LLM-driven educational tools.\n Medium\n\n\n\nüìñ Paper Publications\n\nFrom Complexity to Clarity: AI/NLP‚Äôs Role in Regulatory Compliance\nFindings of the Association for Computational Linguistics 2025\nJivitesh Jain, Nivedhitha Dhanasekaran, Mona Diab\nGraphEHR: Heterogeneous Graph Neural Network for Electronic Health Records\nAI4CI Workshop, IJCAI 2024\nJuho Jung, Minyoung Choe, Kushagra Agarwal, Nivedhitha Dhanasekaran\n Paper  Project Website\nMachine Learning Algorithm for Histopathologic Analysis of Temporal Artery Biopsies\nARVO 2023 & IOVS Journal\nDavid Hinkle, Bradley Thuro, Karthik Desingu, Nivedhitha Dhanasekaran, Naveena Yanamala\n Paper  Poster\nLocalization Systems for Autonomous Operation of Underwater Robotic Vehicles: A Survey\nIEEE OCEANS 2022\nNivedhitha Dhanasekaran, Karthik Desingu, Sakthivel Murugan S\n Paper  Demo\nAutomated Detection of Giant Cell Arteritis Using Deep Learning\nUnder Review @ NEJM\nKarthik Desingu, Bradley Thuro, Nivedhitha Dhanasekaran, Abhijit Bhattaru, Rohit Muralidhar, David M. Hinkle, Naveena Yanamala"
  }
]
[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Project Portfolio",
    "section": "",
    "text": "Arduino Gesture-based Game Controller Joystick Design\n\n\n\nSignal Processing\n\n\nDeep Learning\n\n\nInteraction Design\n\n\nStatistical Analysis\n\n\nUser-Research\n\n\n\nA two-hand arduino-based gaming joystick trained using neural networks for gesture-based control.\n\n\n\nNivedhitha Dhanasekaran\n\n\nMar 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHarnessing Everyday Users’ Power to Detect Harmful Behaviors in GenAI\n\n\n\nHCI\n\n\nResponsible AI\n\n\nUI/UX\n\n\nResearch\n\n\n\nUser-centered Research and Evaluation for Responsible GenAI\n\n\n\nNivedhitha Dhanasekaran\n\n\nMay 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMusic Magician\n\n\n\nInteractive Intelligence\n\n\nData Visualization\n\n\nWeb Development\n\n\nUI/UX\n\n\nHCI\n\n\n\nSpotify Music Artist Influence Analytics Dashboard\n\n\n\nNivedhitha Dhanasekaran\n\n\nDec 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBraille Voice\n\n\n\nComputer Vision\n\n\nNLP\n\n\nWeb Development\n\n\nMobile Development\n\n\nDeep Learning\n\n\nHCI\n\n\nResearch\n\n\n\nA Language Agnostic Assistive Technology for Braille-to-Text Translation\n\n\n\nNivedhitha Dhanasekaran\n\n\nJul 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGiant Cell Arteritis Detection\n\n\n\nComputer Vision\n\n\nDeep Learning\n\n\nData Science\n\n\nImage Processing\n\n\nStatistical Analysis\n\n\nResearch\n\n\n\nMachine Learning Algorithm to Analyze Histopathologic Sections of Temporal Artery Biopsy Specimens\n\n\n\nNivedhitha Dhanasekaran\n\n\nJul 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo Stage Flight Delay Prediction\n\n\n\nMachine Learning\n\n\nData Visualization\n\n\nStatistical Analysis\n\n\n\nPredicting the On-time Performance of Flights in the USA\n\n\n\nNivedhitha Dhanasekaran\n\n\nJun 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nORCA: Underwater Robot\n\n\n\nResearch\n\n\nRobotics\n\n\nHCI\n\n\n\nAn Inspection Class Remotely Operated Underwater Vehicle\n\n\n\nNivedhitha Dhanasekaran\n\n\nMay 1, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/2024-05-03-ucre-project-portfolio/index.html",
    "href": "projects/2024-05-03-ucre-project-portfolio/index.html",
    "title": "Harnessing Everyday Users’ Power to Detect Harmful Behaviors in GenAI",
    "section": "",
    "text": "In the Spring of 2024, I worked on a semester-long project for the 05-610 User-centered Research and Evaluation course as part of my Human-centered Data Science (Human Computer Interaction) concentration for the Master of Computational Data Science program at Carnegie Mellon University.\n\n\n\n\n  \n\n\n\n\n\n\nThe “Engage to Change: Rewarding User Reports for Bias Mitigation in Generative AI” project focuses on mitigating generative AI (GenAI) bias by rewarding users for reporting biases.\n\nObjective: The project aims to transform every AI interaction into an opportunity for eliminating bias, addressing the challenge that users often do not report biases due to cumbersome reporting mechanisms, privacy concerns, and a lack of motivation.\nMethodology: I employed various methods such as contextual inquiry, affinity clustering, prototyping, user flow modeling, and data analysis. These helped understand user behavior and design effective solutions.\nSolution: The core innovation is a digital incentive program where users earn tokens for reporting biases. These tokens can be exchanged for service upgrades or other rewards, integrating a sense of progress and achievement into the reporting process.\nDesign Features: The project introduces non-disruptive, context-aware reminders, robust privacy protections, and an intuitive, effortless UI/UX. These features are not only intended to encourage user participation but also to reassure them about the simplicity, security, and privacy of the reporting process.\nUser Feedback: Initial feedback indicates that users are motivated by rewards such as monetary incentives or platform credits. Strategic reminders and the potential to earn rewards are significant motivators for consistent engagement in bias reporting.\nImpact: The project underscores the transformative potential of every user’s contribution, with the ultimate goal of creating smarter, fairer, and bias-free AI solutions. This emphasis on fairness and bias mitigation inspires confidence in the proposed approach.\n\n\n\n\nIn this project, I had the opportunity to immerse myself in multiple roles, including that of a User Researcher, UI/UX designer, Data Scientist, and Project Manager. I embraced a multifaceted role that spanned several disciplines, allowing me to delve deeply into human-centered research. As a User Researcher, I planned and implemented a mixed-methods approach to gather insights from user study participants. This involved conducting observational fieldwork, utilizing interview techniques to uncover users’ needs and motivations, and collecting quantitative data from both systems and their users. As a Data Scientist, my role also extended to analyzing this diverse data quantitatively to find patterns in behaviors, motivations, and unmet needs. Synthesizing these insights, I envisioned new systems to meet these identified needs. As a UI/UX Designer, I developed conceptual designs and prototypes, further enhancing my contributions by evaluating these through various research methods to ensure they met user requirements effectively. As a Project Manager, I oversaw the project’s progress, ensuring that research findings were communicated effectively to all stakeholders, from study participants to research team members, fostering a collaborative and informed project environment.\n\n\n\n\n\nBefore initiating the project, I conducted thorough preliminary research to build a solid foundation of knowledge and keep existing solutions distinct. My exploration into “Harnessing Everyday Users’ Power to Detect Harmful Behaviors in Generative AI” involved experiential and informational searches.\nI gained firsthand experience by observing how my friends interacted with generative AI platforms and by experimenting with new technologies myself, such as Microsoft Co-pilot and various GPT models on ChatGPT 4.0, along with Midjourney. This allowed me to understand the user experience directly and explore emerging questions in everyday algorithm auditing.\nAdditionally, I consulted various sources, including news articles, academic journals, social media, online blogs, and national reports, to comprehensively understand the subject. From this research, I distilled critical insights into a brief report and organized this information visually on a Figma Jamboard.\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDownload PDF file.\n\n\n\nFollowing the initial research phase, I delved into domain-specific data provided by the teaching staff. This data consisted of aggregated results from 2,197 survey responses collected by the WeAudit team at CMU. This survey assessed how individuals’ identities, experiences, and knowledge affect their ability to detect harmful algorithmic biases in image search contexts.\nTo analyze this data, I first converted it into a CSV file to facilitate exploratory and quantitative analysis using Python. I undertook data-wrangling steps to isolate the necessary columns for deeper analysis. Given the text-based nature of the data, standard analytics and visualizations took time to apply. I had to preprocess the data and engineer features to prepare it for meaningful analysis.\nI then created heatmaps to explore correlations between age groups, users’ familiarity with digital media, and their likelihood of encountering biases. Additionally, I employed word clouds and topic modeling to derive more detailed insights from the data.\nDownload PDF file.\n\n\n\n\n\n\n\n\n\n\n\nUsers aged 25-34 and those 65 and above show noticeable patterns in encountering biases—monthly and a few times a year, respectively. These findings suggest that user-auditing frequency might need adjustment to prevent user overload when interacting with news articles and testing systems.\nMost age groups, including those 55 and older, are familiar with using algorithmic systems. This observation requires further investigation, as it may be influenced by desirability bias or sampling errors inherent in self-reported data.\nThere appears to be a weak correlation between the frequency of using algorithmic systems and encountering algorithmic biases, suggesting that familiarity does not necessarily predict bias encounter frequency. This relationship warrants further validation.\nNo clear correlation was found between users’ location and their sensitivity to bias. Sensitivity levels were consistent across different regions, although the West and Midwest were underrepresented in the survey data, indicating a need for broader geographic representation in future studies.\n\n\n\n\n\nTo assess how current solutions integrate bias reporting mechanisms that empower everyday users to help mitigate biases, I conducted a heuristic evaluation of the WeAudit TAIGA Tool, using Nielsen’s Ten Usability Heuristics.\nThis evaluation examined the system’s design and usability to determine how effectively it enables users to identify harmful algorithmic behaviors in image-generation AI systems. The key functionalities I assessed included:\n\nThe ability for users to input a search prompt and receive relevant images.\nUsers can review the returned images, highlight specific ones, and add comments.\nThe feature allows users to initiate discussion threads, which can be posted on the WeAudit forums.\n\n\n\nDownload PDF file.\n\n\n\nI conducted three usability tests on the WeAudit TAIGA tool to achieve the following objectives: - Identify specific challenges within the UI/UX when participants interact with TAIGA and other Generative AI platforms.\nDownload PDF file.\n\nGauge users’ logical and emotional responses while interacting with websites like TAIGA.\nExplore user perceptions and experiences concerning bias.\nObserve user behavior concerning reporting biases via TAIGA and other GenAI platforms.\nCollect basic demographic information about the participants.\nAssess demographic groups’ preferences regarding specific topics.\n\nTo facilitate these interviews, I developed a script to guide the discussions and ensured that consent forms were briefed to and collected from all participants before beginning the study. Additionally, I created an optional survey for participants to provide demographic information. This was designed to explore potential correlations between these demographic variables and participants’ propensity to report biases influenced by previous experiences.\nDownload PDF file.\nThese interviews provided insight into the primary challenges participants encounter with the current bias reporting mechanisms, their feelings towards algorithmic biases, and the suitability of a collaborative approach for addressing these issues.\nDownload PDF file.\n\n\n\nWalking the Wall is a way of re-immersing your team in the data and the analysis you have performed on it. I collected all my data from the studies conducted so far and walked the wall to reframe and redefine the problem by focusing on the following questions:\n\n\n\n\n  \n\nWhat’s going on here?\nWhat does the user need?\nWhat can we do about it?\nQuestions to conduct further research and conduct data!\n\n\n\n\nAfter identifying users’ pain points and needs regarding current methods for engaging everyday users in bias mitigation, I refined our broad objective into more targeted research questions in the form of How Might We (HMW) questions.\nTo redefine the primary research problem, I challenged the existing assumptions to pinpoint opportunities by applying “Reverse Assumptions,” selected the most critical one, and recast the problem statement. This led to evolving our overarching aim from “Harnessing Everyday Users’ Power to Detect Harmful Behaviors in Generative AI” to “How Might We Transform AI Interactions Into Opportunities for Bias Elimination by Incentivizing Users and Reducing Cognitive Overload to Simplify the Reporting Process?”\nDownload PDF file.\n\n\n\nI conducted additional contextual inquiry interviews to develop new qualitative insights aligned with our more focused research goals. These interviews centered around ChatGPT, utilizing it as the primary generative AI platform for our study, aiming to address specific research questions that correlate with our outlined goals:\n\nGoal #1: To improve user awareness, enhance the quality of reflection on system responses, and identify algorithmic biases within generative AI systems like Microsoft Copilot and ChatGPT.\nGoal #2: Develop strategies to motivate users to report biases they naturally encounter in AI-generated content, exploring intrinsic motivation and possibly gamifying the reporting process.\n\nDownload PDF file.\n\n\n\nWhat types of guidance and feedback are most effective in aiding users in detecting and reporting biases?\nHow can we effectively educate users on the nature and existence of algorithmic biases within generative AI systems?\nWhat design features in the user interface can encourage users to examine the responses they receive from generative AI systems critically?\nHow can feedback mechanisms be seamlessly integrated into generative AI platforms to facilitate straightforward reporting of detected biases by users?\nHow can community-driven platforms improve everyday users’ detection and reporting of algorithmic biases?\n\nDownload PDF file.\n\n\n\n\nAfter completing the interviews, I converted the session notes into interpretation notes. Using these notes, the team engaged in Affinity Clustering to organize the yellow interpretation notes into significant themes, ideas, and overall user concerns. We categorized these using blue, pink, and green labels to group them into a hierarchical structure that narratively outlines the overall user experience. To deepen our understanding of the data collected from the interviews, we constructed two models: an empathy map and a user journey flow map. These models helped us view the information from various perspectives, enhancing our comprehension of user experiences and interactions.\nDownload PDF file.\n\n\n\n\nHow can we make it more natural for users to report AI-generated bias?\n\nTo address the challenge of user unawareness and engagement with AI-generated biases, our initiative seeks to simplify the process for users to identify and report these biases. Recognizing that users often overlook or don’t critically evaluate AI biases, which affect the fairness and accuracy of AI results, we aim to enhance user awareness and interaction. We aimed to design intuitive user interfaces that encourage reflection, incorporate easy-to-use feedback mechanisms, and utilize common user behaviors to motivate more consistent auditing and reporting of biases.\n\nHow can we use current user interactions to unexpected responses to motivate user auditing and reporting?\n\nFurthermore, the interest in using current user reactions to unexpected responses as a catalyst for auditing and reporting behavior highlights an innovative approach to enhancing user participation in the quality control of AI outputs. Recognizing that users typically opt for re-prompting when faced with unsatisfactory AI responses, the goal is to integrate design approaches that make feedback provision a seamless and intuitive part of the user experience.\n\n\n\n\n\n\nWhat forms of guidance and feedback are most effective for supporting users in detecting and reporting biases?\nHow can we effectively educate users about the nature and presence of algorithmic biases within generative AI systems?\nWhat design elements in the user interface can prompt users to critically reflect on the responses they receive from generative AI systems?\nHow can feedback mechanisms be integrated into generative AI platforms to facilitate easy reporting of detected biases by users?\nHow can community-driven platforms enhance everyday users’ detection and reporting of algorithmic biases?\n\n\n\n\n\nUsers do not prioritize identifying biases in GenAI outputs, as their primary focus is leveraging AI to support everyday tasks.\nThe current reporting mechanism is unnatural and doesn’t fit into the natural workflow of users as they typically resort to re-prompting as an immediate solution to unexpected or unsatisfactory GenAI responses, sometimes even before the generation process is complete by interrupting the flow instead of looking for features to report this behavior.\nUser apprehensions about anonymity and privacy loom significant when reporting biases, underscoring a critical barrier to transparency and accountability in addressing GenAI biases.\nUser sensitivity to biases in real life has little influence on reporting behavior since algorithmic biases don’t stand out similarly by eliciting a negative emotional response unless it is pronounced. Users need to be prompted or reminded to look for them in the responses - the more natural interpretation of results to look for how much the response matches their expectations.\nThe reminder strategy and effort required to provide feedback through UI/UX elements on different GenAI tools determine the likelihood of getting user feedback.\n\n\n\n\n\n\nAfter prioritizing the user needs from the contextual inquiry interviews through synthesis methods like affinity clustering and walking the wall again by adding new evidence, I rapidly prototyped solutions using the crazy 8s method. The goal of this activity is to brainstorm a set of ideas that meet user needs inspired by your analysis and reflection on your Walk the Wall activity.\nDownload PDF file.\nAfter analyzing the user breakdowns/needs, approaches, ideas, and unanswered questions raised in Crazy 8’s activity to identify the greatest areas of uncertainty and risk, we generated a list of user needs as a team from this analysis. Then, each team member selected one user need and created a set of three storyboards, each riskier than the previous.\nUsing these storyboards representing possible solution directions, I conducted speed-dating interviews with participants to rapidly explore design futures and prioritize the needs that appear strongly in user research and speed-dating sessions to reveal new design opportunities:\n\nToken Collection Strategy: Earn tokens for bias reports valued by usefulness (1-5 tokens); redeem 200 tokens for 15 free GPT 4.0 prompts or 500 for a week-long GPT 4.0 upgrade.\nCallout Reminder Strategy: Utilize a pop-up tool for users to critically evaluate AI responses, enabling tagging of biases in images or highlighting in text.\nMonetary Reward for Bias Reporting: Offer financial incentives for routine bias reporting or during major incidents, with a simple button or hashtag for direct reporting to social media.\nRecaptcha-Style Engagement Checks: Implement intermittent, non-intrusive prompts to verify user engagement and foster ongoing attention to detail\n\n\n\n\nFinally, I prioritized the token strategy idea, which received the most favorable feedback from the audience, and developed a low-fidelity prototype. I then conducted an initial interview with a participant to assess the prototype’s usefulness and its alignment with user needs. The participant was pleased with the new approach. Although there is still potential for enhancement, the prototype successfully achieved its goal of encouraging users to report biases in exchange for platform credits. Subsequently, I translated this prototype into a high-fidelity version using Figma.\n  \n\n\n\n\nWe presented our project findings during the course’s final poster session and received substantial feedback for future enhancements. Overall, the feedback was positive, and we were excited by the attendees’ engagement level.\nDownload PDF file.\n\n\n\nDuring this project, we didn’t have time to conduct a comprehensive smoke test to verify user engagement increases after implementing the token strategy. I would love to spend more time in understanding how much time and effort users are willing to pay in exchange for platform credits. I also want to quantitatively determine the thresholds at which this scheme is feasible for companies to implement without incurring losses and simultaneously improving user engagement with bias reporting.\n\n\n\nWhen I started this course, my three primary learning objectives were:\n\nUnderstanding how user research is formally conducted and implementing it through the group project complement my second concentration in Human-centered Data Science.\nBecoming comfortable working with and speaking the language of designers and UX researchers to be a better software engineer and data scientist.\nImprove my design thinking skills, visualize data, and rapidly prototype low and high-fidelity designs.\n\nThroughout the group project, I learned the iterative nature of problem-solving, which involves continuous information gathering, problem redefinition, and solution exploration. Our diverse methods provided me with a comprehensive set of UX skills, enriching my approach to future projects. Notably, during the final poster session, I utilized Figma to create low and high-fidelity prototypes for our token strategy, an experience that expanded my technical toolkit. The course readings proved essential, deepening my understanding of each research method’s nuances. The practical application of synthesis methods like Affinity Clustering, Walk-the-Wall, and Speed Dating significantly enhanced my ability to reassess problems and discover varied solutions within the design realm. Additionally, analyzing complex and unstructured text data sharpened my data wrangling and visualization capabilities.\nThis project revealed that synthesis is a nuanced and intricate process. While research methods may appear straightforward in theory, their practical application is challenging, often requiring a shift in perspective to unearth valuable insights. This experience was gratifying, allowing me to view product development through the lens of a product manager and a UI/UX designer beyond my engineering and data science background. I am grateful to have met all my initial learning goals and feel more confident in my professional abilities. A heartfelt thank you to the teaching team for a truly enriching experience!"
  },
  {
    "objectID": "projects/2024-05-03-ucre-project-portfolio/index.html#engage-to-change-rewarding-user-reports-for-bias-mitigation-in-generative-ai",
    "href": "projects/2024-05-03-ucre-project-portfolio/index.html#engage-to-change-rewarding-user-reports-for-bias-mitigation-in-generative-ai",
    "title": "Harnessing Everyday Users’ Power to Detect Harmful Behaviors in GenAI",
    "section": "",
    "text": "The “Engage to Change: Rewarding User Reports for Bias Mitigation in Generative AI” project focuses on mitigating generative AI (GenAI) bias by rewarding users for reporting biases.\n\nObjective: The project aims to transform every AI interaction into an opportunity for eliminating bias, addressing the challenge that users often do not report biases due to cumbersome reporting mechanisms, privacy concerns, and a lack of motivation.\nMethodology: I employed various methods such as contextual inquiry, affinity clustering, prototyping, user flow modeling, and data analysis. These helped understand user behavior and design effective solutions.\nSolution: The core innovation is a digital incentive program where users earn tokens for reporting biases. These tokens can be exchanged for service upgrades or other rewards, integrating a sense of progress and achievement into the reporting process.\nDesign Features: The project introduces non-disruptive, context-aware reminders, robust privacy protections, and an intuitive, effortless UI/UX. These features are not only intended to encourage user participation but also to reassure them about the simplicity, security, and privacy of the reporting process.\nUser Feedback: Initial feedback indicates that users are motivated by rewards such as monetary incentives or platform credits. Strategic reminders and the potential to earn rewards are significant motivators for consistent engagement in bias reporting.\nImpact: The project underscores the transformative potential of every user’s contribution, with the ultimate goal of creating smarter, fairer, and bias-free AI solutions. This emphasis on fairness and bias mitigation inspires confidence in the proposed approach."
  },
  {
    "objectID": "projects/2024-05-03-ucre-project-portfolio/index.html#roles-in-the-project",
    "href": "projects/2024-05-03-ucre-project-portfolio/index.html#roles-in-the-project",
    "title": "Harnessing Everyday Users’ Power to Detect Harmful Behaviors in GenAI",
    "section": "",
    "text": "In this project, I had the opportunity to immerse myself in multiple roles, including that of a User Researcher, UI/UX designer, Data Scientist, and Project Manager. I embraced a multifaceted role that spanned several disciplines, allowing me to delve deeply into human-centered research. As a User Researcher, I planned and implemented a mixed-methods approach to gather insights from user study participants. This involved conducting observational fieldwork, utilizing interview techniques to uncover users’ needs and motivations, and collecting quantitative data from both systems and their users. As a Data Scientist, my role also extended to analyzing this diverse data quantitatively to find patterns in behaviors, motivations, and unmet needs. Synthesizing these insights, I envisioned new systems to meet these identified needs. As a UI/UX Designer, I developed conceptual designs and prototypes, further enhancing my contributions by evaluating these through various research methods to ensure they met user requirements effectively. As a Project Manager, I oversaw the project’s progress, ensuring that research findings were communicated effectively to all stakeholders, from study participants to research team members, fostering a collaborative and informed project environment."
  },
  {
    "objectID": "projects/2024-05-03-ucre-project-portfolio/index.html#methodology",
    "href": "projects/2024-05-03-ucre-project-portfolio/index.html#methodology",
    "title": "Harnessing Everyday Users’ Power to Detect Harmful Behaviors in GenAI",
    "section": "",
    "text": "Before initiating the project, I conducted thorough preliminary research to build a solid foundation of knowledge and keep existing solutions distinct. My exploration into “Harnessing Everyday Users’ Power to Detect Harmful Behaviors in Generative AI” involved experiential and informational searches.\nI gained firsthand experience by observing how my friends interacted with generative AI platforms and by experimenting with new technologies myself, such as Microsoft Co-pilot and various GPT models on ChatGPT 4.0, along with Midjourney. This allowed me to understand the user experience directly and explore emerging questions in everyday algorithm auditing.\nAdditionally, I consulted various sources, including news articles, academic journals, social media, online blogs, and national reports, to comprehensively understand the subject. From this research, I distilled critical insights into a brief report and organized this information visually on a Figma Jamboard.\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDownload PDF file.\n\n\n\nFollowing the initial research phase, I delved into domain-specific data provided by the teaching staff. This data consisted of aggregated results from 2,197 survey responses collected by the WeAudit team at CMU. This survey assessed how individuals’ identities, experiences, and knowledge affect their ability to detect harmful algorithmic biases in image search contexts.\nTo analyze this data, I first converted it into a CSV file to facilitate exploratory and quantitative analysis using Python. I undertook data-wrangling steps to isolate the necessary columns for deeper analysis. Given the text-based nature of the data, standard analytics and visualizations took time to apply. I had to preprocess the data and engineer features to prepare it for meaningful analysis.\nI then created heatmaps to explore correlations between age groups, users’ familiarity with digital media, and their likelihood of encountering biases. Additionally, I employed word clouds and topic modeling to derive more detailed insights from the data.\nDownload PDF file.\n\n\n\n\n\n\n\n\n\n\n\nUsers aged 25-34 and those 65 and above show noticeable patterns in encountering biases—monthly and a few times a year, respectively. These findings suggest that user-auditing frequency might need adjustment to prevent user overload when interacting with news articles and testing systems.\nMost age groups, including those 55 and older, are familiar with using algorithmic systems. This observation requires further investigation, as it may be influenced by desirability bias or sampling errors inherent in self-reported data.\nThere appears to be a weak correlation between the frequency of using algorithmic systems and encountering algorithmic biases, suggesting that familiarity does not necessarily predict bias encounter frequency. This relationship warrants further validation.\nNo clear correlation was found between users’ location and their sensitivity to bias. Sensitivity levels were consistent across different regions, although the West and Midwest were underrepresented in the survey data, indicating a need for broader geographic representation in future studies.\n\n\n\n\n\nTo assess how current solutions integrate bias reporting mechanisms that empower everyday users to help mitigate biases, I conducted a heuristic evaluation of the WeAudit TAIGA Tool, using Nielsen’s Ten Usability Heuristics.\nThis evaluation examined the system’s design and usability to determine how effectively it enables users to identify harmful algorithmic behaviors in image-generation AI systems. The key functionalities I assessed included:\n\nThe ability for users to input a search prompt and receive relevant images.\nUsers can review the returned images, highlight specific ones, and add comments.\nThe feature allows users to initiate discussion threads, which can be posted on the WeAudit forums.\n\n\n\nDownload PDF file.\n\n\n\nI conducted three usability tests on the WeAudit TAIGA tool to achieve the following objectives: - Identify specific challenges within the UI/UX when participants interact with TAIGA and other Generative AI platforms.\nDownload PDF file.\n\nGauge users’ logical and emotional responses while interacting with websites like TAIGA.\nExplore user perceptions and experiences concerning bias.\nObserve user behavior concerning reporting biases via TAIGA and other GenAI platforms.\nCollect basic demographic information about the participants.\nAssess demographic groups’ preferences regarding specific topics.\n\nTo facilitate these interviews, I developed a script to guide the discussions and ensured that consent forms were briefed to and collected from all participants before beginning the study. Additionally, I created an optional survey for participants to provide demographic information. This was designed to explore potential correlations between these demographic variables and participants’ propensity to report biases influenced by previous experiences.\nDownload PDF file.\nThese interviews provided insight into the primary challenges participants encounter with the current bias reporting mechanisms, their feelings towards algorithmic biases, and the suitability of a collaborative approach for addressing these issues.\nDownload PDF file.\n\n\n\nWalking the Wall is a way of re-immersing your team in the data and the analysis you have performed on it. I collected all my data from the studies conducted so far and walked the wall to reframe and redefine the problem by focusing on the following questions:\n\n\n\n\n  \n\nWhat’s going on here?\nWhat does the user need?\nWhat can we do about it?\nQuestions to conduct further research and conduct data!\n\n\n\n\nAfter identifying users’ pain points and needs regarding current methods for engaging everyday users in bias mitigation, I refined our broad objective into more targeted research questions in the form of How Might We (HMW) questions.\nTo redefine the primary research problem, I challenged the existing assumptions to pinpoint opportunities by applying “Reverse Assumptions,” selected the most critical one, and recast the problem statement. This led to evolving our overarching aim from “Harnessing Everyday Users’ Power to Detect Harmful Behaviors in Generative AI” to “How Might We Transform AI Interactions Into Opportunities for Bias Elimination by Incentivizing Users and Reducing Cognitive Overload to Simplify the Reporting Process?”\nDownload PDF file.\n\n\n\nI conducted additional contextual inquiry interviews to develop new qualitative insights aligned with our more focused research goals. These interviews centered around ChatGPT, utilizing it as the primary generative AI platform for our study, aiming to address specific research questions that correlate with our outlined goals:\n\nGoal #1: To improve user awareness, enhance the quality of reflection on system responses, and identify algorithmic biases within generative AI systems like Microsoft Copilot and ChatGPT.\nGoal #2: Develop strategies to motivate users to report biases they naturally encounter in AI-generated content, exploring intrinsic motivation and possibly gamifying the reporting process.\n\nDownload PDF file.\n\n\n\nWhat types of guidance and feedback are most effective in aiding users in detecting and reporting biases?\nHow can we effectively educate users on the nature and existence of algorithmic biases within generative AI systems?\nWhat design features in the user interface can encourage users to examine the responses they receive from generative AI systems critically?\nHow can feedback mechanisms be seamlessly integrated into generative AI platforms to facilitate straightforward reporting of detected biases by users?\nHow can community-driven platforms improve everyday users’ detection and reporting of algorithmic biases?\n\nDownload PDF file.\n\n\n\n\nAfter completing the interviews, I converted the session notes into interpretation notes. Using these notes, the team engaged in Affinity Clustering to organize the yellow interpretation notes into significant themes, ideas, and overall user concerns. We categorized these using blue, pink, and green labels to group them into a hierarchical structure that narratively outlines the overall user experience. To deepen our understanding of the data collected from the interviews, we constructed two models: an empathy map and a user journey flow map. These models helped us view the information from various perspectives, enhancing our comprehension of user experiences and interactions.\nDownload PDF file.\n\n\n\n\nHow can we make it more natural for users to report AI-generated bias?\n\nTo address the challenge of user unawareness and engagement with AI-generated biases, our initiative seeks to simplify the process for users to identify and report these biases. Recognizing that users often overlook or don’t critically evaluate AI biases, which affect the fairness and accuracy of AI results, we aim to enhance user awareness and interaction. We aimed to design intuitive user interfaces that encourage reflection, incorporate easy-to-use feedback mechanisms, and utilize common user behaviors to motivate more consistent auditing and reporting of biases.\n\nHow can we use current user interactions to unexpected responses to motivate user auditing and reporting?\n\nFurthermore, the interest in using current user reactions to unexpected responses as a catalyst for auditing and reporting behavior highlights an innovative approach to enhancing user participation in the quality control of AI outputs. Recognizing that users typically opt for re-prompting when faced with unsatisfactory AI responses, the goal is to integrate design approaches that make feedback provision a seamless and intuitive part of the user experience.\n\n\n\n\n\n\nWhat forms of guidance and feedback are most effective for supporting users in detecting and reporting biases?\nHow can we effectively educate users about the nature and presence of algorithmic biases within generative AI systems?\nWhat design elements in the user interface can prompt users to critically reflect on the responses they receive from generative AI systems?\nHow can feedback mechanisms be integrated into generative AI platforms to facilitate easy reporting of detected biases by users?\nHow can community-driven platforms enhance everyday users’ detection and reporting of algorithmic biases?\n\n\n\n\n\nUsers do not prioritize identifying biases in GenAI outputs, as their primary focus is leveraging AI to support everyday tasks.\nThe current reporting mechanism is unnatural and doesn’t fit into the natural workflow of users as they typically resort to re-prompting as an immediate solution to unexpected or unsatisfactory GenAI responses, sometimes even before the generation process is complete by interrupting the flow instead of looking for features to report this behavior.\nUser apprehensions about anonymity and privacy loom significant when reporting biases, underscoring a critical barrier to transparency and accountability in addressing GenAI biases.\nUser sensitivity to biases in real life has little influence on reporting behavior since algorithmic biases don’t stand out similarly by eliciting a negative emotional response unless it is pronounced. Users need to be prompted or reminded to look for them in the responses - the more natural interpretation of results to look for how much the response matches their expectations.\nThe reminder strategy and effort required to provide feedback through UI/UX elements on different GenAI tools determine the likelihood of getting user feedback.\n\n\n\n\n\n\nAfter prioritizing the user needs from the contextual inquiry interviews through synthesis methods like affinity clustering and walking the wall again by adding new evidence, I rapidly prototyped solutions using the crazy 8s method. The goal of this activity is to brainstorm a set of ideas that meet user needs inspired by your analysis and reflection on your Walk the Wall activity.\nDownload PDF file.\nAfter analyzing the user breakdowns/needs, approaches, ideas, and unanswered questions raised in Crazy 8’s activity to identify the greatest areas of uncertainty and risk, we generated a list of user needs as a team from this analysis. Then, each team member selected one user need and created a set of three storyboards, each riskier than the previous.\nUsing these storyboards representing possible solution directions, I conducted speed-dating interviews with participants to rapidly explore design futures and prioritize the needs that appear strongly in user research and speed-dating sessions to reveal new design opportunities:\n\nToken Collection Strategy: Earn tokens for bias reports valued by usefulness (1-5 tokens); redeem 200 tokens for 15 free GPT 4.0 prompts or 500 for a week-long GPT 4.0 upgrade.\nCallout Reminder Strategy: Utilize a pop-up tool for users to critically evaluate AI responses, enabling tagging of biases in images or highlighting in text.\nMonetary Reward for Bias Reporting: Offer financial incentives for routine bias reporting or during major incidents, with a simple button or hashtag for direct reporting to social media.\nRecaptcha-Style Engagement Checks: Implement intermittent, non-intrusive prompts to verify user engagement and foster ongoing attention to detail\n\n\n\n\nFinally, I prioritized the token strategy idea, which received the most favorable feedback from the audience, and developed a low-fidelity prototype. I then conducted an initial interview with a participant to assess the prototype’s usefulness and its alignment with user needs. The participant was pleased with the new approach. Although there is still potential for enhancement, the prototype successfully achieved its goal of encouraging users to report biases in exchange for platform credits. Subsequently, I translated this prototype into a high-fidelity version using Figma."
  },
  {
    "objectID": "projects/2024-05-03-ucre-project-portfolio/index.html#poster-session",
    "href": "projects/2024-05-03-ucre-project-portfolio/index.html#poster-session",
    "title": "Harnessing Everyday Users’ Power to Detect Harmful Behaviors in GenAI",
    "section": "",
    "text": "We presented our project findings during the course’s final poster session and received substantial feedback for future enhancements. Overall, the feedback was positive, and we were excited by the attendees’ engagement level.\nDownload PDF file."
  },
  {
    "objectID": "projects/2024-05-03-ucre-project-portfolio/index.html#future-improvements",
    "href": "projects/2024-05-03-ucre-project-portfolio/index.html#future-improvements",
    "title": "Harnessing Everyday Users’ Power to Detect Harmful Behaviors in GenAI",
    "section": "",
    "text": "During this project, we didn’t have time to conduct a comprehensive smoke test to verify user engagement increases after implementing the token strategy. I would love to spend more time in understanding how much time and effort users are willing to pay in exchange for platform credits. I also want to quantitatively determine the thresholds at which this scheme is feasible for companies to implement without incurring losses and simultaneously improving user engagement with bias reporting."
  },
  {
    "objectID": "projects/2024-05-03-ucre-project-portfolio/index.html#reflection",
    "href": "projects/2024-05-03-ucre-project-portfolio/index.html#reflection",
    "title": "Harnessing Everyday Users’ Power to Detect Harmful Behaviors in GenAI",
    "section": "",
    "text": "When I started this course, my three primary learning objectives were:\n\nUnderstanding how user research is formally conducted and implementing it through the group project complement my second concentration in Human-centered Data Science.\nBecoming comfortable working with and speaking the language of designers and UX researchers to be a better software engineer and data scientist.\nImprove my design thinking skills, visualize data, and rapidly prototype low and high-fidelity designs.\n\nThroughout the group project, I learned the iterative nature of problem-solving, which involves continuous information gathering, problem redefinition, and solution exploration. Our diverse methods provided me with a comprehensive set of UX skills, enriching my approach to future projects. Notably, during the final poster session, I utilized Figma to create low and high-fidelity prototypes for our token strategy, an experience that expanded my technical toolkit. The course readings proved essential, deepening my understanding of each research method’s nuances. The practical application of synthesis methods like Affinity Clustering, Walk-the-Wall, and Speed Dating significantly enhanced my ability to reassess problems and discover varied solutions within the design realm. Additionally, analyzing complex and unstructured text data sharpened my data wrangling and visualization capabilities.\nThis project revealed that synthesis is a nuanced and intricate process. While research methods may appear straightforward in theory, their practical application is challenging, often requiring a shift in perspective to unearth valuable insights. This experience was gratifying, allowing me to view product development through the lens of a product manager and a UI/UX designer beyond my engineering and data science background. I am grateful to have met all my initial learning goals and feel more confident in my professional abilities. A heartfelt thank you to the teaching team for a truly enriching experience!"
  },
  {
    "objectID": "projects/2023-02-17-gca/index.html",
    "href": "projects/2023-02-17-gca/index.html",
    "title": "Giant Cell Arteritis Detection",
    "section": "",
    "text": "Project Poster\n\n\n\n\n\nDownload PDF file.\n\n\nProject Description\nIn this project, I created an automated system to detect and diagnose Giant Cell Arteritis (GCA) from digital pathology slides using a deep residual convolutional network (ResNet) model for ROI-level classification, which produced a 91.65% whole-slide inference score. I collaborated with the core research team of clinical professionals to determine the inclusion and exclusion criteria for the design studies and randomized datasets, rigorously moved dates to protect patient privacy, and employed a common naming strategy across multiple health databases. Finally, I used GradCAM representations to visualize the model’s findings for medical specialists to validate. This system is the first published automated detector for Giant Cell Arteritis, and we plan to patent this technology soon."
  },
  {
    "objectID": "projects/2021-06-30-flight-delay-prediction/index.html",
    "href": "projects/2021-06-30-flight-delay-prediction/index.html",
    "title": "Two Stage Flight Delay Prediction",
    "section": "",
    "text": "Flight Delay Prediction\n\nI designed a two-stage predictive machine learning engine to forecast the on-time performance of flights at 15 different USA airports using 2016-2017 data. This project involved data cleaning, pre-processing, and merging flight and weather data. To address flight delays, the engine first classifies whether a flight will be delayed and then predicts the arrival delay in minutes for delayed flights. Due to class imbalance favoring on-time flights, SMOTE sampling was used before classification. The Random Forest classifier achieved the best performance with an F1 score of 0.78 and a Recall of 0.74 for delayed flights. For regression, the Random Forest regressor yielded an MAE of 7.178 minutes, an RMSE of 11.283 minutes, and an R-squared score of 0.977.\nDownload PDF file."
  },
  {
    "objectID": "leadership.html",
    "href": "leadership.html",
    "title": "Volunteering, Leadership & Teaching",
    "section": "",
    "text": "🎓 Teaching Assistant (May 2022 - Sep 2022)\n\nTaught a C Programming certificate course for undergraduates at SSNCE under the guidance of Dr. T. T. Mirnalinee and Dr. B. Prabavathy.\n\nDeveloped a curated lesson plan with offline & online classes, weekly assignments, code demos, and a capstone project for issuing merit certificates.\n\nGitHub Repo\n\n📰 Chief Editor, Smriti Newsletter (Apr - Jun 2022)\n\nProofread, edited, and wrote articles to meet publication standards.\n\nOversaw photography, design, and artwork selection for the publication.\n\nAssisted staff in meeting deadlines and raised the profile of the publication.\n\nPublication"
  },
  {
    "objectID": "leadership.html#section",
    "href": "leadership.html#section",
    "title": "Volunteering, Leadership & Teaching",
    "section": "",
    "text": "🎓 Teaching Assistant (May 2022 - Sep 2022)\n\nTaught a C Programming certificate course for undergraduates at SSNCE under the guidance of Dr. T. T. Mirnalinee and Dr. B. Prabavathy.\n\nDeveloped a curated lesson plan with offline & online classes, weekly assignments, code demos, and a capstone project for issuing merit certificates.\n\nGitHub Repo\n\n📰 Chief Editor, Smriti Newsletter (Apr - Jun 2022)\n\nProofread, edited, and wrote articles to meet publication standards.\n\nOversaw photography, design, and artwork selection for the publication.\n\nAssisted staff in meeting deadlines and raised the profile of the publication.\n\nPublication"
  },
  {
    "objectID": "leadership.html#section-1",
    "href": "leadership.html#section-1",
    "title": "Volunteering, Leadership & Teaching",
    "section": "2021",
    "text": "2021\n\n🎤 Secretary, Assoc. of Computer Engineers Student Chapter (Aug 2021 - Jun 2022)\n\nSecured ₹2.3 lakhs in sponsorship.\nLed execution of 8 CSE department events and planned a new flagship hackathon at Invente 6.0 with 2000+ participants.\n\nManaged sponsorship, marketing, editorial, brochure & poster design.\n\nCertificate\n\n📢 Secretary, ACM Student Chapter (Jul 2020 - Apr 2022)\n\nCoordinated chapter operations, handled correspondence, and led the Content & Editorial Team along with the PR, Marketing & Social Media Team for 9+ events.\n\nEvents\n\n🏅 Global Ambassador, WomenTech Network (May 2021 - Aug 2021)\n\nInspired 100,000 women in technology and helped drive change with purpose and impact.\n\nReceived an exclusive invite to the annual conference to learn about diversity & inclusion, leadership, and networking."
  },
  {
    "objectID": "leadership.html#section-2",
    "href": "leadership.html#section-2",
    "title": "Volunteering, Leadership & Teaching",
    "section": "2020",
    "text": "2020\n\n🤝 Student Volunteer, National Service Scheme (Dec 2018 - Jul 2020)\n\nCompleted 85+ hours of community service, including campus cleanups, content writing, workshops, organic gardening & fireless cooking.\n\nManaged paperwork for three webinars."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Nivedhitha Dhanasekaran. I am a Master of Computational Data Science Student at Language Technologies Institute, School of Computer Science, Carnegie Mellon University.\nI completed my undergraduate studies at Sri Sivasubramaniya Nadar College of Engineering with a B.E. in Computer Science and Engineering from 2018-2022 in Chennai, India. While progressing through my undergraduate curriculum, I had the opportunity to work on projects in Full Stack Engineering, Data Science, and Deep Learning. In these projects, I participated in all the stages of the software development lifecycle, focusing on scalability, reliability, and user experience. I also have limited research experience in Computer Vision, Computational Biomedicine, and Robotics.\nSubsequently, I was a Technical Analyst at Citi, collaborating with the engineering and product teams to comprehend customer needs, implement technology solutions, and deliver exciting projects.\nIn my spare time, I enjoy exploring strategies, books, habits & tools that help me live a happier, healthier, and more productive life. A fun fact about me is that I’m a textbook INTJ-A personality type."
  },
  {
    "objectID": "awards.html",
    "href": "awards.html",
    "title": "Honors & Awards",
    "section": "",
    "text": "🏅 Smart India Hackathon Winner (Aug 2022)\n\nPrize: ₹1,00,000 (Ministry of Earth Sciences, Gov. of India)\n\n\n🚀 Internally Funded Student Project - IFSP (May 2022)\n\nGrant: ₹30,000 (SSN College of Engineering)\n\nFounder & Lead of the Intelligent Navigation Software Systems Sub-team\n\nDeveloped the underwater robots ORCA & KYOGRE under Dr. S. Sakthivel Murugan with the UWARL Lab."
  },
  {
    "objectID": "awards.html#section",
    "href": "awards.html#section",
    "title": "Honors & Awards",
    "section": "",
    "text": "🏅 Smart India Hackathon Winner (Aug 2022)\n\nPrize: ₹1,00,000 (Ministry of Earth Sciences, Gov. of India)\n\n\n🚀 Internally Funded Student Project - IFSP (May 2022)\n\nGrant: ₹30,000 (SSN College of Engineering)\n\nFounder & Lead of the Intelligent Navigation Software Systems Sub-team\n\nDeveloped the underwater robots ORCA & KYOGRE under Dr. S. Sakthivel Murugan with the UWARL Lab."
  },
  {
    "objectID": "awards.html#section-1",
    "href": "awards.html#section-1",
    "title": "Honors & Awards",
    "section": "2021",
    "text": "2021\n\n🛰️ Winner of `AIRSA’ Hackathon (Nov 2021)\n\nProposed a novel deep learning model for Satellite Image Segmentation\n\nGitHub Repo\n\n🎓 Selected for Online Asian Machine Learning School & ACML (Oct 2021)\n\n🎬 IEEE R10 Undergraduate Student Project Video Contest Winner (Aug 2021)\n\nAsia-Pacific Region Champion (7 countries, 50+ councils)\n\nPrize: 300 USD\n\nWatch Video\n\n👩‍💻 vGHC2021 Scholarship Awardee (Jul 2021)\n\nSelected among 1,200 scholars worldwide\n\n💡 Runner-Up at Techstars Startup Weekend Chennai (Feb 2021)\n\nRanked 4th among 90 idea pitches"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nivedhitha Dhanasekaran",
    "section": "",
    "text": "Hey there!"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Nivedhitha Dhanasekaran",
    "section": "Education",
    "text": "Education\nCarnegie Mellon University | Pittsburgh, PA  Master of Computational Data Science | Aug 2023 - May 2025\nSri Sivasubramaniya Nadar College of Engineering | Chennai, India  B.E. Computer Science & Engineering | Aug 2018 - Jul 2022"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Nivedhitha Dhanasekaran",
    "section": "Experience",
    "text": "Experience\nCiticorp Services India Private Limited | Software Engineer | Jul 2022 - Jun 2023\nFidelity Investments | Full Stack Engineer Intern | Jun 2021- Jun 2021"
  },
  {
    "objectID": "projects/2021-05-01-orca/index.html",
    "href": "projects/2021-05-01-orca/index.html",
    "title": "ORCA: Underwater Robot",
    "section": "",
    "text": "Engineering ORCA\n\n\n\nProject Description\nAn underwater vehicle (UWV) is an underwater robot that helps carry out certain tasks for which it can be customized and designed. A Remotely Operated Vehicle (ROV) is a highly manoeuvrable underwater robot that can be used to explore ocean depths and is controlled by a human operator. A group of cables, or tethers, connects the ROV to a manned controlling station, sending data and operational instructions in the form of electrical signals back and forth between the operator and the vehicle.\nOur robot, named ORCA, is a 500 x 350 x 210 mm inspection class ROV. It weighs 10.24Kgs and houses various sensors (temperature, pressure, leak sensor, Inertial Measurement Unit) and a camera to carry out underwater missions. This 6-thruster model is driven by a navigation suite that takes manual control instructions through a joystick, maps them to desired levels of thrust and rudder values and relays them to the actuators to achieve on-demand movement of the vehicle. The navigation suite is built on top of a set of open-source C++ modules namely, Mission Oriented Operating Suite - Interval Programming (MOOS-IvP).\nORCA has been bench-tested and subsequently pool-tested in shallow water regions. It was able to carry out multiple 5-15 minute long straight line missions with consistent data collection through sensors and satisfactory manoeuvrability in all 6 degrees of freedom."
  },
  {
    "objectID": "projects/2022-07-01-braillevoice/index.html",
    "href": "projects/2022-07-01-braillevoice/index.html",
    "title": "Braille Voice",
    "section": "",
    "text": "Project Poster\n\n\n\n\n\nDownload PDF file.\n\n\nProject Description\nA novel multi-stage pipeline (Mobile & Web app) for Braille-to-Text translation of whole single-sided printed braille documents with support for text-to-speech audio playback & text summarization in two languages: English & Tamil."
  },
  {
    "objectID": "projects/2023-12-31-music-magician-spotify-dashboard/index.html",
    "href": "projects/2023-12-31-music-magician-spotify-dashboard/index.html",
    "title": "Music Magician",
    "section": "",
    "text": "Music Magicians: Spotify Music Artist Influence Analytics Dashboard\n\nI developed an interactive analytics dashboard using Streamlit, Vega-Altair, Plotly, NetworkX, and Python. The dashboard focuses on exploring the evolution of music and quantifying artists’ influence. It features informative visualizations and interactive elements to analyze music trends, artist characteristics, and the influence of past music on new compositions. This project successfully addressed the challenge by creating a user-friendly interface that facilitates in-depth exploration of the dataset, enabling storytelling and uncovering patterns and correlations in music."
  },
  {
    "objectID": "projects/2025-03-01-Gesture-Game-Controller/index.html",
    "href": "projects/2025-03-01-Gesture-Game-Controller/index.html",
    "title": "Arduino Gesture-based Game Controller Joystick Design",
    "section": "",
    "text": "Project Report\n\n\n\n\n\nDownload PDF file.\n\n\nProject Description\nThis project explores mid-air UI manipulation using two IMU-equipped Arduino boards for gesture-based control. Various prototypes were evaluated, including two-handed, single-hand, and wand-based designs. The final prototype was re- fined through model enhancements, stability thresholds, confidence filtering, and UI optimizations. User testing measured task completion time, error rates, and prediction stability, demonstrating improved efficiency and usability. The final system achieved robust gesture recognition, highlighting its potential for intuitive human-computer interaction."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "GraphEHR: Heterogeneous Graph Neural Network for Electronic Health Records\nAI for Critical Infrastructure Workshop @ the International Joint Conference on Arificial Intelligence 2024\nJuho Jung, Minyoung Choe, Kushagra Agarwal, Nivedhitha Dhanasekaran\npaper website\n\nMachine Learning Algorithm to Analyze Histopathologic Sections of Temporal Artery Biopsy Specimens\nThe Association for Research in Vision and Ophthalmology Conference 2023 & Investigative Ophthalmology & Visual Science Journal\nDavid Hinkle, Bradley Thuro, Karthik Desingu, Nivedhitha Dhanasekaran, Naveena Yanamala\npaper poster\n\nLocalization Systems for Autonomous Operation of Underwater Robotic Vehicles: A Survey\nIEEE OCEANS 2022 Conference\nNivedhitha Dhanasekaran, Karthik Desingu, Sakthivel Murugan S\npaper video\n\nFrom Complexity to Clarity: AI/NLP’s Role in Regulatory Compliance\nUnder Review @ ACL\nJivitesh Jain, Nivedhitha Dhanasekaran, Mona Diab\n\n\nAutomated Detection of Giant Cell Arteritis from Temporal Artery Biopsy Specimens using Deep Learning Approaches\nUnder Review @ NEJM\nKarthik Desingu, Bradley Thuro, Nivedhitha Dhanasekaran, Abhijit Bhattaru, Rohit Muralidhar, David M. Hinkle, Naveena Yanamala"
  }
]
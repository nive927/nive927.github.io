<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Nivedhitha Dhanasekaran</title>
<link>https://nive927.github.io/projects.html</link>
<atom:link href="https://nive927.github.io/projects.xml" rel="self" type="application/rss+xml"/>
<description>Nive&#39;s Portfolio Website.</description>
<generator>quarto-1.3.450</generator>
<lastBuildDate>Fri, 25 Apr 2025 04:00:00 GMT</lastBuildDate>
<item>
  <title>Real-time Movie Recommender Engine</title>
  <dc:creator>Nivedhitha Dhanasekaran</dc:creator>
  <link>https://nive927.github.io/projects/2025-04-25-movie-recommender-engine/index.html</link>
  <description><![CDATA[ 




<section id="related-links-artifacts" class="level1">
<h1>Related Links &amp; Artifacts</h1>
<div style="display: flex; flex-wrap: wrap; gap: 0.75rem; margin-top: 1rem; font-size: 0.85em;">
<p><!-- <a href="./documents/movie_recommender_final_report.pdf" target="_blank" style="background-color:#ea4335; color:white; padding:8px 16px; border-radius:6px; text-decoration:none;"> --> <!--   <i class="bi bi-file-earmark-text" style="margin-right:6px;"></i>Final Report (PDF) --> <!-- </a> --></p>
<p><a href="https://austinhenley.com/" target="_blank" style="background-color:#0b57d0; color:white; padding:8px 16px; border-radius:6px; text-decoration:none;"> <i class="bi bi-person-lines-fill" style="margin-right:6px;"></i>Advisor: Austin Henley </a></p>
<p><a href="https://clairelegoues.com/" target="_blank" style="background-color:#0b57d0; color:white; padding:8px 16px; border-radius:6px; text-decoration:none;"> <i class="bi bi-person-lines-fill" style="margin-right:6px;"></i>Advisor: Claire Le Goues </a></p>
<p><span style="background-color:#f3f4f6; color:#111; padding:8px 16px; border-radius:6px;"> üìÖ Project Duration: Feb - Apr 2025 </span></p>
</div>
</section>
<section id="project-overview" class="level1">
<h1>Project Overview</h1>
<p>A robust and production-ready movie recommendation engine integrating collaborative filtering with an MLOps pipeline. The system supports continuous deployment, online evaluation, real-time telemetry, and fairness monitoring using modern tooling.</p>
<blockquote class="blockquote">
<p>‚ú®Achieved &lt;0.4s average latency, 2.34 RMSE (online), and &gt;90% request success rate<br>
‚ú®Enabled automated retraining via MLflow with schema validation, drift detection, and CI triggers<br>
‚ú®Integrated A/B testing with LaunchDarkly for user-level model experimentation<br>
‚ú®Monitored service uptime and accuracy via Prometheus + Grafana with alerting and version tagging<br>
‚ú®Built fault-tolerant Kafka pipelines with PostgreSQL for ingestion, tracking, and evaluation</p>
</blockquote>
</section>
<section id="project-description" class="level1">
<h1>Project Description</h1>
<p><img src="https://nive927.github.io/projects/2025-04-25-movie-recommender-engine/images/project-card-image.jpg" style="width:100%; margin-top:1rem;"></p>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>1. Architecture &amp; Inference Pipeline</b></span>
</summary>
<ul>
<li>User-item collaborative filtering using cosine similarity<br>
</li>
<li>Dockerized Flask API downloads model artifacts from MLflow at runtime<br>
</li>
<li>Real-time Kafka consumers feed data into a PostgreSQL backend<br>
</li>
<li>Jenkins pipelines auto-trigger ingestion, schema validation, retraining, and promotion<br>
</li>
<li>Prometheus scrapes metrics for latency, availability, and fairness<br>
</li>
<li>LaunchDarkly assigns users to A/B test variants (user-user vs.&nbsp;item-item models)</li>
</ul>
</details>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>2. Evaluation &amp; Impact</b></span>
</summary>
<ul>
<li><strong>Offline RMSE</strong>: 2.09 (test set); <strong>Online RMSE</strong>: 2.34<br>
</li>
<li><strong>Inference latency</strong>: &lt;0.4 seconds<br>
</li>
<li><strong>Request success rate</strong>: &gt;90% (Prometheus-tracked)<br>
</li>
<li><strong>Memory footprint</strong>: ~87MB (user-user model)<br>
</li>
<li>Monitored schema violations, drift, and real-time traffic distribution<br>
</li>
<li>A/B testing showed statistically significant improvements in engagement (p &lt; 0.0001)</li>
</ul>
</details>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>3. Observability &amp; Monitoring</b></span>
</summary>
<ul>
<li>Kafka logs pushed to PostgreSQL for watch/rating/recommendation logs<br>
</li>
<li>Exposed metrics via <code>/metrics</code> endpoints for Prometheus<br>
</li>
<li>Visualized service uptime, schema violations, latency trends via Grafana<br>
</li>
<li>Tracked online engagement and fairness using watch completion and group disparity metrics<br>
</li>
<li>Alerting integrated (Grafana ‚Üí SMTP) to notify system failures</li>
</ul>
</details>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>4. Fairness, Feedback Loops &amp; Security</b></span>
</summary>
<ul>
<li>Evaluated engagement disparity across age and gender groups<br>
</li>
<li>Implemented fairness alerting (disparity ratio &lt; 0.8) and proposed balancing fallback models<br>
</li>
<li>Detected and mitigated feedback loops (popularity/self-confirmation) using telemetry analysis<br>
</li>
<li>Threat model included poisoning attacks and model extraction via APIs<br>
</li>
<li>Data poisoning detection used chi-squared tests; mitigation via schema validators + manual audits</li>
</ul>
</details>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>5. Continuous Integration &amp; Retraining</b></span>
</summary>
<ul>
<li>Jenkins pipelines retrain and promote models using MLflow tracking<br>
</li>
<li>Scheduled ingestion every 2 hours from Kafka to PostgreSQL<br>
</li>
<li>Retraining evaluated RMSE improvements and used force-promote override flag<br>
</li>
<li>Schema and data quality tests run via Pytest before retraining<br>
</li>
<li>Canary deploys tested via LaunchDarkly flags for rollback safety</li>
</ul>
</details>
</section>
<section id="tools-frameworks" class="level1">
<h1>Tools &amp; Frameworks</h1>
<table class="table">
<colgroup>
<col style="width: 24%">
<col style="width: 75%">
</colgroup>
<thead>
<tr class="header">
<th>Area</th>
<th>Tools / Stack Used</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Model Training</td>
<td><code>scikit-learn</code>, <code>pandas</code>, <code>joblib</code>, <code>numpy</code></td>
</tr>
<tr class="even">
<td>Data Ingestion &amp; Storage</td>
<td><code>Kafka</code>, <code>PostgreSQL</code>, <code>tmux</code>, <code>SQLAlchemy</code></td>
</tr>
<tr class="odd">
<td>Inference &amp; Serving</td>
<td><code>Flask</code>, <code>gunicorn</code>, <code>MLflow</code>, <code>Docker</code>, <code>NGINX</code></td>
</tr>
<tr class="even">
<td>CI/CD</td>
<td><code>Jenkins</code>, <code>GitHub Actions</code>, <code>pytest</code>, <code>Docker Hub</code>, <code>cron</code></td>
</tr>
<tr class="odd">
<td>Monitoring &amp; Telemetry</td>
<td><code>Prometheus</code>, <code>Grafana</code></td>
</tr>
<tr class="even">
<td>Experimentation</td>
<td><code>LaunchDarkly</code>, <code>pandas</code>, <code>scipy.stats</code>, <code>Prometheus Python Client</code></td>
</tr>
<tr class="odd">
<td>Security &amp; Validation</td>
<td><code>Pytest</code>, <code>psycopg2</code>, <code>pydantic</code></td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>A/B Testing</category>
  <category>Data Science</category>
  <category>Deployment</category>
  <category>Machine Learning System</category>
  <category>Model Explainability</category>
  <category>Monitoring</category>
  <category>Observability</category>
  <category>Responsible AI</category>
  <category>Web Development</category>
  <guid>https://nive927.github.io/projects/2025-04-25-movie-recommender-engine/index.html</guid>
  <pubDate>Fri, 25 Apr 2025 04:00:00 GMT</pubDate>
  <media:content url="https://nive927.github.io/projects/2025-04-25-movie-recommender-engine/images/project-card-image.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Promo Miner: Shopping AI Agent</title>
  <dc:creator>Nivedhitha Dhanasekaran</dc:creator>
  <link>https://nive927.github.io/projects/2025-04-13-promo-miner/index.html</link>
  <description><![CDATA[ 




<section id="related-links-artifacts" class="level1">
<h1>Related Links &amp; Artifacts</h1>
<div style="display: flex; flex-wrap: wrap; gap: 0.75rem; margin-top: 1rem; font-size: 0.85em;">
<p><a href="https://cmudsc.com/agents/" target="_blank" style="background-color:#0033cc; color:white; padding:8px 16px; border-radius:6px; text-decoration:none;"><i class="bi bi-link-45deg" style="margin-right:6px;"></i>Hackathon Website</a></p>
<p><a href="./documents/promo_miner_pitch_deck_nive_2025.pdf" target="_blank" style="background-color:#fc3939; color:white; padding:8px 16px; border-radius:6px; text-decoration:none;"><i class="bi bi-file-earmark-pdf-fill" style="margin-right:6px;"></i>Download Pitch Deck (PDF)</a></p>
<p><span style="background-color:#13b955; color:#111; padding:8px 16px; border-radius:6px;">üí∞ Sponsored by Google &amp; DeepMind ($500 Prize)</span></p>
</div>
</section>
<section id="project-overview" class="level1">
<h1>Project Overview</h1>
<p>Promo Miner is an autonomous AI-powered shopping assistant that uncovers hidden savings in users‚Äô Gmail inboxes. It identifies promotional offers from the Gmail Promotions tab, uses LLMs to rank deal quality, and performs real-time web comparisons to ensure users always get the best available deal.</p>
<blockquote class="blockquote">
<p>‚ú® Built a <strong>fully integrated shopping agent</strong> that autonomously performs Gmail parsing, promo reasoning, cross-site comparison, and Chrome-based alerting in real time.<br>
‚ú® Combines <strong>Gemini LLM reasoning</strong> with <strong>Exa-based search</strong> and <strong>push notifications</strong> to deliver ranked savings with minimal user effort.</p>
</blockquote>
</section>
<section id="description" class="level1">
<h1>Description</h1>
<p>The Promo Miner system <strong>securely accesses only the Gmail Promotions tab</strong> using <code>OAuth</code> and filters out irrelevant emails like welcome messages. Valid promotional content is <strong>parsed and structured</strong>, extracting metadata such as discounts, brand names, expiration dates, and coupon codes. These are stored in <code>Firestore</code> for persistent deal tracking.</p>
<p>A <strong>Gemini-based LLM agent</strong> then reasons over this data to <strong>classify and rank deals</strong> based on urgency, value, and usefulness. Deals that are likely to <strong>expire soon or offer significant savings</strong> are highlighted. The system also employs an <strong>Exa-powered agent</strong> to compare prices across platforms like <code>Amazon</code>. If a better offer is found, the user receives a <strong>real-time push notification</strong> through a <code>Chrome Extension</code> interface.</p>
<p><object data="./documents/promo_miner_pitch_deck_nive_2025.pdf" type="application/pdf" width="100%" height="850"><a href="./documents/promo_miner_pitch_deck_nive_2025.pdf" download="">Download PDF file.</a></object></p>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>1. Email Access &amp; Filtering</b></span>
</summary>
<ul>
<li>Uses <code>Gmail API</code> with <code>OAuth 2.0</code> to securely access the Promotions tab<br>
</li>
<li>Filters out onboarding/welcome emails and identifies valid promotional content<br>
</li>
<li>Parses subject line, sender, body text, promo code, and expiration cues</li>
</ul>
</details>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>2. Data Storage &amp; Processing</b></span>
</summary>
<ul>
<li>Extracted metadata is stored in <code>Google Firestore</code> for persistent tracking<br>
</li>
<li>Supports longitudinal deal analysis and browser-based rendering<br>
</li>
<li>Integrates with <code>Chrome Extension</code> for lightweight front-end access</li>
</ul>
</details>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>3. LLM-Based Promo Reasoning</b></span>
</summary>
<ul>
<li><code>Gemini</code> LLM classifies each deal on urgency, product type, savings, and utility<br>
</li>
<li>Generates <strong>user-specific promo rankings</strong> using context-aware heuristics<br>
</li>
<li>Annotates whether a deal includes <strong>coupons</strong>, <strong>bundle offers</strong>, or <strong>price drops</strong></li>
</ul>
</details>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>4. Web Comparison Agent</b></span>
</summary>
<ul>
<li><code>Exa</code> search agent compares extracted deals to live listings on platforms like <code>Amazon</code><br>
</li>
<li>Flags <strong>cheaper alternatives</strong> or <strong>higher-value bundles</strong><br>
</li>
<li>Triggers <code>push notifications</code> to users for better savings</li>
</ul>
</details>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>5. Deployment &amp; Interface</b></span>
</summary>
<ul>
<li><code>Chrome Extension</code> provides a <strong>ranked deal dashboard</strong><br>
</li>
<li>Users receive timely alerts on new or expiring promos<br>
</li>
<li>Entire pipeline runs in <strong>real time</strong>, with low-latency triggers from backend to browser</li>
</ul>
</details>
</section>
<section id="tools-frameworks" class="level1">
<h1>Tools &amp; Frameworks</h1>
<table class="table">
<colgroup>
<col style="width: 19%">
<col style="width: 80%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Stack / Tools Used</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Email Access</td>
<td><code>Gmail API</code>, <code>OAuth 2.0</code></td>
</tr>
<tr class="even">
<td>Parsing &amp; Storage</td>
<td><code>Flask</code>, <code>Google Firestore</code>, <code>Regex</code>, <code>JSON Extractors</code></td>
</tr>
<tr class="odd">
<td>Reasoning Agent</td>
<td><code>Gemini</code>, <code>Python</code>, <code>Structured Query Prompting</code>, <code>LlamaIndex</code></td>
</tr>
<tr class="even">
<td>Web Comparison Agent</td>
<td><code>Exa</code>, <code>Semantic Search</code>, <code>Jina Reader API</code></td>
</tr>
<tr class="odd">
<td>Frontend Interface</td>
<td><code>Chrome Extension</code>, <code>JavaScript</code>, <code>HTML</code>, <code>CSS</code>, <code>Browser Notifications API</code></td>
</tr>
<tr class="even">
<td>Back-end Infrastructure</td>
<td><code>Flask REST API</code>, <code>Firestore Rules</code></td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>AI Agents</category>
  <category>Award Winner</category>
  <category>Deployment</category>
  <category>Large Language Models</category>
  <category>Monitoring</category>
  <category>Natural Language Processing</category>
  <category>UI/UX</category>
  <category>Web Development</category>
  <guid>https://nive927.github.io/projects/2025-04-13-promo-miner/index.html</guid>
  <pubDate>Sun, 13 Apr 2025 04:00:00 GMT</pubDate>
  <media:content url="https://nive927.github.io/projects/2025-04-13-promo-miner/images/project-card-image.png" medium="image" type="image/png" height="79" width="144"/>
</item>
<item>
  <title>Arduino Gesture-based Game Controller Joystick</title>
  <dc:creator>Nivedhitha Dhanasekaran</dc:creator>
  <link>https://nive927.github.io/projects/2025-03-01-Gesture-Game-Controller/index.html</link>
  <description><![CDATA[ 




<section id="related-links-artifacts" class="level1">
<h1>Related Links &amp; Artifacts</h1>
<div style="display: flex; flex-wrap: wrap; gap: 0.75rem; margin-top: 1rem; font-size: 0.85em;">
<p><a href="./documents/ndhanase_P2_Gesture_Recognition.pdf" target="_blank" style="background-color:#ea4335; color:white; padding:8px 16px; border-radius:6px; text-decoration:none;"><i class="bi bi-file-earmark-text" style="margin-right:6px;"></i>Final Report (PDF)</a></p>
<p><a href="https://hcii.cmu.edu/people/alexandra-ion" target="_blank" style="background-color:#d93025; color:white; padding:8px 16px; border-radius:6px; text-decoration:none;"><i class="bi bi-person-lines-fill" style="margin-right:6px;"></i>Advisor: Alexandra Ion</a></p>
<p><span style="background-color:#f3f4f6; color:#111; padding:8px 16px; border-radius:6px;">üìÖ Project Duration: Jan 2025 ‚Äì Mar 2025</span></p>
</div>
</section>
<section id="project-overview" class="level1">
<h1>Project Overview</h1>
<p>This project presents a mid-air gesture recognition system using a dual-IMU Arduino controller, developed for hands-free 3D box manipulation tasks. Built on a two-handed ergonomic design and a neural model trained on 10 DOF IMU signals, the system supports 9 gestures for intuitive real-time control. It uses BLE for streaming predictions wirelessly and runs fully on-device via TinyML.</p>
<blockquote class="blockquote">
<p>‚ú® Built and deployed a <strong>two-handed Arduino Nano 33 BLE gesture controller</strong> with custom neural classifier<br>
‚ú® Achieved <strong>99% on-device classification accuracy</strong> using stability filtering and confidence thresholds<br>
‚ú® Reduced average task time from <strong>~170s to ~33s</strong> across 5 users by optimizing gesture mappings and prototype comfort<br>
‚ú® Integrated real-time BLE gesture streaming with Python client and custom UI for live square control</p>
</blockquote>
</section>
<section id="description" class="level1">
<h1>Description</h1>
<p>The final system used two Arduinos mounted on a cardboard cutout for ergonomic handling. Extensive prototyping was done with glove and wand variants before finalizing this version based on user feedback, comfort, and accuracy. Gestures like up/down/left/right, clockwise/counterclockwise, and resizing (+/-) were supported. A trained dense neural network was deployed using TensorFlow Lite Micro for real-time on-device inference.</p>
<p><object data="./documents/ndhanase_P2_Gesture_Recognition.pdf" type="application/pdf" width="100%" height="850"><a href="./documents/ndhanase_P2_Gesture_Recognition.pdf" download="">Download PDF file.</a></object></p>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>1. Interaction Prototyping &amp; User Testing</b></span>
</summary>
<ul>
<li>Compared 3 form factors: wand, glove, and dual-hand controller<br>
</li>
<li>Final prototype mounted on cardboard for comfort and stability<br>
</li>
<li>Gesture sets included static poses, flicks, and rotation gestures<br>
</li>
<li>Tested across 5 users for timing, fatigue, and prediction stability<br>
</li>
<li>Final task averaged ~33s vs ~170s for initial glove/wand variants</li>
</ul>
</details>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>2. Model Architecture &amp; Training</b></span>
</summary>
<ul>
<li>Trained a feed-forward dense neural network (ReLU activations) on 50√ó12 time-series windows<br>
</li>
<li>Collected balanced gesture data with capped 1-minute recordings per class<br>
</li>
<li>Evaluated confusion matrices, training/validation curves, and misclassification overlap<br>
</li>
<li>Accuracy peaked at 99% on-device due to clean data, axis separation, and class balancing<br>
</li>
<li>Model exported to <code>.tflite</code> for microcontroller deployment</li>
</ul>
</details>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>3. On-Device Deployment &amp; BLE Integration</b></span>
</summary>
<ul>
<li>Deployed the <code>.tflite</code> model on Arduino Nano 33 BLE Rev2 using TFLite Micro<br>
</li>
<li>Used <code>xxd</code> to convert model into a C byte array<br>
</li>
<li>Real-time inference pipeline: IMU ‚Üí buffer ‚Üí model ‚Üí BLE transmission<br>
</li>
<li>Bluetooth client built in Python using <code>bleak</code> library<br>
</li>
<li>Enabled mobile, power bank‚Äìdriven untethered usage</li>
</ul>
</details>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>4. Post-Processing &amp; Stability Enhancements</b></span>
</summary>
<ul>
<li>Confidence thresholding (‚â• 0.95) filtered noisy predictions<br>
</li>
<li>Added a 20-frame stability window to trigger sensitive gestures like <code>+</code>, <code>-</code>, CW, CCW<br>
</li>
<li>Remapped axes for rotation to reduce overlap with movement controls<br>
</li>
<li>Dynamic gestures (flicks) improved intuitiveness over static poses</li>
</ul>
</details>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>5. Results Summary</b></span>
</summary>
<ul>
<li><strong>Task time</strong> reduced from ~170s (glove) and ~146s (wand) to <strong>~33s</strong> in final version<br>
</li>
<li><strong>Accuracy:</strong> 99% on-device with filtered predictions<br>
</li>
<li><strong>Error rate:</strong> Near-zero for all users after filtering<br>
</li>
<li><strong>User feedback:</strong> Positive on cardboard comfort, flick gesture intuitiveness, and UI responsiveness</li>
</ul>
</details>
</section>
<section id="tools-frameworks" class="level1">
<h1>Tools &amp; Frameworks</h1>
<table class="table">
<colgroup>
<col style="width: 26%">
<col style="width: 73%">
</colgroup>
<thead>
<tr class="header">
<th>Area</th>
<th>Tools / Stack Used</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Microcontroller + Sensors</td>
<td><code>Arduino Nano 33 BLE Rev2</code>, <code>BMI270 IMU</code>, <code>Two-Board Sync</code></td>
</tr>
<tr class="even">
<td>ML Model Training</td>
<td><code>Keras</code>, <code>TensorFlow</code>, <code>NumPy</code>, <code>scikit-learn</code>, <code>Matplotlib</code>, <code>Pandas</code></td>
</tr>
<tr class="odd">
<td>On-Device Inference</td>
<td><code>TensorFlow Lite Micro</code>, <code>xxd</code>, <code>ArduinoBLE</code>, <code>C++</code>, <code>tflite::MicroInterpreter</code></td>
</tr>
<tr class="even">
<td>Real-Time BLE Streaming</td>
<td><code>bleak (Python)</code>, <code>Serial</code>, <code>UUID</code></td>
</tr>
<tr class="odd">
<td>UI + Post-Processing</td>
<td><code>PyQt5</code>, <code>Matplotlib</code>, <code>Python UI Thread</code>, <code>Stability Filter</code>, <code>Confidence Filter</code></td>
</tr>
<tr class="even">
<td>Prototyping &amp; Analysis</td>
<td><code>Training Curves</code>, <code>User Logs</code>, <code>Confusion Matrix</code>, <code>Cardboard Mounting</code>, <code>GIFs</code></td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>Data Science</category>
  <category>Deep Learning</category>
  <category>GPU Training</category>
  <category>Human-Computer Interaction</category>
  <category>Machine Learning System</category>
  <category>Model Explainability</category>
  <category>On-Device AI</category>
  <category>Signal Processing</category>
  <category>UI/UX</category>
  <guid>https://nive927.github.io/projects/2025-03-01-Gesture-Game-Controller/index.html</guid>
  <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
  <media:content url="https://nive927.github.io/projects/2025-03-01-Gesture-Game-Controller/images/project-card-image.png" medium="image" type="image/png" height="80" width="144"/>
</item>
<item>
  <title>NoteRecall: On-Device RAG for Privacy-preserving QA</title>
  <dc:creator>Nivedhitha Dhanasekaran</dc:creator>
  <link>https://nive927.github.io/projects/2024-12-07-note-recall/index.html</link>
  <description><![CDATA[ 




<section id="related-links-artifacts" class="level1">
<h1>Related Links &amp; Artifacts</h1>
<div style="display: flex; flex-wrap: wrap; gap: 0.75rem; margin-top: 1rem; font-size: 0.85em;">
<p><a href="./documents/noterecall_final_report.pdf" target="_blank" style="background-color:#ea4335; color:white; padding:8px 16px; border-radius:6px; text-decoration:none;"><i class="bi bi-file-earmark-text" style="margin-right:6px;"></i>Final Report (PDF)</a></p>
<p><a href="https://www.lti.cs.cmu.edu/people/faculty/strubell-emma.html" target="_blank" style="background-color:#d93025; color:white; padding:8px 16px; border-radius:6px; text-decoration:none;"><i class="bi bi-person-lines-fill" style="margin-right:6px;"></i>Advisor: Emma Strubell</a></p>
<p><span style="background-color:#f3f4f6; color:#111; padding:8px 16px; border-radius:6px;">üìÖ Project Duration: Mar 2025 - May 2025</span></p>
</div>
</section>
<section id="project-overview" class="level1">
<h1>Project Overview</h1>
<p>NoteRecall is a lightweight, privacy-preserving Retrieval-Augmented Generation (RAG) system that enables secure, on-device question answering over personal documents such as medical notes or offline records. Built with quantized embedding and reader models, the system performs end-to-end inference on devices like M2 MacBooks while ensuring user data never leaves the device.</p>
<blockquote class="blockquote">
<p>‚ú® Developed a privacy-preserving RAG pipeline for on-device question answering over medical documents<br>
‚ú® Applied quantization, pruning, and model distillation to optimize inference efficiency on consumer hardware<br>
‚ú® Achieved a BERTScore of 0.56 while preserving retrieval and generation quality<br>
‚ú® Enabled 2.4√ó more inference cycles under a 10Wh energy budget compared to baseline models</p>
</blockquote>
</section>
<section id="description" class="level1">
<h1>Description</h1>
<p>The NoteRecall pipeline embeds user documents and retrieves relevant chunks to answer questions locally using a distilled and quantized reader model.</p>
<p><object data="./documents/ODML_Final_Report_ACL_Personal.pdf" type="application/pdf" width="100%" height="850"><a href="./documents/ODML_Final_Report_ACL_Personal.pdf" download="">Download PDF file.</a></object></p>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>1. Motivation &amp; Privacy-First Setup</b></span>
</summary>
<ul>
<li>Eliminates the need for cloud-based LLMs and reduces risk of data leakage<br>
</li>
<li>Designed for privacy-critical use cases like medical records and personal notes<br>
</li>
<li>Empowers users with offline, secure information retrieval<br>
</li>
<li>Ensures all computation (embedding, search, generation) remains local</li>
</ul>
</details>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>2. Task Setup &amp; Data</b></span>
</summary>
<ul>
<li>Input: User documents (context) and natural language queries<br>
</li>
<li>Output: Retrieved passages and generated answers<br>
</li>
<li>Dataset: BioASQ Task B medical QA corpus (3,680 docs, 300 QA pairs)<br>
</li>
<li>Metrics: BERTScore F1, end-to-end latency, energy efficiency (inferences per 10Wh)</li>
</ul>
</details>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>3. Model Architecture &amp; Pipeline</b></span>
</summary>
<ul>
<li>Dense retriever: GTE-Qwen-2 Instruct (1.5B)<br>
</li>
<li>Reader model: LLaMA3.2-1B or distilled Flan-T5<br>
</li>
<li>Vector store: FAISS with HNSW indexing<br>
</li>
<li>Chunking strategy: 512-token overlapping spans<br>
</li>
<li>Top-k retrieval (k=3) used as reader context<br>
</li>
<li>End-to-end inference optimized with MLX and MPS</li>
</ul>
</details>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>4. Efficiency Optimizations</b></span>
</summary>
<ul>
<li>Quantization (Q3_K_S, Q5_K_M via llama.cpp)<br>
</li>
<li>Structured/unstructured pruning of FFNs &amp; attention heads<br>
</li>
<li>Model distillation from LLaMA3 to Flan-T5 for latency and energy gains<br>
</li>
<li>Evaluated MoE-style readers for tradeoff exploration</li>
</ul>
</details>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>5. Results Summary</b></span>
</summary>
<ul>
<li>BERTScore F1: 0.5693 (full-precision), 0.5597 (distilled Flan-T5)<br>
</li>
<li>Latency: Reduced from 8.5s to 1.45s (quantized models)<br>
</li>
<li>Energy: ~2.4√ó more inferences under 10Wh using Flan-T5<br>
</li>
<li>MoE models yielded higher accuracy but were memory inefficient on M2 hardware</li>
</ul>
</details>
</section>
<section id="tools-frameworks" class="level1">
<h1>Tools &amp; Frameworks</h1>
<table class="table">
<colgroup>
<col style="width: 23%">
<col style="width: 76%">
</colgroup>
<thead>
<tr class="header">
<th>Area</th>
<th>Tools / Stack Used</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Retriever</td>
<td><code>GTE-Qwen-2 Instruct</code>, <code>FAISS</code>, <code>HNSW</code></td>
</tr>
<tr class="even">
<td>Reader Models</td>
<td><code>LLaMA3.2-1B-Instruct</code>, <code>Flan-T5-Base</code>, <code>Qwen1.5-MoE-A2.7B</code></td>
</tr>
<tr class="odd">
<td>Quantization &amp; Pruning</td>
<td><code>llama.cpp</code>, <code>GGUF</code>, <code>L1-based head pruning</code>, <code>Optimum</code>, <code>torch.nn.utils.prune</code></td>
</tr>
<tr class="even">
<td>Distillation</td>
<td><code>Synthetic QA pairs</code>, <code>LLaMA 7B (teacher) ‚Üí Flan-T5 (student)</code></td>
</tr>
<tr class="odd">
<td>On-Device Inference</td>
<td><code>MLX</code>, <code>MPS (Apple Silicon)</code>, <code>GGML</code></td>
</tr>
<tr class="even">
<td>Evaluation</td>
<td><code>BioASQ</code>, <code>BERTScore</code>, <code>Latency profiler</code>, <code>CodeCarbon (for energy tracking)</code></td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>Data Science</category>
  <category>Deep Learning</category>
  <category>Deployment</category>
  <category>Large Language Models</category>
  <category>Machine Learning System</category>
  <category>Natural Language Processing</category>
  <category>On-Device AI</category>
  <category>Responsible AI</category>
  <guid>https://nive927.github.io/projects/2024-12-07-note-recall/index.html</guid>
  <pubDate>Sat, 07 Dec 2024 05:00:00 GMT</pubDate>
  <media:content url="https://nive927.github.io/projects/2024-12-07-note-recall/images/project-card-image.png" medium="image" type="image/png" height="69" width="144"/>
</item>
<item>
  <title>Harnessing Everyday Users‚Äô Power to Detect Harmful Behaviors in GenAI</title>
  <dc:creator>Nivedhitha Dhanasekaran</dc:creator>
  <link>https://nive927.github.io/projects/2024-05-03-ucre-project-portfolio/index.html</link>
  <description><![CDATA[ 




<!-- [ shiny dashboard](https://samanthacsik.shinyapps.io/strava_dashboard/){.btn target=_blank} [ source code](https://github.com/samanthacsik/strava-dashboard){.btn target=_blank} -->
<!-- <br> -->
<!-- <div> -->
<!-- </div> -->
<section id="journey-through-the-project" class="level1">
<h1>Journey through the Project</h1>
<p>In the Spring of 2024, I worked on a semester-long project for the 05-610 User-centered Research and Evaluation course as part of my Human-centered Data Science (Human Computer Interaction) concentration for the Master of Computational Data Science program at Carnegie Mellon University.</p>
<!-- <br> -->
<!-- <img src="project-card-image.png" alt="Broken image." width="100%"> -->
<!-- <br> -->
<!-- ![Broken Image.](./project-card-image.png) -->
<p><br> <img src="https://nive927.github.io/projects/2024-05-03-ucre-project-portfolio/images/project-card-image.png" alt="Broken image." width="100%"> <br></p>
<!-- ::: {.gray-italic .center-text} -->
<!-- UCRE Project Caption -->
<!-- ::: -->
<!-- <img src="cathedral-peak.jpeg" alt="A woman stands on top of a rocky peak with blue sky and chapparal-covered mountains in the background. Blue sky is visible to the left, and foggy, dark clouds are rolling in from the right." width="70%"> -->
<section id="engage-to-change-rewarding-user-reports-for-bias-mitigation-in-generative-ai" class="level2">
<h2 class="anchored" data-anchor-id="engage-to-change-rewarding-user-reports-for-bias-mitigation-in-generative-ai">Engage to Change: Rewarding User Reports for Bias Mitigation in Generative AI</h2>
<p>The ‚ÄúEngage to Change: Rewarding User Reports for Bias Mitigation in Generative AI‚Äù project focuses on mitigating generative AI (GenAI) bias by rewarding users for reporting biases.</p>
<ul>
<li><strong>Objective:</strong> The project aims to transform every AI interaction into an opportunity for eliminating bias, addressing the challenge that users often do not report biases due to cumbersome reporting mechanisms, privacy concerns, and a lack of motivation.</li>
<li><strong>Methodology:</strong> I employed various methods such as contextual inquiry, affinity clustering, prototyping, user flow modeling, and data analysis. These helped understand user behavior and design effective solutions.</li>
<li><strong>Solution:</strong> The core innovation is a digital incentive program where users earn tokens for reporting biases. These tokens can be exchanged for service upgrades or other rewards, integrating a sense of progress and achievement into the reporting process.</li>
<li><strong>Design Features:</strong> The project introduces non-disruptive, context-aware reminders, robust privacy protections, and an intuitive, effortless UI/UX. These features are not only intended to encourage user participation but also to reassure them about the simplicity, security, and privacy of the reporting process.</li>
<li><strong>User Feedback:</strong> Initial feedback indicates that users are motivated by rewards such as monetary incentives or platform credits. Strategic reminders and the potential to earn rewards are significant motivators for consistent engagement in bias reporting.</li>
<li><strong>Impact:</strong> The project underscores the transformative potential of every user‚Äôs contribution, with the ultimate goal of creating smarter, fairer, and bias-free AI solutions. This emphasis on fairness and bias mitigation inspires confidence in the proposed approach.</li>
</ul>
</section>
<section id="roles-in-the-project" class="level2">
<h2 class="anchored" data-anchor-id="roles-in-the-project">Roles in the Project</h2>
<p>In this project, I had the opportunity to immerse myself in multiple roles, including that of a User Researcher, UI/UX designer, Data Scientist, and Project Manager. I embraced a multifaceted role that spanned several disciplines, allowing me to delve deeply into human-centered research. As a User Researcher, I planned and implemented a mixed-methods approach to gather insights from user study participants. This involved conducting observational fieldwork, utilizing interview techniques to uncover users‚Äô needs and motivations, and collecting quantitative data from both systems and their users. As a Data Scientist, my role also extended to analyzing this diverse data quantitatively to find patterns in behaviors, motivations, and unmet needs. Synthesizing these insights, I envisioned new systems to meet these identified needs. As a UI/UX Designer, I developed conceptual designs and prototypes, further enhancing my contributions by evaluating these through various research methods to ensure they met user requirements effectively. As a Project Manager, I oversaw the project‚Äôs progress, ensuring that research findings were communicated effectively to all stakeholders, from study participants to research team members, fostering a collaborative and informed project environment.</p>
</section>
<section id="methodology" class="level2">
<h2 class="anchored" data-anchor-id="methodology">Methodology</h2>
<section id="background-research" class="level3">
<h3 class="anchored" data-anchor-id="background-research">Background Research</h3>
<p>Before initiating the project, I conducted thorough preliminary research to build a solid foundation of knowledge and keep existing solutions distinct. My exploration into ‚ÄúHarnessing Everyday Users‚Äô Power to Detect Harmful Behaviors in Generative AI‚Äù involved experiential and informational searches.</p>
<p>I gained firsthand experience by observing how my friends interacted with generative AI platforms and by experimenting with new technologies myself, such as Microsoft Co-pilot and various GPT models on ChatGPT 4.0, along with Midjourney. This allowed me to understand the user experience directly and explore emerging questions in everyday algorithm auditing.</p>
<p>Additionally, I consulted various sources, including news articles, academic journals, social media, online blogs, and national reports, to comprehensively understand the subject. From this research, I distilled critical insights into a brief report and organized this information visually on a Figma Jamboard.</p>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="https://nive927.github.io/projects/2024-05-03-ucre-project-portfolio/images/bg-context-01.png" class="img-fluid"> <!-- ![](./images/bg-exp.png){fig-align="center"}  --></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="https://nive927.github.io/projects/2024-05-03-ucre-project-portfolio/images/bg-context-02.png" class="img-fluid"></p>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<!-- ![](./images/bg-info.png){fig-align="center"} -->
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<!-- <div style="display: flex; justify-content: space-around; align-items: center;">> -->
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<!--   <img src="./images/bg-exp.png"  width="50%"> -->
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<!--   <img src="./images/bg-info.png"  width="50%"> -->
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<!-- </div> -->
</div>
</div>
</div>
<p><object data="./documents/01-bg.pdf" type="application/pdf" width="100%" height="600"><a href="./documents/01-bg.pdf" download="">Download PDF file.</a></object></p>
</section>
<section id="quantitative-data-analysis" class="level3">
<h3 class="anchored" data-anchor-id="quantitative-data-analysis">Quantitative Data Analysis</h3>
<p>Following the initial research phase, I delved into domain-specific data provided by the teaching staff. This data consisted of aggregated results from 2,197 survey responses collected by the WeAudit team at CMU. This survey assessed how individuals‚Äô identities, experiences, and knowledge affect their ability to detect harmful algorithmic biases in image search contexts.</p>
<p>To analyze this data, I first converted it into a CSV file to facilitate exploratory and quantitative analysis using Python. I undertook data-wrangling steps to isolate the necessary columns for deeper analysis. Given the text-based nature of the data, standard analytics and visualizations took time to apply. I had to preprocess the data and engineer features to prepare it for meaningful analysis.</p>
<p>I then created heatmaps to explore correlations between age groups, users‚Äô familiarity with digital media, and their likelihood of encountering biases. Additionally, I employed word clouds and topic modeling to derive more detailed insights from the data.</p>
<p><object data="./documents/02-data.pdf" type="application/pdf" width="100%" height="600"><a href="./documents/02-data.pdf" download="">Download PDF file.</a></object></p>
<!-- ![](./images/data-analysis.png) -->
<!-- ![](./images/data-text.png) -->
<!-- <br> -->
<!-- <img src="./images/data-analysis.png" alt="Broken image." style="width: 50%; display: block; margin: auto;"> -->
<!-- </br> -->
<!-- <br> -->
<!-- <img src="./images/data-text.png" alt="Broken image." style="width: 50%; display: block; margin: auto;"> -->
<!-- </br> -->
<section id="key-insights" class="level4">
<h4 class="anchored" data-anchor-id="key-insights">Key Insights</h4>
<ul>
<li>Users aged 25-34 and those 65 and above show noticeable patterns in encountering biases‚Äîmonthly and a few times a year, respectively. These findings suggest that user-auditing frequency might need adjustment to prevent user overload when interacting with news articles and testing systems.</li>
<li>Most age groups, including those 55 and older, are familiar with using algorithmic systems. This observation requires further investigation, as it may be influenced by desirability bias or sampling errors inherent in self-reported data.</li>
<li>There appears to be a weak correlation between the frequency of using algorithmic systems and encountering algorithmic biases, suggesting that familiarity does not necessarily predict bias encounter frequency. This relationship warrants further validation.</li>
<li>No clear correlation was found between users‚Äô location and their sensitivity to bias. Sensitivity levels were consistent across different regions, although the West and Midwest were underrepresented in the survey data, indicating a need for broader geographic representation in future studies.</li>
</ul>
</section>
</section>
<section id="heuristic-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="heuristic-evaluation">Heuristic Evaluation</h3>
<p>To assess how current solutions integrate bias reporting mechanisms that empower everyday users to help mitigate biases, I conducted a heuristic evaluation of the <a href="https://taiga.weaudit.org/">WeAudit TAIGA Tool</a>, using <a href="http://www.nngroup.com/articles/ten-usability-heuristics/">Nielsen‚Äôs Ten Usability Heuristics</a>.</p>
<p>This evaluation examined the system‚Äôs design and usability to determine how effectively it enables users to identify harmful algorithmic behaviors in image-generation AI systems. The key functionalities I assessed included:</p>
<ul>
<li>The ability for users to input a search prompt and receive relevant images.</li>
<li>Users can review the returned images, highlight specific ones, and add comments.</li>
<li>The feature allows users to initiate discussion threads, which can be posted on the WeAudit forums.</li>
</ul>
<!-- quarto add jmgirard/embedpdf -->
<!-- https://www.github.com/jmgirard/embedpdf -->
<p><object data="./documents/03-heuristic.pdf" type="application/pdf" width="100%" height="600"><a href="./documents/03-heuristic.pdf" download="">Download PDF file.</a></object></p>
</section>
<section id="usability-testing-through-think-aloud-interviews" class="level3">
<h3 class="anchored" data-anchor-id="usability-testing-through-think-aloud-interviews">Usability Testing through Think Aloud Interviews</h3>
<p>I conducted three usability tests on the WeAudit TAIGA tool to achieve the following objectives: - Identify specific challenges within the UI/UX when participants interact with TAIGA and other Generative AI platforms.</p>
<p><object data="./documents/04-usability-notes.pdf" type="application/pdf" width="100%" height="600"><a href="./documents/04-usability-notes.pdf" download="">Download PDF file.</a></object></p>
<ul>
<li>Gauge users‚Äô logical and emotional responses while interacting with websites like TAIGA.</li>
<li>Explore user perceptions and experiences concerning bias.</li>
<li>Observe user behavior concerning reporting biases via TAIGA and other GenAI platforms.</li>
<li>Collect basic demographic information about the participants.</li>
<li>Assess demographic groups‚Äô preferences regarding specific topics.</li>
</ul>
<p>To facilitate these interviews, I developed a script to guide the discussions and ensured that consent forms were briefed to and collected from all participants before beginning the study. Additionally, I created an optional survey for participants to provide demographic information. This was designed to explore potential correlations between these demographic variables and participants‚Äô propensity to report biases influenced by previous experiences.</p>
<p><object data="./documents/04-ut-script.pdf" type="application/pdf" width="100%" height="600"><a href="./documents/04-ut-script.pdf" download="">Download PDF file.</a></object></p>
<p>These interviews provided insight into the primary challenges participants encounter with the current bias reporting mechanisms, their feelings towards algorithmic biases, and the suitability of a collaborative approach for addressing these issues.</p>
<p><object data="./documents/04-ut-demographics.pdf" type="application/pdf" width="100%" height="600"><a href="./documents/04-ut-demographics.pdf" download="">Download PDF file.</a></object></p>
</section>
<section id="synthesis-by-walking-the-wall" class="level3">
<h3 class="anchored" data-anchor-id="synthesis-by-walking-the-wall">Synthesis by Walking-the-Wall</h3>
<p>Walking the Wall is a way of re-immersing your team in the data and the analysis you have performed on it. I collected all my data from the studies conducted so far and walked the wall to reframe and redefine the problem by focusing on the following questions:</p>
<!-- ```{=html} -->
<!-- <iframe width="780" height="500" src="<iframe style="border: 1px solid rgba(0, 0, 0, 0.1);" width="800" height="450" src="https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FBGVlKWsaz6MlvXN4bZBwRq%2FTeam-D1-Team-Contract-Spring24%3Ftype%3Dwhiteboard%26node-id%3D150%253A918%26t%3DAew6evaQQJKXNumv-1" allowfullscreen></iframe>" title="FigJam"></iframe> -->
<!-- ``` -->
<!-- ![](./images/wall-nd.png) -->
<p><br> <img src="https://nive927.github.io/projects/2024-05-03-ucre-project-portfolio/images/wall-nd.png" alt="Broken image." style="width: 50%; display: block; margin: auto;"> <br></p>
<ul>
<li>What‚Äôs going on here?</li>
<li>What does the user need?</li>
<li>What can we do about it?</li>
<li>Questions to conduct further research and conduct data!</li>
</ul>
</section>
<section id="reframing-and-redefining-the-problem" class="level3">
<h3 class="anchored" data-anchor-id="reframing-and-redefining-the-problem">Reframing and Redefining the Problem</h3>
<p>After identifying users‚Äô pain points and needs regarding current methods for engaging everyday users in bias mitigation, I refined our broad objective into more targeted research questions in the form of How Might We (HMW) questions.</p>
<p>To redefine the primary research problem, I challenged the existing assumptions to pinpoint opportunities by applying ‚ÄúReverse Assumptions,‚Äù selected the most critical one, and recast the problem statement. This led to evolving our overarching aim from <strong>‚ÄúHarnessing Everyday Users‚Äô Power to Detect Harmful Behaviors in Generative AI‚Äù</strong> to <strong>‚ÄúHow Might We Transform AI Interactions Into Opportunities for Bias Elimination by Incentivizing Users and Reducing Cognitive Overload to Simplify the Reporting Process?‚Äù</strong></p>
<p><object data="./documents/05-reframe.pdf" type="application/pdf" width="100%" height="600"><a href="./documents/05-reframe.pdf" download="">Download PDF file.</a></object></p>
</section>
<section id="context-research-inquiry-through-interviews" class="level3">
<h3 class="anchored" data-anchor-id="context-research-inquiry-through-interviews">Context Research Inquiry through Interviews</h3>
<p>I conducted additional contextual inquiry interviews to develop new qualitative insights aligned with our more focused research goals. These interviews centered around ChatGPT, utilizing it as the primary generative AI platform for our study, aiming to address specific research questions that correlate with our outlined goals:</p>
<ul>
<li><strong>Goal #1</strong>: To improve user awareness, enhance the quality of reflection on system responses, and identify algorithmic biases within generative AI systems like Microsoft Copilot and ChatGPT.</li>
<li><strong>Goal #2</strong>: Develop strategies to motivate users to report biases they naturally encounter in AI-generated content, exploring intrinsic motivation and possibly gamifying the reporting process.</li>
</ul>
<p><object data="./documents/06-res-guide.pdf" type="application/pdf" width="100%" height="600"><a href="./documents/06-res-guide.pdf" download="">Download PDF file.</a></object></p>
<section id="research-questions" class="level4">
<h4 class="anchored" data-anchor-id="research-questions">Research Questions</h4>
<ul>
<li>What types of guidance and feedback are most effective in aiding users in detecting and reporting biases?</li>
<li>How can we effectively educate users on the nature and existence of algorithmic biases within generative AI systems?</li>
<li>What design features in the user interface can encourage users to examine the responses they receive from generative AI systems critically?</li>
<li>How can feedback mechanisms be seamlessly integrated into generative AI platforms to facilitate straightforward reporting of detected biases by users?</li>
<li>How can community-driven platforms improve everyday users‚Äô detection and reporting of algorithmic biases?</li>
</ul>
<p><object data="./documents/06-contextual-inquiry.pdf" type="application/pdf" width="100%" height="600"><a href="./documents/06-contextual-inquiry.pdf" download="">Download PDF file.</a></object></p>
</section>
</section>
<section id="interpretation-notes-affinity-clustering-modeling" class="level3">
<h3 class="anchored" data-anchor-id="interpretation-notes-affinity-clustering-modeling">Interpretation Notes, Affinity Clustering &amp; Modeling</h3>
<p>After completing the interviews, I converted the session notes into interpretation notes. Using these notes, the team engaged in Affinity Clustering to organize the yellow interpretation notes into significant themes, ideas, and overall user concerns. We categorized these using blue, pink, and green labels to group them into a hierarchical structure that narratively outlines the overall user experience. To deepen our understanding of the data collected from the interviews, we constructed two models: an empathy map and a user journey flow map. These models helped us view the information from various perspectives, enhancing our comprehension of user experiences and interactions.</p>
<p><object data="./documents/07-report.pdf" type="application/pdf" width="100%" height="600"><a href="./documents/07-report.pdf" download="">Download PDF file.</a></object></p>
<!--  -->
<section id="how-might-we-reframing" class="level4">
<h4 class="anchored" data-anchor-id="how-might-we-reframing">How Might We Reframing</h4>
<ul>
<li>How can we make it more natural for users to report AI-generated bias?
<ul>
<li>To address the challenge of user unawareness and engagement with AI-generated biases, our initiative seeks to simplify the process for users to identify and report these biases. Recognizing that users often overlook or don‚Äôt critically evaluate AI biases, which affect the fairness and accuracy of AI results, we aim to enhance user awareness and interaction. We aimed to design intuitive user interfaces that encourage reflection, incorporate easy-to-use feedback mechanisms, and utilize common user behaviors to motivate more consistent auditing and reporting of biases.</li>
</ul></li>
<li>How can we use current user interactions to unexpected responses to motivate user auditing and reporting?
<ul>
<li>Furthermore, the interest in using current user reactions to unexpected responses as a catalyst for auditing and reporting behavior highlights an innovative approach to enhancing user participation in the quality control of AI outputs. Recognizing that users typically opt for re-prompting when faced with unsatisfactory AI responses, the goal is to integrate design approaches that make feedback provision a seamless and intuitive part of the user experience.</li>
</ul></li>
</ul>
</section>
<section id="related-questions-driving-our-contextual-inquiry" class="level4">
<h4 class="anchored" data-anchor-id="related-questions-driving-our-contextual-inquiry">Related Questions Driving Our Contextual Inquiry:</h4>
<ul>
<li>What forms of guidance and feedback are most effective for supporting users in detecting and reporting biases?</li>
<li>How can we effectively educate users about the nature and presence of algorithmic biases within generative AI systems?</li>
<li>What design elements in the user interface can prompt users to critically reflect on the responses they receive from generative AI systems?</li>
<li>How can feedback mechanisms be integrated into generative AI platforms to facilitate easy reporting of detected biases by users?</li>
<li>How can community-driven platforms enhance everyday users‚Äô detection and reporting of algorithmic biases?</li>
</ul>
</section>
<section id="insights" class="level4">
<h4 class="anchored" data-anchor-id="insights">Insights</h4>
<ul>
<li>Users do not prioritize identifying biases in GenAI outputs, as their primary focus is leveraging AI to support everyday tasks.</li>
<li>The current reporting mechanism is unnatural and doesn‚Äôt fit into the natural workflow of users as they typically resort to re-prompting as an immediate solution to unexpected or unsatisfactory GenAI responses, sometimes even before the generation process is complete by interrupting the flow instead of looking for features to report this behavior.</li>
<li>User apprehensions about anonymity and privacy loom significant when reporting biases, underscoring a critical barrier to transparency and accountability in addressing GenAI biases.</li>
<li>User sensitivity to biases in real life has little influence on reporting behavior since algorithmic biases don‚Äôt stand out similarly by eliciting a negative emotional response unless it is pronounced. Users need to be prompted or reminded to look for them in the responses - the more natural interpretation of results to look for how much the response matches their expectations.</li>
<li>The reminder strategy and effort required to provide feedback through UI/UX elements on different GenAI tools determine the likelihood of getting user feedback.</li>
</ul>
<!--  -->
</section>
</section>
<section id="crazy-8s-storyboarding-and-speed-dating-interviews" class="level3">
<h3 class="anchored" data-anchor-id="crazy-8s-storyboarding-and-speed-dating-interviews">Crazy 8s, Storyboarding, and Speed Dating Interviews</h3>
<p>After prioritizing the user needs from the contextual inquiry interviews through synthesis methods like affinity clustering and walking the wall again by adding new evidence, I rapidly prototyped solutions using the crazy 8s method. The goal of this activity is to brainstorm a set of ideas that meet user needs inspired by your analysis and reflection on your Walk the Wall activity.</p>
<p><object data="./documents/08-SD.pdf" type="application/pdf" width="100%" height="600"><a href="./documents/08-SD.pdf" download="">Download PDF file.</a></object></p>
<p>After analyzing the user breakdowns/needs, approaches, ideas, and unanswered questions raised in Crazy 8‚Äôs activity to identify the greatest areas of uncertainty and risk, we generated a list of user needs as a team from this analysis. Then, each team member selected one user need and created a set of three storyboards, each riskier than the previous.</p>
<p>Using these storyboards representing possible solution directions, I conducted speed-dating interviews with participants to rapidly explore design futures and prioritize the needs that appear strongly in user research and speed-dating sessions to reveal new design opportunities:</p>
<ul>
<li><strong>Token Collection Strategy</strong>: Earn tokens for bias reports valued by usefulness (1-5 tokens); redeem 200 tokens for 15 free GPT 4.0 prompts or 500 for a week-long GPT 4.0 upgrade.</li>
<li><strong>Callout Reminder Strategy</strong>: Utilize a pop-up tool for users to critically evaluate AI responses, enabling tagging of biases in images or highlighting in text.</li>
<li><strong>Monetary Reward for Bias Reporting</strong>: Offer financial incentives for routine bias reporting or during major incidents, with a simple button or hashtag for direct reporting to social media.</li>
<li><strong>Recaptcha-Style Engagement Checks</strong>: Implement intermittent, non-intrusive prompts to verify user engagement and foster ongoing attention to detail</li>
</ul>
</section>
<section id="prototyping-low-and-high-fidelity" class="level3">
<h3 class="anchored" data-anchor-id="prototyping-low-and-high-fidelity">Prototyping: Low and High Fidelity</h3>
<p>Finally, I prioritized the token strategy idea, which received the most favorable feedback from the audience, and developed a low-fidelity prototype. I then conducted an initial interview with a participant to assess the prototype‚Äôs usefulness and its alignment with user needs. The participant was pleased with the new approach. Although there is still potential for enhancement, the prototype successfully achieved its goal of encouraging users to report biases in exchange for platform credits. Subsequently, I translated this prototype into a high-fidelity version using Figma.</p>
<p><br> <img src="https://nive927.github.io/projects/2024-05-03-ucre-project-portfolio/images/lofi.png" alt="Broken image." width="100%"> <br></p>
</section>
</section>
<section id="poster-session" class="level2">
<h2 class="anchored" data-anchor-id="poster-session">Poster Session</h2>
<p>We presented our project findings during the course‚Äôs final poster session and received substantial feedback for future enhancements. Overall, the feedback was positive, and we were excited by the attendees‚Äô engagement level.</p>
<p><object data="./documents/09-only-poster.pdf" type="application/pdf" width="100%" height="600"><a href="./documents/09-only-poster.pdf" download="">Download PDF file.</a></object></p>
</section>
<section id="future-improvements" class="level2">
<h2 class="anchored" data-anchor-id="future-improvements">Future Improvements</h2>
<p>During this project, we didn‚Äôt have time to conduct a comprehensive smoke test to verify user engagement increases after implementing the token strategy. I would love to spend more time in understanding how much time and effort users are willing to pay in exchange for platform credits. I also want to quantitatively determine the thresholds at which this scheme is feasible for companies to implement without incurring losses and simultaneously improving user engagement with bias reporting.</p>
</section>
<section id="reflection" class="level2">
<h2 class="anchored" data-anchor-id="reflection">Reflection</h2>
<p>When I started this course, my three primary learning objectives were:</p>
<ol type="1">
<li>Understanding how user research is formally conducted and implementing it through the group project complement my second concentration in Human-centered Data Science.</li>
<li>Becoming comfortable working with and speaking the language of designers and UX researchers to be a better software engineer and data scientist.</li>
<li>Improve my design thinking skills, visualize data, and rapidly prototype low and high-fidelity designs.</li>
</ol>
<p>Throughout the group project, I learned the iterative nature of problem-solving, which involves continuous information gathering, problem redefinition, and solution exploration. Our diverse methods provided me with a comprehensive set of UX skills, enriching my approach to future projects. Notably, during the final poster session, I utilized Figma to create low and high-fidelity prototypes for our token strategy, an experience that expanded my technical toolkit. The course readings proved essential, deepening my understanding of each research method‚Äôs nuances. The practical application of synthesis methods like Affinity Clustering, Walk-the-Wall, and Speed Dating significantly enhanced my ability to reassess problems and discover varied solutions within the design realm. Additionally, analyzing complex and unstructured text data sharpened my data wrangling and visualization capabilities.</p>
<p>This project revealed that synthesis is a nuanced and intricate process. While research methods may appear straightforward in theory, their practical application is challenging, often requiring a shift in perspective to unearth valuable insights. This experience was gratifying, allowing me to view product development through the lens of a product manager and a UI/UX designer beyond my engineering and data science background. I am grateful to have met all my initial learning goals and feel more confident in my professional abilities. A heartfelt thank you to the teaching team for a truly enriching experience!</p>


</section>
</section>

 ]]></description>
  <category>A/B Testing</category>
  <category>Data Science</category>
  <category>Human-Computer Interaction</category>
  <category>Responsible AI</category>
  <category>UI/UX</category>
  <category>Research</category>
  <guid>https://nive927.github.io/projects/2024-05-03-ucre-project-portfolio/index.html</guid>
  <pubDate>Fri, 03 May 2024 04:00:00 GMT</pubDate>
  <media:content url="https://nive927.github.io/projects/2024-05-03-ucre-project-portfolio/images/project-card-image.png" medium="image" type="image/png" height="121" width="144"/>
</item>
<item>
  <title>Music Magician Interactive Dashboard</title>
  <dc:creator>Nivedhitha Dhanasekaran</dc:creator>
  <link>https://nive927.github.io/projects/2023-12-31-music-magician-spotify-dashboard/index.html</link>
  <description><![CDATA[ 




<section id="music-magicians-spotify-music-artist-influence-analytics-dashboard" class="level1">
<h1>Music Magicians: Spotify Music Artist Influence Analytics Dashboard</h1>
<!-- ![](images/Flight_Delay_Prediction_Pipelining_Flowchart.png) -->
<p>I developed an interactive analytics dashboard using Streamlit, Vega-Altair, Plotly, NetworkX, and Python. The dashboard focuses on exploring the evolution of music and quantifying artists‚Äô influence. It features informative visualizations and interactive elements to analyze music trends, artist characteristics, and the influence of past music on new compositions. This project successfully addressed the challenge by creating a user-friendly interface that facilitates in-depth exploration of the dataset, enabling storytelling and uncovering patterns and correlations in music.</p>
<!-- RESOLVED ERROR: iframe embed to streamlit share.streamlit.io redirected you too many times. -->
<!-- Modify streamlit url with embedded=true to fix error -->
<!-- https://discuss.streamlit.io/t/embeding-streamlit-cloud-url-with-iframe/27511 -->
<iframe height="850" style="width:100%;" src="https://music-magician-dashboard.streamlit.app/?embedded=true" title="Music Magician"></iframe>
<!--  -->


</section>

 ]]></description>
  <category>Data Science</category>
  <category>Deployment</category>
  <category>Human-Computer Interaction</category>
  <category>UI/UX</category>
  <category>Web Development</category>
  <guid>https://nive927.github.io/projects/2023-12-31-music-magician-spotify-dashboard/index.html</guid>
  <pubDate>Sat, 30 Dec 2023 05:00:00 GMT</pubDate>
  <media:content url="https://nive927.github.io/projects/2023-12-31-music-magician-spotify-dashboard/images/project-card-image.png" medium="image" type="image/png" height="71" width="144"/>
</item>
<item>
  <title>Automated Detection of Giant Cell Arteritis from Temporal Artery Biopsy Specimens Using Deep Learning</title>
  <dc:creator>Nivedhitha Dhanasekaran</dc:creator>
  <link>https://nive927.github.io/projects/2023-02-17-gca/index.html</link>
  <description><![CDATA[ 




<section id="related-links-artifacts" class="level1">
<h1>Related Links &amp; Artifacts</h1>
<div style="display: flex; flex-wrap: wrap; gap: 0.75rem; margin-top: 1rem; font-size: 0.85em;">
<p><a href="https://github.com/karthik-d/gca-detection-tab-slides" target="_blank" style="background-color:#24292f; color:white; padding:8px 16px; border-radius:6px; text-decoration:none;"><i class="bi bi-github" style="margin-right:6px;"></i>GitHub Repository</a></p>
<p><a href="./documents/GCA-ARVO-Poster.pdf" target="_blank" style="background-color:#fc3939; color:white; padding:8px 16px; border-radius:6px; text-decoration:none;"><i class="bi bi-file-earmark-pdf-fill" style="margin-right:6px;"></i>Download Poster (PDF)</a></p>
<p><a href="https://iovs.arvojournals.org/article.aspx?articleid=2787145" target="_blank" style="background-color:#593196; color:white; padding:8px 16px; border-radius:6px; text-decoration:none;"><i class="bi bi-journal-text" style="margin-right:6px;"></i>Read on IOVS (ARVO 2022)</a></p>
<p><a href="https://www.linkedin.com/in/naveena-yanamala-ms-phd-fase-240a229/" target="_blank" style="background-color:#0077b5; color:white; padding:8px 16px; border-radius:6px; text-decoration:none;"><i class="bi bi-linkedin" style="margin-right:6px;"></i>Dr.&nbsp;Naveena Yanamala</a></p>
<p><span style="background-color:#f3f4f6; color:#111; padding:8px 16px; border-radius:6px;">üìÖ Project Duration: Jan 2021 ‚Äì Jul 2022</span></p>
</div>
</section>
<section id="project-overview" class="level1">
<h1>Project Overview</h1>
<p>This project was developed at the intersection of medical imaging and applied deep learning, in collaboration with pathologists from <strong>Carnegie Mellon University, Rutgers and Tulane</strong>. The work focused on building an end-to-end diagnostic system that brings explainable, high-performance ML models into digital pathology pipelines for vascular autoimmune disorders like <strong>Giant Cell Arteritis (GCA)</strong>.</p>
<blockquote class="blockquote">
<p>‚ú® Built a fully automated ML pipeline achieving <strong>92.32% ROI-level accuracy</strong> and <strong>0.93 AUC</strong> for inflammation detection in digitized biopsy slides.<br>
‚ú® Trained and deployed a <strong>ResNet-34 classifier</strong> optimized for low-latency inference (<strong>168 ms/16 ROIs</strong>) using clinical-grade image preprocessing.<br>
‚ú® Published and presented results at <strong>ARVO 2022</strong>, with GradCAM visual validation accepted by board-certified ophthalmic pathologists.</p>
</blockquote>
<div style="display: flex; flex-wrap: wrap; gap: 0.75rem; margin-top: 1rem; font-size: 0.85em;">
<p><strong>FUNDED BY: </strong> <span style="background-color:#13b955; color:white; padding:8px 16px; border-radius:6px;"><i class="bi bi-cash-stack" style="margin-right:6px;"></i>NIH U54 GM104942</span> <span style="background-color:#13b955; color:white; padding:8px 16px; border-radius:6px;">Oliver and Carroll Dabezies Tulane Endowed Chair</span></p>
</div>
</section>
<section id="description" class="level1">
<h1>Description</h1>
<p>This project automated the detection of Giant Cell Arteritis (GCA) from digitized temporal artery biopsy (TAB) slides, spanning every stage from raw data handling to clinical deployment readiness and publication. I collaborated directly with clinicians, developed the ML pipeline, validated it with expert feedback, and co-authored the peer-reviewed publication.</p>
<p><object data="./documents/GCA-ARVO-Poster.pdf" type="application/pdf" width="100%" height="725"><a href="./documents/GCA-ARVO-Poster.pdf" download="">Download PDF file.</a></object></p>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>1. Data Collection &amp; Expert Labeling</b></span>
</summary>
<ul>
<li>Curated a dataset of <strong>472 high-resolution TAB slides</strong> spanning 20 years (2000‚Äì2019).<br>
</li>
<li>Participated in quality control and slide review, excluding 192 suboptimal samples.<br>
</li>
<li>Collaborated with ophthalmic pathologists to annotate slides with <strong>binary GCA labels</strong>, forming the ground truth.</li>
</ul>
</details>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>2. Image Preprocessing</b></span>
</summary>
<ul>
<li>Developed a CV pipeline to <strong>automatically detect artery regions</strong> from stain intensity.<br>
</li>
<li>Extracted and padded <strong>3,558 ROIs</strong> to standardized <strong>512√ó512 px</strong> RGB tiles.<br>
</li>
<li>Applied <strong>color jittering and rotation (0¬∞, 90¬∞, 180¬∞, 270¬∞)</strong> for augmentation and rotational invariance.</li>
</ul>
</details>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>3. Model Development</b></span>
</summary>
<ul>
<li>Trained a <strong>ResNet-34 model</strong> fine-tuned on ImageNet for binary GCA classification.<br>
</li>
<li>Designed a dual-level prediction strategy: <strong>ROI-level and slide-level classification</strong>.<br>
</li>
<li>Balanced model complexity and latency to enable <strong>fast inference (~168 ms for 16 ROIs)</strong> on an Apple M1 GPU.</li>
</ul>
</details>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>4. Validation &amp; Explainability</b></span>
</summary>
<ul>
<li>Integrated <strong>GradCAM</strong> to generate region-wise visual explanations for clinical review.<br>
</li>
<li>Validated that heatmaps aligned with features like <strong>lymphocytic infiltrates and giant cells</strong>.<br>
</li>
<li>Worked with pathologists to verify that model attention <strong>matched diagnostic hotspots</strong> in expert-labeled slides.</li>
</ul>
</details>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>5. Model Optimization</b></span>
</summary>
<ul>
<li>Benchmarked performance across <strong>ResNet-18, 34, and 50</strong>, selecting ResNet-34 for its <strong>92.32% accuracy and 0.93 AUC</strong> on the held-out 2019 test set.<br>
</li>
<li>Tuned augmentation, data balancing, and inference pipeline for <strong>clinical deployment feasibility</strong>.<br>
</li>
<li>Demonstrated robust performance across time-split validation, reinforcing generalizability.</li>
</ul>
</details>
<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>6. Publication &amp; Reporting</b></span>
</summary>
<ul>
<li>Co-authored and published results in <a href="https://iovs.arvojournals.org/article.aspx?articleid=2787145">Investigative Ophthalmology &amp; Visual Science (ARVO 2022)</a><br>
</li>
<li>Designed all pipeline diagrams, GradCAM visualizations, and tables for the final report and poster.<br>
</li>
<li>Created the project poster for dissemination at conferences and institutional reviews.</li>
</ul>
</details>
</section>
<section id="tools-frameworks" class="level1">
<h1>Tools &amp; Frameworks</h1>
<table class="table">
<colgroup>
<col style="width: 26%">
<col style="width: 73%">
</colgroup>
<thead>
<tr class="header">
<th>Research Focus</th>
<th>Stack / Tools Used</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Deep Learning Pipelines</td>
<td><code>PyTorch</code>, <code>Torchvision</code>, <code>ImageNet</code>, <code>Transfer Learning</code>, <code>scikit-learn</code></td>
</tr>
<tr class="even">
<td>Medical Image Preprocessing</td>
<td><code>OpenSlide</code>, <code>PIL</code>, <code>OpenCV</code>, <code>skimage</code>, <code>IBM H&amp;E Normalization</code>, <code>ROI Extraction</code>, <code>optparse</code></td>
</tr>
<tr class="odd">
<td>Explainability &amp; Debugging</td>
<td><code>GradCAM</code>, <code>pytorch-grad-cam</code>, <code>Matplotlib</code>, <code>Seaborn</code></td>
</tr>
<tr class="even">
<td>High-Performance Inference</td>
<td><code>Batch Processing</code>, <code>Multi-threading</code>, <code>GPU Training</code>, <code>WSI Multiprocessing</code></td>
</tr>
<tr class="odd">
<td>Deployment Readiness</td>
<td><code>Pipeline Automation (CMD)</code>, <code>PDF Reporting</code>, <code>Clinical Evaluation Integration</code></td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>Data Science</category>
  <category>Deep Learning</category>
  <category>Funding</category>
  <category>GPU Training</category>
  <category>Machine Learning System</category>
  <category>Model Explainability</category>
  <category>Publication</category>
  <category>Research</category>
  <guid>https://nive927.github.io/projects/2023-02-17-gca/index.html</guid>
  <pubDate>Fri, 15 Jul 2022 04:00:00 GMT</pubDate>
  <media:content url="https://nive927.github.io/projects/2023-02-17-gca/images/project-card-image.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Braille Voice</title>
  <dc:creator>Nivedhitha Dhanasekaran</dc:creator>
  <link>https://nive927.github.io/projects/2022-07-01-braillevoice/index.html</link>
  <description><![CDATA[ 




<section id="project-poster" class="level1">
<h1>Project Poster</h1>
<!-- <br> -->
<!-- <div style="display: flex; flex-direction: row; align-items: center; justify-content: center;"> -->
<!--   <img src="./images/project-card-image.png" alt="Broken image."> -->
<!-- </div> -->
<!-- </br>\ -->
<p><object data="./documents/D10_Poster.pdf" type="application/pdf" width="100%" height="600"><a href="./documents/D10_Poster.pdf" download="">Download PDF file.</a></object></p>
</section>
<section id="project-description" class="level1">
<h1>Project Description</h1>
<p>A novel multi-stage pipeline (Mobile &amp; Web app) for Braille-to-Text translation of whole single-sided printed braille documents with support for text-to-speech audio playback &amp; text summarization in two languages: English &amp; Tamil.</p>


</section>

 ]]></description>
  <category>Computer Vision</category>
  <category>Data Science</category>
  <category>Deep Learning</category>
  <category>Deployment</category>
  <category>GPU Training</category>
  <category>Human-Computer Interaction</category>
  <category>Machine Learning System</category>
  <category>Mobile Development</category>
  <category>Natural Language Processing</category>
  <category>Research</category>
  <category>UI/UX</category>
  <category>Web Development</category>
  <guid>https://nive927.github.io/projects/2022-07-01-braillevoice/index.html</guid>
  <pubDate>Fri, 15 Jul 2022 04:00:00 GMT</pubDate>
  <media:content url="https://nive927.github.io/projects/2022-07-01-braillevoice/images/project-card-image.png" medium="image" type="image/png" height="87" width="144"/>
</item>
<item>
  <title>ORCA: Underwater Robot</title>
  <dc:creator>Nivedhitha Dhanasekaran</dc:creator>
  <link>https://nive927.github.io/projects/2021-05-01-orca/index.html</link>
  <description><![CDATA[ 




<section id="related-links-artifacts" class="level1">
<h1>Related Links &amp; Artifacts</h1>
<div style="display: flex; flex-wrap: wrap; gap: 0.75rem; margin-top: 1rem; font-size: 0.85em;">
<p><a href="https://www.youtube.com/watch?v=OBnoacUi01A" target="_blank" style="background-color:#ff0000; color:white; padding:8px 16px; border-radius:6px; text-decoration:none;"><i class="bi bi-youtube" style="margin-right:6px;"></i>Watch Demo</a></p>
<p><a href="https://ieeexplore.ieee.org/document/9775325" target="_blank" style="background-color:#593196; color:white; padding:8px 16px; border-radius:6px; text-decoration:none;"><i class="bi bi-journal-text" style="margin-right:6px;"></i>IEEE Publication</a></p>
<p><a href="https://sites.google.com/prod/view/uwarlssn/rd?authuser=0" target="_blank" style="background-color:#0077b5; color:white; padding:8px 16px; border-radius:6px; text-decoration:none;"><i class="bi bi-globe" style="margin-right:6px;"></i>Lab Website</a></p>
<p><a href="https://sites.google.com/prod/view/uwarlssn/rd/consortium?authuser=0" target="_blank" style="background-color:#13b955; color:white; padding:8px 16px; border-radius:6px;"><i class="bi bi-cash-stack" style="margin-right:6px;"></i>Funded: ‚Çπ9.5 Lakh INR</a></p>
<p><span style="background-color:#f3f4f6; color:#111; padding:8px 16px; border-radius:6px;">üìÖ Project Duration: Jun 2019 - Jun 2022</span></p>
</div>
</section>
<section id="project-overview" class="level1">
<h1>Project Overview</h1>
<p>A 500x350x210 mm inspection-class underwater robot equipped for hybrid manual-autonomous navigation. <strong>ORCA</strong> features a 6-thruster mobility suite, real-time telemetry, and multi-sensor feedback including pressure, temperature, leak, and IMU modules. This system integrates a joystick-driven manual mode and an autonomous navigation module built on <strong>MOOS-IvP</strong> (C++ mission planning suite from MIT).</p>
<blockquote class="blockquote">
<p>‚ú® Led a 7-member team to build and test ORCA‚Äôs software stack with kill-switch safety, homing, and GAN-enhanced visibility.<br>
‚ú® Achieved &gt;90% mission success across test runs with real-time diagnostics and fail-safe handling.<br>
‚ú® Integrated autonomous control, camera feedback, and tethered communication using custom C++ modules.</p>
</blockquote>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/OBnoacUi01A" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
<section id="timeline-of-contributions" class="level1">
<h1>Timeline of Contributions</h1>
<details style="background-color:#f5f4fa; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>1. ORCA Robot: Joystick Operation</b></span>
</summary>
<ul>
<li>Developed an autonomous + manual navigation stack on MOOS-IvP.<br>
</li>
<li>Built joystick control, kill switch, IMU fusion, and thrust mapping systems.<br>
</li>
<li>Bench + pool tested with full sensor telemetry and thrust validation.<br>
</li>
<li>Published at IEEE and won the <strong>Asia-Pacific Region IEEE R10 UGPVC</strong>.</li>
</ul>
</details>
<details style="background-color:#f5f4fa; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>2. KYOGRE: Autonomous Navigation Suite with Perception Sub-systems</b></span>
</summary>
<ul>
<li>Internally funded with ‚Çπ30,000 (SSN IFSP).<br>
</li>
<li>Developed deep RL-based waypoint control for UWVs.<br>
</li>
<li>Integrated into ORCA system as a future planning module.</li>
</ul>
</details>
<details style="background-color:#f5f4fa; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>3. EyeSea: Real-Time Marine Threat Alerting Sub-system</b></span>
</summary>
<ul>
<li>Built a YOLOv3 + MobileNetV2 pipeline to detect marine species threats.<br>
</li>
<li>Arduino-based IMU fusion + tethered alert system for swimmer safety.<br>
</li>
<li>Developed for <strong>National Institute of Ocean Technology</strong>, winning <strong>Smart India Hackathon 2022 (Prelims)</strong>.</li>
</ul>
</details>
<details style="background-color:#f5f4fa; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>4. SatVision: Satellite-Based Built-Up Detection</b></span>
</summary>
<ul>
<li>Developed a two-step segmentation and merging approach using satellite imagery.<br>
</li>
<li>Identified non-residential built-up clusters in dense Indian cities (e.g., Mumbai, Delhi).<br>
</li>
<li>üèÖ <strong>Won Smart India Hackathon 2022 ‚Äì Software Edition</strong>, awarded ‚Çπ1,00,000 by Ministry of Earth Sciences.</li>
</ul>
</details>
<details style="background-color:#f5f4fa; border-radius:10px; padding:0.75em;">
<summary>
<span style="color:#593196;"><b>5. C-GAN for Enhanced Localization</b></span>
</summary>
<ul>
<li>Designed a Conditional GAN to refine robot localization sequences.<br>
</li>
<li>Corrected drift and improved tracking precision for mission-critical tasks.<br>
</li>
<li>Contributed to robustness of multi-agent localization under low-visibility conditions.</li>
</ul>
</details>
</section>
<section id="awards-recognition" class="level1">
<h1>Awards &amp; Recognition</h1>
<ul>
<li><p>üèÜ <strong>IEEE R10 Undergraduate Project Video Contest Winner (Aug 2021)</strong><br>
‚Äì Winner from 7 countries and 50+ councils<br>
‚Äì Project video featured on IEEE YouTube channel<br>
‚Äì $300 cash prize</p></li>
<li><p>üèÖ <strong>Smart India Hackathon 2022 ‚Äì Software Edition</strong><br>
‚Äì Winner for PSID: GR823 (SatVision Project)<br>
‚Äì ‚Çπ1,00,000 grant awarded by the Ministry of Earth Sciences, Government of India</p></li>
<li><p>üí∞ <strong>Internally Funded Student Project (SSN College of Engineering)</strong><br>
‚Äì ‚Çπ30,000 grant for KYOGRE (autonomous navigation sub-system for ORCA)</p></li>
</ul>
</section>
<section id="tools-frameworks" class="level1">
<h1>Tools &amp; Frameworks</h1>
<table class="table">
<colgroup>
<col style="width: 22%">
<col style="width: 77%">
</colgroup>
<thead>
<tr class="header">
<th>System Layer</th>
<th>Tools / Frameworks</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Navigation &amp; Control</td>
<td><code>C++</code>, <code>MOOS-IvP</code>, <code>Arduino</code>, <code>Joystick</code></td>
</tr>
<tr class="even">
<td>Computer Vision</td>
<td><code>YOLOv3</code>, <code>MobileNetV2</code>, <code>OpenCV</code>, <code>GAN</code>, <code>Real-time Video Analytics</code></td>
</tr>
<tr class="odd">
<td>Embedded Systems</td>
<td><code>IMU Fusion</code>, <code>Leak Sensor</code>, <code>Pressure Sensor</code>, <code>Arduino Serial Comms</code></td>
</tr>
<tr class="even">
<td>Robotics</td>
<td><code>6-Thruster Dynamics</code>, <code>Manual-Autonomous Switch</code>, <code>Kill Switch Logic</code></td>
</tr>
<tr class="odd">
<td>Deployment &amp; Monitoring</td>
<td><code>Telemetry Monitoring</code>, <code>Underwater Testing</code>, <code>Sensor Drift Handling</code></td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>Award Winner</category>
  <category>Computer Vision</category>
  <category>Deep Learning</category>
  <category>Deployment</category>
  <category>Funding</category>
  <category>GPU Training</category>
  <category>Human-Computer Interaction</category>
  <category>Machine Learning System</category>
  <category>Monitoring</category>
  <category>On-Device AI</category>
  <category>Publication</category>
  <category>Research</category>
  <category>Robotics</category>
  <category>Signal Processing</category>
  <guid>https://nive927.github.io/projects/2021-05-01-orca/index.html</guid>
  <pubDate>Fri, 24 Jun 2022 04:00:00 GMT</pubDate>
  <media:content url="https://nive927.github.io/projects/2021-05-01-orca/images/project-card-image.png" medium="image" type="image/png" height="103" width="144"/>
</item>
<item>
  <title>Two Stage Flight Delay Prediction</title>
  <dc:creator>Nivedhitha Dhanasekaran</dc:creator>
  <link>https://nive927.github.io/projects/2021-06-30-flight-delay-prediction/index.html</link>
  <description><![CDATA[ 




<section id="flight-delay-prediction" class="level1">
<h1>Flight Delay Prediction</h1>
<p><img src="https://nive927.github.io/projects/2021-06-30-flight-delay-prediction/images/Flight_Delay_Prediction_Pipelining_Flowchart.png" class="img-fluid"></p>
<p>I designed a two-stage predictive machine learning engine to forecast the on-time performance of flights at 15 different USA airports using 2016-2017 data. This project involved data cleaning, pre-processing, and merging flight and weather data. To address flight delays, the engine first classifies whether a flight will be delayed and then predicts the arrival delay in minutes for delayed flights. Due to class imbalance favoring on-time flights, SMOTE sampling was used before classification. The Random Forest classifier achieved the best performance with an F1 score of 0.78 and a Recall of 0.74 for delayed flights. For regression, the Random Forest regressor yielded an MAE of 7.178 minutes, an RMSE of 11.283 minutes, and an R-squared score of 0.977.</p>
<p><object data="./documents/Two_Stage_Flight_Delay_Prediction_Report_Nivedhitha_D.pdf" type="application/pdf" width="100%" height="650"><a href="./documents/Two_Stage_Flight_Delay_Prediction_Report_Nivedhitha_D.pdf" download="">Download PDF file.</a></object></p>


</section>

 ]]></description>
  <category>Data Science</category>
  <category>Machine Learning System</category>
  <category>Model Explainability</category>
  <guid>https://nive927.github.io/projects/2021-06-30-flight-delay-prediction/index.html</guid>
  <pubDate>Wed, 30 Jun 2021 04:00:00 GMT</pubDate>
  <media:content url="https://nive927.github.io/projects/2021-06-30-flight-delay-prediction/images/project-card-image.png" medium="image" type="image/png" height="76" width="144"/>
</item>
</channel>
</rss>

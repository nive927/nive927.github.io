---
title: "NoteRecall: On-Device RAG for Privacy-preserving QA"
author: Nivedhitha Dhanasekaran
description: "Efficient and privacy-preserving on-device notes retrieval and question answering system"
date: 2025-05-12
categories: [On-Device AI, Large Language Models, Machine Learning Systems, Natural Language Processing]
image: ./images/project-card-image.png
format:
  html:
    toc: true
    number-sections: false
    css: styles/custom.css
draft: false
---

# Related Links & Artifacts

<div style="display: flex; flex-wrap: wrap; gap: 0.75rem; margin-top: 1rem; font-size: 0.85em;">

  <a href="./documents/noterecall_final_report.pdf" target="_blank" style="background-color:#ea4335; color:white; padding:8px 16px; border-radius:6px; text-decoration:none;"><i class="bi bi-file-earmark-text" style="margin-right:6px;"></i>Final Report (PDF)</a>

  <a href="https://www.lti.cs.cmu.edu/people/faculty/strubell-emma.html" target="_blank" style="background-color:#d93025; color:white; padding:8px 16px; border-radius:6px; text-decoration:none;"><i class="bi bi-person-lines-fill" style="margin-right:6px;"></i>Advisor: Emma Strubell</a>

  <span style="background-color:#f3f4f6; color:#111; padding:8px 16px; border-radius:6px;">ðŸ“… Project Duration: Mar 2025 - May 2025</span>

</div>

# Project Overview

NoteRecall is a lightweight, privacy-preserving Retrieval-Augmented Generation (RAG) system that enables secure, on-device question answering over personal documents such as medical notes or offline records. Built with quantized embedding and reader models, the system performs end-to-end inference on devices like M2 MacBooks while ensuring user data never leaves the device.

> âœ¨ Developed a privacy-preserving RAG pipeline for on-device question answering over medical documents  
> âœ¨ Applied quantization, pruning, and model distillation to optimize inference efficiency on consumer hardware  
> âœ¨ Achieved a BERTScore of 0.56 while preserving retrieval and generation quality  
> âœ¨ Enabled 2.4Ã— more inference cycles under a 10Wh energy budget compared to baseline models  


# Description

The NoteRecall pipeline embeds user documents and retrieves relevant chunks to answer questions locally using a distilled and quantized reader model.

{{< pdf ./documents/ODML_Final_Report_ACL_Personal.pdf width=100% height=850 >}}

<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;"><summary><span style="color:#593196;"><b>1. Motivation & Privacy-First Setup</b></span></summary>

- Eliminates the need for cloud-based LLMs and reduces risk of data leakage  
- Designed for privacy-critical use cases like medical records and personal notes  
- Empowers users with offline, secure information retrieval  
- Ensures all computation (embedding, search, generation) remains local  

</details>

<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;"><summary><span style="color:#593196;"><b>2. Task Setup & Data</b></span></summary>

- Input: User documents (context) and natural language queries  
- Output: Retrieved passages and generated answers  
- Dataset: BioASQ Task B medical QA corpus (3,680 docs, 300 QA pairs)  
- Metrics: BERTScore F1, end-to-end latency, energy efficiency (inferences per 10Wh)  

</details>

<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;"><summary><span style="color:#593196;"><b>3. Model Architecture & Pipeline</b></span></summary>

- Dense retriever: GTE-Qwen-2 Instruct (1.5B)  
- Reader model: LLaMA3.2-1B or distilled Flan-T5  
- Vector store: FAISS with HNSW indexing  
- Chunking strategy: 512-token overlapping spans  
- Top-k retrieval (k=3) used as reader context  
- End-to-end inference optimized with MLX and MPS  

</details>

<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;"><summary><span style="color:#593196;"><b>4. Efficiency Optimizations</b></span></summary>

- Quantization (Q3_K_S, Q5_K_M via llama.cpp)  
- Structured/unstructured pruning of FFNs & attention heads  
- Model distillation from LLaMA3 to Flan-T5 for latency and energy gains  
- Evaluated MoE-style readers for tradeoff exploration  

</details>

<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;"><summary><span style="color:#593196;"><b>5. Results Summary</b></span></summary>

- BERTScore F1: 0.5693 (full-precision), 0.5597 (distilled Flan-T5)  
- Latency: Reduced from 8.5s to 1.45s (quantized models)  
- Energy: ~2.4Ã— more inferences under 10Wh using Flan-T5  
- MoE models yielded higher accuracy but were memory inefficient on M2 hardware  

</details>

# Tools & Frameworks

| Area                      | Tools / Stack Used                                                                  |
|---------------------------|--------------------------------------------------------------------------------------|
| Retriever                 | `GTE-Qwen-2 Instruct`, `FAISS`, `HNSW`                                               |
| Reader Models             | `LLaMA3.2-1B-Instruct`, `Flan-T5-Base`, `Qwen1.5-MoE-A2.7B`                           |
| Quantization & Pruning    | `llama.cpp`, `GGUF`, `L1-based head pruning`, `Optimum`, `torch.nn.utils.prune`     |
| Distillation              | `Synthetic QA pairs`, `LLaMA 7B (teacher) â†’ Flan-T5 (student)`                      |
| On-Device Inference       | `MLX`, `MPS (Apple Silicon)`, `GGML`                                                 |
| Evaluation                | `BioASQ`, `BERTScore`, `Latency profiler`, `CodeCarbon (for energy tracking)`          |

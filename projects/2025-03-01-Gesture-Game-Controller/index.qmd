---
title: "Arduino Gesture-based Game Controller Joystick Design"
author: Nivedhitha Dhanasekaran
description: "Mid-air, two-hand Arduino gesture controller with on-device neural inference and BLE-based wireless control for 3D interaction"
date: 2025-03-18
categories: [Computer Vision, Deep Learning, Human-computer Interaction, Interaction Design, Machine Learning Systems, On-Device AI, Signal Processing]
image: ./images/project-card-image.png
format:
  html:
    toc: true
    number-sections: false
    css: styles/custom.css
draft: false
---

# Related Links & Artifacts

<div style="display: flex; flex-wrap: wrap; gap: 0.75rem; margin-top: 1rem; font-size: 0.85em;">

<!-- TODO: Change LINKS later -->
  <!-- <a href="https://docs.google.com/presentation/d/1TfJ77IAXKsWr_72yX_Gy3CM7c8GRqdyjcQc_PHpHO0M/edit?usp=sharing" target="_blank" style="background-color:#0b57d0; color:white; padding:8px 16px; border-radius:6px; text-decoration:none;"><i class="bi bi-file-earmark-slides" style="margin-right:6px;"></i>60s Pitch Deck</a> -->

  <!-- <a href="https://drive.google.com/drive/folders/1lYVCIa6JRh4rkgkOQeiAazqBx5hhv1lU?usp=sharing" target="_blank" style="background-color:#34a853; color:white; padding:8px 16px; border-radius:6px; text-decoration:none;"><i class="bi bi-folder-symlink-fill" style="margin-right:6px;"></i>Artifacts Drive</a> -->

  <a href="./documents/ndhanase_P2_Gesture_Recognition.pdf" target="_blank" style="background-color:#ea4335; color:white; padding:8px 16px; border-radius:6px; text-decoration:none;"><i class="bi bi-file-earmark-text" style="margin-right:6px;"></i>Final Report (PDF)</a>

  <a href="https://hcii.cmu.edu/people/alexandra-ion" target="_blank" style="background-color:#d93025; color:white; padding:8px 16px; border-radius:6px; text-decoration:none;"><i class="bi bi-person-lines-fill" style="margin-right:6px;"></i>Advisor: Alexandra Ion</a>

  <span style="background-color:#f3f4f6; color:#111; padding:8px 16px; border-radius:6px;">ðŸ“… Project Duration: Jan 2025 â€“ Mar 2025</span>

</div>

# Project Overview

This project introduces a mid-air gesture-based interaction system for 3D gaming using a dual-IMU Arduino Nano 33 BLE controller. Designed for ergonomic, untethered play, it leverages TinyML for on-device classification and Bluetooth for wireless interaction. Three controller form factors were tested, with the final version delivering reliable gesture recognition and low-latency UI control.

> âœ¨ Achieved **99% on-device classification accuracy** using an optimized neural network trained on 10 DOF time-series IMU signals  
> âœ¨ Reduced average user task time from **~170s to ~33s** through iterative interaction design and model refinement  
> âœ¨ Deployed a real-time streaming pipeline using **TensorFlow Lite Micro** and **BLE** on the Arduino Nano 33 BLE Rev2  

# Description

The project spanned interaction prototyping, embedded model deployment, user testing, and performance tuning. Gesture-to-command mappings were incrementally refined using user feedback, statistical evaluation, and post-processing filters. The final system supports 9 unique gestures including movement, rotation, and resizing.

{{< pdf ./documents/ndhanase_P2_Gesture_Recognition.pdf width=100% height=850 >}}

<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;"><summary><span style="color:#593196;"><b>1. Interaction Prototyping & User Testing</b></span></summary>

- Explored glove-based, wand, and two-hand cardboard-mounted designs  
- Conducted pilot studies and structured testing with 5 users (200+ total gesture interactions)  
- Final prototype optimized for comfort and stability with clear gesture isolation  
- Introduced dynamic gestures (flicks and rotations) for resizing and rotation commands  
- Logged average task completion times across design variants  

</details>

<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;"><summary><span style="color:#593196;"><b>2. Model Architecture & Training</b></span></summary>

- Trained a feed-forward neural network using 50-frame input windows with 12-channel IMU data  
- Implemented ReLU-activated dense layers for feature extraction  
- Achieved balanced class distribution with 1-minute capped recording per gesture  
- Evaluated model performance using training curves, confusion matrices, and signal overlap analysis  
- Future work: Evaluate LSTM-based temporal models for sequential gesture disambiguation  

</details>

<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;"><summary><span style="color:#593196;"><b>3. On-Device Deployment with TinyML</b></span></summary>

- Converted trained Keras model to `.tflite` with post-training quantization  
- Deployed model using TensorFlow Lite Micro with C-array headers on Arduino Nano 33 BLE  
- Used sliding window and normalization logic for real-time inference loop  
- Output predictions streamed via BLE to Python client using `bleak`  
- Power bank setup enabled untethered, mobile interaction  

</details>

<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;"><summary><span style="color:#593196;"><b>4. Post-Processing & Stability Filters</b></span></summary>

- Implemented prediction confidence filtering (threshold â‰¥ 0.95)  
- Added temporal stability check: command triggered only after 10â€“20 consistent predictions  
- Filtered flick gestures to prevent misclassification with `up/down` motions  
- UI responsiveness improved through tuning command sensitivity and gesture transitions  

</details>

<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;"><summary><span style="color:#593196;"><b>5. Results & Quantitative Evaluation</b></span></summary>

- Completion times across versions:  
  - Wand: ~174s  
  - Glove: ~146s  
  - Final 2-hand config: ~33s (avg. across users)  
- On-device accuracy: **99%**  
- Error rate: near-zero due to stability + confidence filters  
- Best performance demo captured in video and testing logs (see Drive)  

</details>

<details style="background-color:#f9f9fc; border-radius:10px; padding:0.75em;"><summary><span style="color:#593196;"><b>6. User Feedback & Observations</b></span></summary>

- Users preferred holding cardboard-mounted prototypes over glove-based wearables  
- Flick gestures were perceived as natural and intuitive  
- Static poses for movement felt predictable and easy to learn  
- Fatigue reduced in final version due to ergonomic form factor and gesture separation  
- Common confusion between `+/-` and `up/down` was addressed via axis remapping  

</details>

# Tools & Frameworks

| Area                          | Tools / Stack Used                                                                 |
|-------------------------------|-------------------------------------------------------------------------------------|
| Microcontroller + Sensors     | `Arduino Nano 33 BLE Rev2`, `BMI270`, `MPU6050`, `10 DOF IMU`                      |
| ML Model & Training           | `Keras`, `TensorFlow`, `NumPy`, `scikit-learn`, `Matplotlib`, `Pandas`             |
| On-Device ML Deployment       | `TensorFlow Lite Micro`, `xxd`, `ArduinoBLE`, `tflite::MicroInterpreter`           |
| Gesture Streaming & UI        | `BLE (Arduino + bleak)`, `PyQt5`, `Matplotlib`, `Custom UI Loop`                   |
| Prototyping & Analysis        | `User Testing Logs`, `Training Curves`, `Confusion Matrices`, `PDF Reporting`      |

